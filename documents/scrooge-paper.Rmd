---
title: "Improving Fisheries Stock Assessments by Integrating Economic Datar"
author: "Dan Ovando"
date: "3/7/2018"
output: bookdown::html_document2
linkcolor: blue
# bibliography: dissertation.bib
biblio-style: apalike
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}
library(hrbrthemes)
library(extrafont)
library(scales)
library(rstan)
library(wesanderson)
library(patchwork)
library(rstanarm)
library(tidybayes)
library(ggridges)
library(caret)
library(tidyverse)

extrafont::loadfonts()
rstan::rstan_options(auto_write = TRUE)
functions <- list.files(here::here("functions"))

walk(functions, ~ here::here("functions", .x) %>% source()) # load local functions

in_clouds <-  F

run_name <- "d1.0"

scrooge_theme <-
theme_ipsum(
base_size = 16,
axis_title_size = 18,
strip_text_size = 18
)

theme_set(scrooge_theme)

run_dir <- here::here("results", run_name)

load(here::here("processed_data", "fisheries_sandbox.Rdata"))


case_studies <- readRDS(file = here::here("results", run_name,"case_studies.RDS"))

perf_summaries <- readRDS(file = here::here("results", run_name,"perf_summaries.RDS"))

performance_model <-
readRDS(file =   here::here("results", run_name,"performance_model.RDS"))

sandbox_performance <-
readRDS(file = here::here("results", run_name,"sandbox_performance.RDS"))

processed_sandbox <- readRDS(file =here::here("results","scrooge-results",run_name,"processed_fisheries_sandbox.RDS"))


```


## Abstract

The wellbeing of coastal communities and ecosystems around the world depends on our ability to provide accurate and timely assessments of the status of fished populations. This is typically accomplished by collecting biologically-focused data such as the amount and size of fish captured, and using those data to fit statistical models that estimate factors such as current biomass levels and exploitation rates, usually relative to some benchmark set by stakeholders.

This stock assessment process has had successes throughout the world especially where the data and resources are available to perform what we might call "traditional" statistical stock assessments [@Hilborn2014]. While many of the world's largest and most valuable fisheries fall into this category, the majority of people that depend on fisheries do so in smaller scale operations that lack the capacity for traditional stock assessment, a group that has been generally called "data limited fisheries"[wakefield citation]. This problem has led to an explosion of "data limited stock assessments" (DLAs), that aim to provide management advice using fewer data (but more assumptions). These DLAs have made fisheries assessment possible in previously unassessed places, but increasingly they have sought to simply use more involved statistics to extract more knowledge from the same data (often length composition of the catch).

This study improves the management capacity of data-limited fisheries by demonstrating how expanding the pool of available data to include economic and biological information together can dramatically (hopefully) improve the accuracy of stock assessments.

While fisheries are coupled socio-eclogical systems [@Ostrom2009], the human dimensions of fisheries, the incentives that drive behavior, have been left to economists and other social scientists, while the stock assessment side of the equation has focused on the ecological and biological aspects of a fishery. Our results build off of fisheries economics to show that utilizing data on the economic history of a fishery (prices, costs, technology, labor, and profitability) together with biological data can dramatically improve the ability of stock assessment models to accurately estimate fishing mortality rates. In many realistic simulations, we demonstrate that this bio-economic estimation model provides XX% more accurate results than published methods utilizing biological data alone. We also show that inclusion of these data result in XX% greater confidence when applied to real-world data. Lastly,  we  provide a generalizable simulation framework using machine learning techniques to assess the the relative performance of these models under different states of nature.

Our results demonstrate how often available, but to date underutilized, data can improve stock assessment in data-limited contexts. While we present one specific methodology for incorporating economic data into stock assessment, these results can serve as a foundation for a broader field of study investigating methods for utilizing economic data in biological stock assessment, improving both the accuracy and effectiveness of fisheries management around the globe.


## Introduction

Why are data limited assessments worse?

### General fisheries stuff

Effective fisheries management requires that managers and stakeholders have some ability to estimate and react to the abundance of fishes in the ocean. The history of fisheries science is largely concerned with developing and improving our ability to accomplish this difficult task, starting from early models of growth overfishing [citation] and leading up to multi-species bio-geo-economic models[e.g. citation some mice model].  While the field has made dramatic advances in our ability to assess the status of fisheries, by and large we have found two solutions to this problem: Fit highly complex integrated statistical models to diverse data streams [cite fancy stock assessment technique], or utilize increasing levels of statistical wizardry to try and squeeze more information out of limited data [what has lately been termed Data Limited Stock Assessments, or DLAs; cite]. The explosion of DLAs has been both promising and concerning. The majority of fisheries in the world lack the resources for fully integrated stock assessments, and so depend on this world of "data limited stock assessment". While there has been tremendous growth in this field, nearly all DLAs rely on the same streams of information that would have been available to a fisheries scientist in the 1800s: lengths [cite], captures [cite], and catch per unit effort[cite], generally only one at a time. While these biological data can be highly informative, economic data can also provide information as to the history and status of a fishery. We present here a novel tool for combining historic economic information with traditional fisheries data to improve fisheries stock assessment.

Why do we need a new line of evidence in stock assessment? One could certainly make the case that statistical stock assessments are complicated enough as it is. But, while these "gold standard" assessments appear to by and large perform well using solely biological data, data-limited stock assessments, in which models are fit by trading in data for assumptions, often struggle if the exact requirements of their assumptions are not satisfied. This can present a major problem for communities and ecosystems that depend on the outcomes of these DLAs to guide their management practices. While future work can examine the usefulness of economic data in a data-rich context, our focus here is in demonstrating how economic information augment biological data to improve the performance of data-limited stock assessments.

What defines a data limited assessment? @Dowling2015 provides a useful summary of what we mean by a data-limited fishery, but for now we can broadly consider data-limited assessments as fisheries lacking sufficient quality information to perform a "traditional" stock assessment, meaning at minimum total catch records and catch-per-unit-effort, on up to a fully integrated statistical catch-at-age model requiring catch, CPUE, length compositions, growth and aging, tagging, etc. A common example of a data-limited fishery might then include a fishery for which only CPUE data is available, or for which the only species-specific information are sampled length frequencies from the port or market.

This paper builds off the length-based DLA literature, and so we focus our discussion on the nature of these methods. See  @Carruthers2014 and @Andersona for thorough summaries of catch-based DLAs. Length-based DLAs all use life history data or assumptions of some kind to translate the distribution of observed lengths in a fished population into some meaningful management metric. Catch-curves, perhaps the oldest of the DLAs, dating back to at least @Chapman1960, use assumptions and estimates of the age-at-length relationship to translate lengths into ages, and measure the slope of the logarithm of the numbers at age to provide an estimate of total mortality *Z*. Assumptions or estimates of natural mortality *m* can then be used to extract fishing mortality *f* simply by $f = Z - m$. Recently, newer methods have evolved that try and estimate fishing mortality rates, recruitment, and selectivity by examining the overall shape of the length composition data[@Hordyk2014; @Rudd2017]. These models use life history data (or assumptions) to simulate what the length composition of a given population would be expected to be if it were left unfished. This estimate of the unfished length composition is then compared to the observed length composition, and estimates of *f*, recruitment, and selectivity are made that best explain the observed length composition, given the expectation provided by life history data.

These methods have proven effective and useful in many circumstances, but their reliance length composition leaves them sensitive to relatively common features in fisheries such as autocorrelated shifts in recruitment regimes [citations]. Given length data alone, it is difficult to separate out recruitment from fishing mortality, since both manifest themselves as change in the relative proportions of different length classes. The most straightforward solution to this problem is to assume that the population is at equilibrium, and any deviations in expected recruitment are on average zero during the time period of analysis. Year-to-year shifts in the length composition are then attributed to fishing mortality. Given limited data, say only one year of length composition data, this may be the only assumption possible.

Since recruitment regimes are likely to be more the rule than the exception though, we would like to be able to do better than this. @Rudd2017 provided an important extension to the equilibrium assumptions underpinning @Hordyk2014 by relaxing the equilibrium assumption and allowing the user to estimate a vector of recruitment deviates and fishing mortality rates given a time series of length composition data. In order to get around the confounded nature of recruitment and fishing mortality, given only length data the LIME model presented in @Rudd2017 requires a user specified penalty constraining the amount that fishing mortality can vary year-to-year.

The other important feature of the LIME model though is that it allows the user to add in additional lines of evidence, for example catch-per-unit effort data... this is good.

An informal synthesis presented at the 2015 Wakefield Symposium

@Dowling2016, @Carruthers2014, and @Prince2003

### What's the Problem that Economic Data Fix?

GET TO HERE FASTER. USE ECONOMIC DATA. HERE'S AN EXAMPLE OF WHY. FOLLOW YOUR OWN ADVICE AND MENTION THE ECON STUFF EARLIER.

LIME presents an important framework for integrating multiple streams of "limited data" together into a comprehensive assessment. However, the data types that can be incorporated into LIME are all components of the traditional fisheries toolbox: lengths, catches, and CPUEs XX. These data are important and useful, but there is an untapped source of information present in many fisheries that our model makes use of: economic data. NEEDS TO BE CLEAR HERE THAT LENGTH BASED IS JUST A VEHICLE FOR THE BIGGER POINT

Why should we expect economic data to be useful?

Summary of fisheries as bio-economic systems

- @Fulton2011 human behavior: the key source of uncertainty. Objectives and uncertainty drive consequences, good source for thinking about this broad problem.
- @Gordon1954
- Empirical evidence for This
- Acknowledgement of SSCOM @Thorson2013a

- Why useful?
  - reasonable to think that data-limited = open access (as a starting point)
  - Why can economic data be resurrected where biological can't

[@Gordon1954; @Szuwalski; @Thorson2013a]

  - Neilsen 2017 (@Nielsen2017) is that lit review of coupled ecological-economic models. Ah right, these are all MSE style models though, not assessment modules. So, a good reference for ways to model effort dynamics, but not redundant on the effort dynamics.

  - [@Branch2006] fleet dynamics and management. Right, this thinks about the way that fleet dynamics interact with different kinds of management. Interesting insights on how you want to include this into DLAs, but not much in the direct sense of integration to stock assessment

  - [@Salas2004] is similar but focuses on small scale fisheries. Interesting stuff but not much on the modeling front. But lots of good content thinking about behavioral dynamics of small scale fisheries that would be valuable to dive into.

-[@vanPutten2012] theories and behavioral drivers underlying fleet dynamics. See @Vermard2008 for some modeling examples. Has lots of useful references in here. Random utility models for individual behavior. Basically, has a nice summary of sources for different ways to model and think about behavior. So, you can start with the profit maximizing side of things, but expand that out for different types of fleet dynamics.

- @Marchal2013 added value of fleet dynamics in models. This one looks at MSE style problems again, but has aome interesting fleet model specifications that you could steal. Equation 11 is handy, just more justification for change is the effort last year times some factor and profits over the last few years

- @Clark1990

- @Hoff2008


### Research statement

This paper demonstrates the how to use historic economic information to improve outcomes of fisheries stock assessments.

## Methods

The overall method for this paper are as follows:

  - We utilize and age-structured bio-economic operating model to database of simulated fisheries
  - Fishing mortality rates from the simulated fishery are estiamted using our bio-economic estimation model, which can take a variety of forms
    - Random walk behavior
    - Open access 
    - ...
  - We assess the performance of the estimation model using a set of case study fisheries
  - We assess broader model performance using a Bayesian hierarchichal model and classification algorithms
  

### Equations

Process Name | Equation Number |Equation
-------------|--------------------------
Thing        |                  |$y \sim x$


### Operating Model

The operating model is adapted from @Ovando2014. It is a relatively straightforward sinlge-species age-structured bioeconomic model. For our case study runs we utilized a more or less "standard" species, red snapper (*Lutjanus campechanus*). For our broader simulation analysis, we randomly selected fish species from the commercial landings statistics of the Food and Agriculture Organziation of the United Nations [FAO, @FAO2018]. Life history data were then estimated for that species using the `FishLife` [@Thorson2017d] package in `R` [@RCoreTeam2018]. Additionally, for each run we drawn random levels of recruitment variaions ($\sigma_r$ from the range 0 to 0.75), as well as recruitment autocorrelation ($ac_{r}$) from the range of 0 to 0.75. 

The economic component of the operating model has four possible modes that are selected for any given simulation... blah blah


Fit alternative models

scrooge-lite

scrooge

LIME/LBSPR/MAYBE

compare fit and bias

### Why A Bayesian Approach?

A general argument for informative priors in data limited assessment

statistical rethinking,

@McElreath2016

@Hobbs2015

@Thorson2017a

@Jiao2011

@Monnahan2016

@Karnauskas2011

@Myers2002

@McAllister1998

@Punt1997 (IMPORTANT TO INCLUDE)



### `scrooge` Model

The estimating model itself, which, since it is motivated by money we call `scrooge`, was coded in `stan` using the `rstan` package [citation]. The core internal operating model is an age structured model identical in structure to the operating model used for the simulations. This means that the model requires user-supplied estimates of life history data, specifically
  - Von Bertalanffy growth parameters
  - Allometric weight and maturity at length/age equations
  - An estimate of natural mortality 
  - An estimate of Beverton-Holt steepness

We will focus our discussion on the estimation model. We constructed four candidate process models describing the economic behavior of the fishing flee  and three candidate likelihoods, each defined by different structural assumptions and data availability. We then fit factorial combinations of each process model with each likelihood structure, omitting combinations that would have double counted some information as both a prior and data (see Table.XX).

The model has a number of parameters it must estimate, namely fishing mortality rates, recruitment deviates, the length at 50% selectivity, and associated process and observation errors as required (see Table.XX for a complete description of all estimated parameters and their prior distributions). The model is initialized at unfished biomass, and then an estimate of initial fishing mortality is applied for 100 burn-in years, achieving a level of depletion at the start of the data-period of the model. The data period of the model is of length *t*, and defines the timesteps during which the model estiamtes dynamic parameters, though the model estimate *t* + age 50% selectivity recruitment deviates, to allow the model to estimate recruitment pulses which start before the data-period but whose signal can be observed during the early years of the data period. Length-composition data must be available for 1 or more years of the data-period, but are not required in all years. For example, the model can function if given ten years of economic information and only one year of length composition data available at the end of the time period. 

All estimation models share common components of recruitment deviates and length composition data. Recruitment is assumed to on average have Beverton-Holt dynamics [@Beverton1959], reparemeterized around steepness. Process error around this mean relationship is assumed to be log-normally distributed with a bias correction


$$r_{t} = BH(SSB_{t-1},h)e^{r^{dev}_{t}}$$


$$r^{dev}_{t} \sim normal(-\sigma_{r}^2/2, \sigma_r)$$

$$\sigma_r \sim normal(0.4,0.4)$$

Length composition data are structured as discrete numbers of fish counted within 1cm length bins per year. While each estimation model differs in its methods for estimating fishing mortality rates, for a given generated mortality rate, vector of estimated recruitment events $r$, and estimated selectivity $s^{50}$, the model produces a vector of probability of capture at length $p^{capture}$ for each time step, given the structural population equations of the model $g()$. 

$$p^{capture}_{t,l} \sim g(f, r, s^{50})$$

The observed numbers at length $N_{t,l}$ are then assumed to be draws from a multinomial distribution of the form

$N_{t,1:L} \sim multinomial(p^{capture}_{t,1:L})$

The key difference in the estimation models is how they estimate *f* and the data that enter the likelihood. All estimation of *f* begins by estimating a parameter $f^{init}$, which is the fishing mortality rate that is held constant over a burn-in period to achieve a given level of depletion by the time of the start of the data period of the model. The estimation models diverge from there in that each specifies a different structural model for how *f* evolves from $f^{init}$. 

#### Random Walk Model (random-walk)

This estimation model is similar in flavor to the penalty on deviations in fishing mortality used in LIME. Under this model, effort deviates $E^{dev}$ are assumed to be lognormally distributed

$$E^{dev}_{t} \sim normal(-\sigma_{E}^2/2, \sigma_E)$$

$$\sigma_E \sim normal(0.4,0.4)$$

Effort evolves in a random walk manner from there

$$E_{1} = \frac{f^{init}}{q}e^{E^{dev}_{1}}$$

$$f_{1} = qE_{1}$$

Where *q* is a catchability coefficient, held at 1e-3. 

For the remaining *T* timesteps

$$E_{t} = E_{t-1}e^{E^{dev}_{t}}$$

$$f_{t} = qE_{t}$$

#### Bioeconomic Model with Profit Ingredients (profit-ingredients)

We now turn to our class of bioeconomic models. Across all evaluated bioeconomic models, we assume that fishermen respond to average not marginal profits, per @Gordon1954, which we quantify as profits per unit effort, or *PPUE*. Under the `ingredients` model, the user supplies some data on absolute or relative changes in prices *p*, costs *c*, and technology (catchability) *q* (which can be thought of as ingredients of profitability). For example, users could report a 25% increase in prices due to the arrival of a new buyer with access to a lucrative foreign market, a 10% decrease in fishing costs due to a government fuel subsidy, and/or a increase in catchability due to the introduciton of fish-finder technology. These ingredients of profitability can be provided either as qualititave information or hard data. Any ingredients for which no estimtes of relative rates of change are available are assumed to remain constant over the time period of the model. The key feature of this model is that it uses these ingredients to inform our prior on $\hat{PPUE_{t}}$ in a given time step. Therefore, if costs go down and prices go up in a given time step, our prior on $\hat{PPUE_{t}}$ will increase appropriately, subsequently increasing our prior on the amount of effort in the following timestep. 


Under this model, $\hat{PPUE_{t}}$ is calculated as 

$$\hat{PPUE_{t}} = \frac{p_{t}C(q_{t},B_{t},E_t,s^{50}) - c_{t}E_{t}^2}{E_{t}}$$


where *C()* represents that Baranov catch equation [citation]. 

Notice now that the components price (*p*), catchability (*q*), and cost (*c*) are allowed to vary over time. This is because the model allows for user supplied information on the evoloution of these economic traits over time, either in the form of actual values (e.g. the price in a given year), or in relative changes (prices are 10% higher than they were last year). For the relative change cases, user supplied inputs are converted to deviations from a mean value and then multiplied by a default mean value set by the model. This is the default for both *q* and *c*, since converting user knowledge into the appropriate units is challenging. For a given value of $\hat{PPUE_{t}}$, we calcualte effort in the next time step per


$$E_{t + 1} = (E_{t} + \theta{\hat{PPUE_{t}}})e^{E^{dev}_{t}}$$

where $\hat{PPUE_{t}}$ the profit per unit effort, and $\theta$ adjusts the responsiveness of effort to *PPUE*. From there

$$f_{t} = q_{t}E_{t}$$


One difficulty in this is that the dynamics of the open access model are driven by the relative profitability of the fishery, and as such largely to the relative scale of revenue and costs. Therefore, getting the relative magnitude of prices and costs to be relatively correct is important. Unfortunately, while prices can be relatively easily determined, costs are much more difficult to obtain, especially in units matching the exact effort units of the operating model. To solve this problem, we estimate an additional parameter in this estimation model, $c^{max}$. The cost in any time is then calculated as

$$c_t  = c^{max}c^{rel}$$

where $c^{rel}$ are the relative costs (scaled from 0 to 1) over time *t* supplied by users. $c^{max}$ itself is tuned in a rather long process that results in much cleaner estimation that can actually be provided with informative priors. Rather than estimating  $c^{max}$ itself, we estimate a cost-to-revenue ratio at the start of the fishery. We first estimate a guess of, given the simulated nature of the fishery, something close to maximum revenues (assumed to come by fishing a bit harder than *m* when the population is completely unfished). We then calculate the profits associated with these maximum reveunes given the current estimate of the cost to revenue ratio CR. 

$$PROFITS^{max} \sim  MAX(PRICE)CATCH^{B0}*(1 - CR) $$


Given this estimate of $PROFITS^{max}$, we can then back out the cost coefficient $c_{max}$ that, given the other parameters, would produce those profits. 

Estimating the cost to revenue ratio instead of the actual costs allows for informative priors to be set. A fishery that at its heyday was incredible profitablt will have a low cost to revenue ratio, while a fishery that was scrapign on its best day would have a high cost to revenue ratio. 

While the prior can be informed from local stakeholders, we set a zero truncated normal prior on the cost ratio of 

$$CR \sim normal(0.5,1)$$

The other challenging parameter in the open access equation is $\theta$, the amount that effort changes for a one unit change in $PPUE$. Similarly to estimating the cost to revenue ratio instead of raw costs, rather than estimate $theta$ directly, we estimate the maximum percentage change in effort from one time step to the next. As part of the CR process, we estimated the maximum profits, and the effor that would produce those profies. Together that provides us with the maxium expected PPUE. For a given max percentage change ($\Delta^{max}$) in effort then, we calculate $\theta$ as

$$\theta = (\Delta^{max}  E^{MAX}) / PPUE^{MAX}$$

While $\theta$ has no intuitative sense for most stakeholders, the maximum percentage year-to-year change in effort can be elicitied from stakeholders. For now, we assume a zero truncated normal prior on the max expansion

$$\Delta^{max} \sim normal(0,0.25)$$

Together, the `ingredients` model allows us to utilize provided ingredients of profitability to drive our prior expectations of the dynamcis of fishing efforts. 


#### Bioeconomic Model with PPUE Data (`ppue`)

The `ingredients` model makes use of individual components of the factors that make up profitability, and under the theory that a) these data are informative to the evolution of effort and b) these data may be easier to obtain than for example actual mean profitability across the fishery. However, we can also consider an effort process model in which *PPUE* is assumed to be known. While complete knowledge of average *PPUE* in a fishery is unlikely, especially in a data-limited context, survey methods could be constructed to collect estimates of *PPUE*, which being a central part of a fisherman's business, is not an unreasonable piece of information to think could be obtained, given the right questions and sufficient trust. 

So, the the `ppue` model, rather than estimating $\hat{PPUE_{t}}$ as a function of its ingredients, we simply take collected values of *PPUE* as data, and estimate effort per 

$$E_{t + 1} = (E_{t} + \theta{PPUE_{t}})e^{E^{dev}_{t}}$$

We estimate $\theta$ using the same methods described in the `ingredients` method (estimating the maximum percent change in year-to-year effort instead of $\theta$ directly). 

Since we no longer assume knowledge of technology changes over time, we assume a static *q*. This assumption could be relaxed in future model runs to consider knowledge of both PPUE and technlogy, but for now we make this assumption to make a cleaner distinction between the `ingredients` model and the other models. 

$$f_{t} = qE_{t}$$


#### Effort Data Model (`effort`)

Both of the bio-economic methods `ingredients` and `ppue` model the change in effort as a function of profits per unit effort. Their key function, from the perspective of a model focused on estimating biological fishery metrics, is to help inform estimates of timestep-to-timestep changes in fishing mortality. To follow the old adage "keep it simple stupid", we also build a model that assumes data on the timestep-to-timestep proportional changes in fishing effort. While such data are unlikely to be available for a fishery covering a large and diverse geographic range, for more localized small-scale fisheries such knowledge is not unreasonable. For example, fishing cooperative in Chile often maintain data on fishing effort [citation]. 

Under the `effort` model then, we assume knowledge of the proportional changes in effort over time $\Delta^{effort}$, where

$$\Delta^{effort} = E_{t+1}/E_{t}$$

and 

$$E_{t + 1} = (E_{t}\Delta^{effort})e^{E^{dev}_{t}}$$

and 

$$f_{t} = qE_{t}$$


#### Simulation Testing

### Results

####

#### Case Studies


```{r prep_figures}
prep_figures <- 
  
  
modelo_name <- c(
  em0_lm0 = "Lengths Only",
  em1_lm1 =  "Lengths + Economics"
)


```


```{r cs1, fig.cap = "Summary of performance under case study 1"}

perf_summaries <- perf_summaries %>% 
  mutate(model = glue::glue("em{economic_model}_lm{likelihood_model}"))

performance_stats <- case_studies %>% 
  select(-scrooge_fit, -prepped_fishery) %>% 
  mutate(others = map(performance,"others")) %>% 
  select(-performance) %>% 
  unnest() %>% 
  mutate(model = glue::glue("em{economic_model}_lm{likelihood_model}")) %>% 
  group_by(case_study, 
           period,
           window,
           prop_years_lcomp_data,
           model, 
           variable,
           .iteration) %>% 
  summarise(rmse = sqrt(mean(sq_er)),
            bias = median((predicted - observed) / observed))

cs1_summary <- performance_stats %>% 
  ungroup() %>% 
  filter(case_study == "realistic",
         prop_years_lcomp_data == 1,
         variable == "f",
         model == "em0_lm0" | model == "em1_lm1")

cs1_summary$model <- fct_recode(as.factor(cs1_summary$model),  "Lengths Only" = "em0_lm0",
                                "Lengths + Economics" = "em1_lm1" )
cs1_summ_plot <- cs1_summary %>% 
  gather(metric, value, rmse:bias) %>% 
  ggplot(aes(value, fill = model)) + 
  geom_density_ridges(aes(x = value, y = metric, fill = model), alpha = 0.75, show.legend = F) +
  theme(plot.margin = unit(c(0,0,0,1), units = "lines"),
        axis.title.y = element_blank(),
        axis.title.x = element_blank()) + 
  geom_vline(aes(xintercept = 0)) +
  theme(legend.title = element_blank()) +
  ggsci::scale_fill_aaas()



lcomp_years <- case_studies %>% 
  filter(case_study == "realistic",
         prop_years_lcomp_data == max(prop_years_lcomp_data),
         period == "middle")

lcomp_years <- lcomp_years$prepped_fishery[[1]]$scrooge_data$length_comps_years


cs1_summary$model <- fct_recode(as.factor(cs1_summary$model),  "Lengths Only" = "em0_lm0",
                                "Lengths + Economics" = "em1_lm1" )

cs1 <- perf_summaries %>% 
  ungroup() %>% 
  filter(case_study == "realistic",
         prop_years_lcomp_data == 1,
         model == "em0_lm0" | model == "em1_lm1") 

fill_vector <- rep("transparent", length (unique(cs1$year[cs1$variable == "f"])))

fill_vector[lcomp_years] <- "tomato"

fill_vector <- c(fill_vector, fill_vector)

cs1_plot <- cs1 %>% 
  filter(variable == "f") %>% 
  mutate(year = year - min(year) + 1) %>% 
  mutate(lcomp_year = year %in% lcomp_years) %>% 
  arrange(experiment) %>% 
  ggplot() + 
  geom_point(aes(year, observed), size = 3, alpha = 0.75, shape = 21, fill =fill_vector) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = model), alpha = 0.2, show.legend = FALSE) +
  geom_line(aes(year,mean_predicted, color = model), show.legend = FALSE) + 
  labs(x = "Year", y = "Fishing Mortality") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas() + 
  facet_wrap(~model, labeller = labeller(model = modelo_name)) + 
  theme(plot.margin = unit(c(0,0,0,0), units = "lines"),
        panel.spacing = unit(0.1, units = "lines"),
        axis.title.x = element_blank())



cs1_plot + cs1_summ_plot + plot_layout(nrow = 1, ncol = 2, widths = c(3,1))


```



```{r}

cs2_summary <- performance_stats %>% 
  ungroup() %>% 
  filter(case_study == "realistic",
         prop_years_lcomp_data == min(prop_years_lcomp_data),
         variable == "f",
         model == "em0_lm0" | model == "em1_lm1")

cs2_summary$model <-
  fct_recode(
  as.factor(cs2_summary$model),
  "Lengths Only" = "em0_lm0",
  "Lengths + Economics" = "em1_lm1"
  )
  
cs2_summary$model <- fct_recode(as.factor(cs2_summary$model),  "Lengths Only" = "em0_lm0",
                                "Lengths + Economics" = "em1_lm1" )

cs2_summ_plot <- cs2_summary %>% 
  gather(metric, value, rmse:bias) %>% 
  ggplot(aes(value, fill = model)) + 
  geom_density_ridges(aes(x = value, y = metric, fill = model), alpha = 0.75, show.legend = F) +
  theme(plot.margin = unit(c(0,0,0,1), units = "lines"),
        axis.title.y = element_blank(),
        axis.title.x = element_blank()) + 
  geom_vline(aes(xintercept = 0)) +
  theme(legend.title = element_blank()) +
  ggsci::scale_fill_aaas()

lcomp_years <- case_studies %>% 
  filter(case_study == "realistic",
         prop_years_lcomp_data == min(prop_years_lcomp_data),
         period == "middle")

lcomp_years <- lcomp_years$prepped_fishery[[1]]$scrooge_data$length_comps_years


cs2 <- perf_summaries %>% 
  ungroup() %>% 
  filter(case_study == "realistic",
         prop_years_lcomp_data == min(prop_years_lcomp_data),
         model == "em0_lm0" | model == "em1_lm1") 

fill_vector <- rep("transparent", length (unique(cs2$year[cs2$variable == "f"])))

fill_vector[lcomp_years] <- "tomato"

fill_vector <- c(fill_vector, fill_vector)

cs2_plot <- cs2 %>% 
  filter(variable == "f") %>% 
  mutate(year = year - min(year) + 1) %>% 
  mutate(lcomp_year = year %in% lcomp_years) %>% 
  arrange(experiment) %>% 
  ggplot() + 
  geom_point(aes(year, observed), size = 3, alpha = 0.75, shape = 21, fill =fill_vector) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = model), alpha = 0.2, show.legend = FALSE) +
  geom_line(aes(year,mean_predicted, color = model), show.legend = FALSE) + 
  labs(x = "Year", y = "Fishing Mortality") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas() + 
  facet_wrap(~model, labeller = labeller(model = modelo_name)) + 
  theme(plot.margin = unit(c(0,0,0,0), units = "lines"),
        panel.spacing = unit(0.1, units = "lines"),
        axis.title.x = element_blank())

cs2_plot + cs2_summ_plot + plot_layout(nrow = 1, ncol = 2, widths = c(3,1))


```


```{r}
cs3_summary <- performance_stats %>% 
  ungroup() %>% 
  filter(case_study == "decoupled",
         prop_years_lcomp_data == min(prop_years_lcomp_data),
         variable == "f",
         model == "em0_lm0" | model == "em1_lm1")

cs3_summary$model <-
  fct_recode(
  as.factor(cs3_summary$model),
  "Lengths Only" = "em0_lm0",
  "Lengths + Economics" = "em1_lm1"
  )
  
cs3_summary$model <- fct_recode(as.factor(cs3_summary$model),  "Lengths Only" = "em0_lm0",
                                "Lengths + Economics" = "em1_lm1" )

cs3_summ_plot <- cs3_summary %>% 
  gather(metric, value, rmse:bias) %>% 
  ggplot(aes(value, fill = model)) + 
  geom_density_ridges(aes(x = value, y = metric, fill = model), alpha = 0.75, show.legend = F) +
  theme(plot.margin = unit(c(0,0,0,1), units = "lines"),
        axis.title.y = element_blank(),
        axis.title.x = element_blank()) + 
  geom_vline(aes(xintercept = 0)) +
  theme(legend.title = element_blank()) +
  ggsci::scale_fill_aaas()

lcomp_years <- case_studies %>% 
  filter(case_study == "decoupled",
         prop_years_lcomp_data == min(prop_years_lcomp_data),
         period == "middle")

lcomp_years <- lcomp_years$prepped_fishery[[1]]$scrooge_data$length_comps_years


cs3 <- perf_summaries %>% 
  ungroup() %>% 
  filter(case_study == "decoupled",
         prop_years_lcomp_data == min(prop_years_lcomp_data),
         model == "em0_lm0" | model == "em1_lm1") 

fill_vector <- rep("transparent", length (unique(cs3$year[cs3$variable == "f"])))

fill_vector[lcomp_years] <- "tomato"

fill_vector <- c(fill_vector, fill_vector)

cs3_plot <- cs3 %>% 
  filter(variable == "f") %>% 
  mutate(year = year - min(year) + 1) %>% 
  mutate(lcomp_year = year %in% lcomp_years) %>% 
  arrange(experiment) %>% 
  ggplot() + 
  geom_point(aes(year, observed), size = 3, alpha = 0.75, shape = 21, fill =fill_vector) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = model), alpha = 0.2, show.legend = FALSE) +
  geom_line(aes(year,mean_predicted, color = model), show.legend = FALSE) + 
  labs(x = "Year", y = "Fishing Mortality") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas() + 
  facet_wrap(~model, labeller = labeller(model = modelo_name)) + 
  theme(plot.margin = unit(c(0,0,0,0), units = "lines"),
        panel.spacing = unit(0.1, units = "lines"),
        axis.title.x = element_blank())

cs3_plot + cs3_summ_plot + plot_layout(nrow = 1, ncol = 2, widths = c(3,1))

```

Focus on performance using economic priors and LIME priors; more cohesive than also then trying to compare to LIME/LBSPR.

Save that for the mega-run, where you can then say something about when different models are likely to perform well, by predicting RMSE as a function of model with effects for each model scenario.

For the case studies, the point is to highlight the general performance of the model. So, let's hold fishery constant, but now run the assessments no economic prior, and then with all incentive, all effort, and equal weighting, and compare performance

### Broader performance stats

```{r best-models}

tidy_performance <- tidybayes::spread_draws(performance_model, b[intercept,model])

rmse_effect_plot <- tidy_performance %>%
  group_by(model) %>%
  mutate(mrmse = mean(b)) %>%
  ungroup() %>%
  mutate(model = fct_reorder(model,b,.fun = mean, .desc = TRUE)) %>%
  ggplot(aes(b, fill = model)) +
  geom_vline(aes(xintercept = 0), color = "red", linetype = 2) +
  geom_density_ridges(aes(x = b, y = model),alpha = 0.75, show.legend = FALSE)  +
  scale_fill_viridis_d(option = "E") +
  labs(x = "Effect on RMSE") +
  theme(axis.title.x = element_blank())

rmse_effect_plot

```


```{r which-chosen}

mod_select <- sandbox_performance %>%
  group_by(fishery,
           experiment,
           fleet_model,
           period,
           window,
           prop_years_lcomp_data,
           variable) %>%
  filter(rmse == min(rmse))


model_selected_plot <- mod_select %>%
  group_by(fit_model) %>%
  count() %>%
  ungroup() %>%
  mutate(p_selected = n / sum(n)) %>%
  mutate(fit_model = fct_reorder(fit_model, p_selected)) %>%
  ggplot(aes(fit_model, p_selected)) +
  geom_col(color = "black", fill = "lightgrey") +
  scale_y_continuous(labels = percent, name = "Percentage Best Model") +
  coord_flip() +
  theme(axis.title.y = element_blank()) +
  labs(title = "A")


econ_selected_plot <- mod_select %>%
  mutate(econ_model = !(fit_model == "em0_lm0")) %>%
  group_by(econ_model) %>%
  count() %>%
  ungroup() %>%
  mutate(p_selected = n / sum(n)) %>%
  # mutate(econ_model = fct_reorder(econ_model, p_selected)) %>%
  ggplot(aes(econ_model, p_selected)) +
  geom_col(color = "black", fill = "lightgrey") +
  scale_y_continuous(labels = percent, name = "Percentage Best Model") +
  labs(x = "Used Economics?", y = "Percentage Best Model",
       title = "B")

model_summary_plot <- model_selected_plot + econ_selected_plot

model_summary_plot

```


```{r dtree}
decision_data <- processed_sandbox %>%
  ungroup() %>%
  left_join(mod_select %>% ungroup() %>%  select(experiment, fit_model), by = "experiment") %>%
  select(fit_model, sigma_r, rec_ac, price_cv, q_cv) %>%
  filter(!is.na(fit_model))

set.seed(42)
decision_tree = train(fit_model ~ .,
                      method = "rpart",
                      data =  decision_data,
                      tuneLength =40)

plot(decision_tree$finalModel)
text(decision_tree$finalModel, use.n=TRUE, all=TRUE, cex=0.5)
```




### Application to PISCO data

show assessment results for PISCO data

## Discussion

This isn't just a tweak to length-based assessments, foundation for a new branch of assessment.


@Dowling2014 - need to then link these to control rules for MSE

@Dowling2015,

Why is this the most groundbreaking thing ever

Highlight some commonly available economic data for places

Discuss in context of channel islands: what does predictive model say should be performance under those circumstances, relative to other assessment methods

recommend ensemble approach

tease next steps, economic only model other kinds of models, etc.

## Works Cited
