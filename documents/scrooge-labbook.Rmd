---
title: "Lab notebook"
author: "Dan Ovando"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
---

Arlington had radioacoustic tags for CPUE and viability in reserves 

Olivier critique the non-fished species aren't a good control. This is the lynchpun

* Following recommendations of "bayesian primer"; think of the biomass as a hierarchical process where the observed biomass is a random variable drawn from a true model, say log-normal, with a mean of observed numbers-at-length times weight ogive and standard deviation, where the numbers-at-length are drawn from a multinomial or something like that. Need to draw this out but that's the right way to account for all this. You'll follow a similar process in project `zissou`, but now you're more concerned with modeling the age structure itself. See Box 6.2.2

# Ideas

- Think about crowding of other vessels as a covariate in the model: both a sign of abundance but also an incentive to go elsewhere if vessels are competing/crowding each other out

## The Pivot

Zissou is now about economic priors. 

Project Summary: 

Many fisheries around the world require management guidance but lack the robust data streams that underpin state-of-the art science driven fisheries management. To resolve this problem, a large and growing suite of "data-limited stock assessments" (DLAs) have emerged, designed to provide management advice using relatively minimal data (but generous assumptions). Interestingly though, nearly all of the quantitative DLAs established rely exclusively on fish-centric data, for example length frequency distributions, catch data, or CPUE trends. While these data are clearly critical to proper understanding of a fishery's status, they also ignore the potential light that the economic history of a fishery may shed on its current biological status. This project proposes to address this challenge, by developing a quantitative method for integrating economic information into the stock assessment process. Length-based DLAs provide a useful entry point for this process. Length-based approaches often rely on equilibrium assumptions in order to "account" for recruitment fluctuations. However, this assumption is rarely justified. Attempts to relax this assumption can be hampered by the difficulties in separating changes in recruitment from changes in fishing mortality. We propose to address this challenge by utilizing economic data to set informative priors on the rate of change of fishing mortality.

Proposed Methods:

  - Collect historic data on fishing mortality *f* and economic data (e.g. prices, costs, labor) for a series of case-study fisheries
  
  - Explore ability of alternative models to utilize economic data to predict observed changes in *f*
  
  - Simultaneously explore simulation testing of evolution of *f* under different economic assumptions
  
  - Once (hopefully) a suitable functional for for estimating changes in *f* as a function of economic data is established, begin integration to DLAs
  
  - Develop model to use lengths (plus additional data as desired) to attempt to estimate a vector of *f*  and recruitment *r* over time. Use the established economic model to provide informative priors on the evolution of *f* over time. 
  
  - Simulation test model
  
  - Test against stock assessments 

Key Questions:

- What might be candidate fisheries for empirical exploration?
    - Something with long time period of *f*, including *f* from period unconstrained by management
    - Must also have access to concurrent dataset of economic data (prices, cost, labor, fleet size, etc.)
  
- Are there good examples of similar approaches in fisheries/other disciplines
    - Predicting demand? E.g. how many widgets should you stock up on this year based on widget consumer behavior last year

- What conditions would make this a non-starter
    - All models are wrong, when is this one useful?
    - What might be management scenarios that would completely invalidate this approach
  
- As an economist, what evidence would you need to see to be convinced of this idea?
  
  
# 2017-05-15

Sent email to Erin Steiner to get the data request ball rolling again https://www.nwfsc.noaa.gov/contact/display_staffprofile.cfm?staffid=2870

Also, don't forget about the regulations database at 

http://calcomfish.ucsc.edu/regulation_main.asp


# Idea for empirical model exploration

Holy cow, this could be a great way to start. Build of Cody's framework for classifying the "predictability" of a fishery. Take the RAM dynamics. Your goal is to predict the dynamics of the fishery using available data. Consider for example the change in fishing effort and or F over time. Can that change by predicted by a simple model of profits and expansion factor?

So, take the F or effort. Calcualte the changes. In each time step, calculate the "profits", as price * catch - cost * effort ^ beta, where you break the cost thing into component factors of labor and fuel, times some coefficient, plus some constant ( c= 1 + fuel * effort + labor * effort). 

See how good that is at predicting dynamics. Test out more complicated forms, a random forest approach, and a GAM based approach. This will be your evidence for the ability of economic parameters to guide stock assessment. 

You can then work on degrading the data to develop a sense for the minimum amount needed to be informative. 

As a simple one, regress delta F on delta profits, on economic indicators, and use those coefficients to describe coefficients for the prior idea. 


# 2017-07 Check in

OK, you really need to make some serious progress on this thing. 

There seem to be a few key things that you need to iron out. 

The first is what you are actually interested in modeling here. Are you trying to estimate F? Effort? Change in F? Actual biomass? SPR?

The cleanest goal still seems to be an informative prior on F based on changes in economic conditions, building off the LIME package. 

But, this should be informed by reading that summary paper that just came out...

Once you have that set out, you need to actually write out the model that you're going to use, how you plan on estimating it, and the theory underpinning it

From there you need data. That will come from two places. A really fast operating model, which I'm thinking needs to be C++ based, since it needs to be able to be spatial, with lots of complex fleet bells and whistles, and I want it to fly. 

You'll also need to try and collect empirical data from some RAM based things

Then test away. 

From there you need to write the model to estimate it

# Paper

Theory + simulation (what you were already pitching)

Theory + emprical (show in a few different places)

Theory: when would the tipping points of value be?


# March 2018 launch party

Thorson et al. 2013 seems liek a good starting place, along with Vasconcellos & Cochrane

@Thorson2013 is a decent starting place, but doesn't really give much besides literature for the general idea. But, that functional form isn't all that useful for me

Neilsen 2017 (@Nielsen2017) is that lit review of coupled ecological-economic models. Ah right, these are all MSE style models though, not assessment modules. So, a good reference for ways to model effort dynamics, but not redundant on the effort dynamics. 

So, the tricky part in this is going to be thinking about how to not cheat too much. Obviously if you model effort as a function of price and pass a prior on that the model is going to work pretty damn well. So, to make realistic you need to introduce what, the simplest would just be process error running on up to model error. 

From a starting point, it would be nice to stick with demonstrating with more or less open access dynamics, and messing around with process/observation error around that. 


So, step one is writing out your economic model. As an easy starting point, you could with that model write out what dE/Dp/c/q/b is, to think about what the shape of the priors would actually be. 

One nice wrinkle in this would be to have the assessment model assume 

# Structure of F priors

consider something of the form 

$$f_{t+1} = f_{t} + \epsilon{P_{t}}$$

To rearrange that then, we can say that 

$$\Delta^f_{t} = \epsilon{P}_{t}$$

Expanding that out then we get

$$\Delta^f_{t} = \epsilon(p_{t}q_{t}E_{t}b_{t} - cE^{b})$$

And you could start taking derivatives from there... if you want to think about an x unit change in delta F as a function of a Y change in a covariate. 

Or, and this is interesting, you could think about it like elasticities? hmmm

One may to make this a bit more tractable. Suppose that there is some max fleet expansion factor, e.g. 120%, and some min, like 0%. So, in any given year you could either expand f to some max value, or decrease it to zero if you wanted to. Then, what you estimate is the % of this expansion percentage. So a massive increase in prices, decreases in cost, increase in tech, produces a massive/small part of that max change

Make yourself a damn shiny app to think this through. as well, or at least make some plots and see what happens. 

could use `dagR` to draw model graphs

So, a relatively simple step would just be to think about each of the parameters in here as a constant besides the variable that you "have" and take derivatives. 

i.e. the df/dp would be qEb, df/dq would be pEB, df/dc would be -E^b^, etc. 

This has some appeal, the problem though is it's trickier to think about what happens if you have arbitrary numbers and combinations of parameters, e.g. what happens if both p and q are changing? You can certainly look up the math on that one... joint derivative?

An alternative though would be a much more structural approach. 

Suppose that you have t years of length frequency data. Right now, let's think about how LIME/LBSPR wowrks. 

It estimates a vector of recruits and f's and selectivities and then compares the observed vs predicted length distributions. 

Now let's focus on the f priors. Right now, it's just saying that the f in any given year has the f in the last year as a prior. 

Now suppose that instead of that you let f evolve through some bioeconomic driver. 

You start the population at r0. Now, you let the F's evolve according to your open access model, holding constant the things that hold constant, and changing the things you have some prior knowledge of. That now gives you a vector of F's, which combined with your recruitment estimates fives you an expected length distribution. 

Now, you could either treat those F's as priors on the latent F's estimated by the model, or maybe better, estimate F deviates with means based on the economic model!


So now, you'd estimate deviates with mean 0 and some standard deviation, around the bioeconomically estimated F's. So in that sense the bioeconomic model is still a prior on the final F... booya? 

The reall nice thing abotu this is that it abstracts to any time series or combination of economic parameters that you have, and any functional form for F that comes in!!!!!!

As a killer extension, you could then repeat the process under alternative model specifications for the bioeconomic side, and pick the model with the most support!. 

Oh I really like this. 

So what needs to happen to make this happen. 

1. You need to modularize the "fleet model" side of things, rather than just if toggles in the code. The end goal is that sim_fishery can now take economic "data" as inputs

2. You need to revisit your length-comps sampling and make sure that that thing works well. 

3. That's the core barrier at this point. Once you have that, you can use spasm to simulate length comps over time given those parameters, and then you pass that on to scrooge

BOOYA. 



# Sea Grant Abstract Update

Many fisheries around the world require management guidance but lack the robust data that underpin state-of-the-art fisheries management. To resolve this problem, a large and growing suite of "data-limited stock assessments" (DLAs) has emerged, designed to provide management advice using relatively minimal data (but generous assumptions). Interestingly though, nearly all of the quantitative DLAs established rely exclusively on fish-centric data, for example length frequency distributions, catch data, or CPUE trends. While these data are clearly critical to proper understanding of a fishery's status, the economic history of a fishery can also shed light on its current biological status. This project proposes to address this challenge, by developing a quantitative method for integrating economic information into the stock assessment process. Length-based DLAs provide a useful entry point for this process. Length-based DLAs either rely on equilibrium assumptions or are faced with the challenge of disentangling trends in fishing mortality from trends in recruitment. We demonstrate how integration of often available data on the economic history of a fishery, such as prices, costs, and labor, can improve the performance of these length-based DLAs. Our expected result is a user-friendly tool for helping communities utilize length and economic data to better manage their fisheries.

# model development

This section is for sketching out the structure of the scrooge estimation model. Here goes nothing!

We're going to stary by writing out the likelihoods and components for the model, as well as the DAC for the model. 

Once you've got that working, it's off to stan for fitting fun! Once that works you can code up a TMB version as well, but I feel more confident in diagnosing STAN


## DAC

## Likelihood

The only true "likelihood" in here is relating the observed vs predicted length frequencies. 

This will likely be some sort of multinomial likelihood. LIME uses Dirichlet-multinomial log-likelihood, think I'll just stick the multinomial for right now to make life a little simpler to start. 

So, this will look like 

$$l_{b,y} \sim multinom(p_{b,t}) $$

where p~b,y~ is the probability of being observed in length bin *b* in time *t*. Stan aslo provides upporto for the dirichelet but let's come to that later. 

So, from there

$$p_{b,y} = \frac{n_{b,t}}{\sum_{1:B}n_{b,t}}$$


the number in each length bin is then your growth function

$$ n_{b,t} = g(f_t,r_t,s_b,bio)$$
Where *bio* is biological data (growth, mortality, etc)

recruitment will be mean BH/ricker, with some sigma

$$r_{t} = bh(ssb_{t}, bio)e^{rdev_t - \sigma_r^2/2} $$

and 

$$ rdev \sim normal(0,\sigma_r)$$

and borrowing from LIME

$$ log(\sigma_r) \sim normal(0.7,0.2)$$

Now, the fun part. estimating F goes actually pretty damn similarly. The current idea is to estiamte F deviates, with the prior being the bioeconomic model. It gets a little tricky thinking about scale here though. 

Suppose that you hold everything constant and just let the bioeconomic model evolve over time. The problem there is that that would require tuning of the qs and things to get the f's in the right units, even if the trend is in the right ballpark. i.e., to say that f is mean bioeconomic f times some deviate, where teh deviate is mean 0, then on avereage the fleet model needs to be getting the scale of the f's correct. 

What if instead you leave the fleet model estimating latent F's, but you put a prior on the change in F based on the economic model. 

Jim does something like 

$$f_t \sim normal(f_{t-1},\sigma_f)$$

Now, suppose that you add in a little wrinkle


$$f_t \sim normal(\Delta_{f}f_{t-1},\sigma_f)$$

Where 

$$\Delta_{f} = \frac{\hat{f_t}}{\hat{f_{t-1}}} $$

where 

$$\hat{f_{t}} = q_tE_t$$

and 

$$E_t  = E_{t-1} + \theta\Pi_{t-1}$$

Where finally 

$$\Pi_{t} = p_tq_tE_tb_t - cE_t^\beta $$
sweeeeet. 

Now, one thing that is a royal pain in the ass in here is $\theta$: Since it scales to profits it's pretty hard to make generic. Do you want a \$1 increase in profits to equal a 1 unit increase in effort? a 1000 increase in effort?

It would be nice to come up with a way to make this a bit cleaner. 

A right, one night thing in here is that because you're dealing with delta effort, you can set the effort itself at some arbitrary level, as well as the r0 so that at least helps you scale things somewhat, given reasonable prices and costs, all of which can be scaled to be relative. 

i.e. you set p,q,E,b,c,beta to baseline levels, and then just amplify those baselines based on data. 

From there you could mess around to set theta at some reasonable level. 

You could also actually think about allowing $\theta$ to be a free parameter with highly informative priors? 

What does sscom do about this?

ah right, they do that by working with biomasses instead of profits, since a given biomass would produce bionomic equilibrium. The problem there though is that that assumes that price etc. are constant, which you implicitly want to change. 

So maybe for now play with what a reasonable level of theta would be such that effort dynamics given yourbase values are somewhat reasonable. 

Use something like ssb0

find $\theta$ such that 

$$1 - \frac{E_{t0 + 1}}{E_{t0}} \sim target \sim 0.25  $$

and 

$$E_{t0 + 1} = E_{t0} + \theta(pqE_{t0}B_{t0} - cE_{t0}^\beta)$$

which would be

$$ target = \theta(pqB_{t0} - cE_{t0}^{\beta - 1})$$

and then 

$$\theta = \frac{target}{(pqB_{t0} - cE_{t0}^{\beta - 1})}$$

```{r}
p <- 1
q <- .01
E <- 100
b <- 1000
cost <- 1
beta <- 1.3
target <- 0.25

profits <- p*q*E*b - cost * E^beta

theta = target / (p * q * b - cost*E^(beta - 1))

E2 = E + theta * profits

```
Which works so long as the fishery is profitable at B0, which makes sense, why would you model something that wasn't profitable at the getgo. So, set a stop that would say fishery is unprofitable at virgin biomass if theta comes up nevative. 

You could then either set that theta as data, or let be a parameter with that theta as a highly informative prior. 

Let's go back to spasm and make that adjustment

## test case


```{r}
library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

fish <-
create_fish(
query_fishlife = T,
mat_mode = "length",
max_age = 30,
time_step = 1
)

fleet <- create_fleet(fish = fish)

manager <- create_manager(mpa_size = 0)

fleet <-
spasm::update_fleet(
fleet = purrr::list_modify(
fleet,
fleet_model = "open-access",
initial_effort = 1,
theta = 0.01,
cost = 20,
price = 1,
target_catch = 100,
sigma_effort = 0,
length_50_sel = 25,
theta_tuner = 0.5
),
fish = fish
)

sim <- spasm::sim_fishery(
fish = fish,
fleet = fleet,
manager = manager,
num_patches = 1,
sim_years = 100,
burn_year = 50,
time_step = fish$time_step
)

sim %>%
group_by(year) %>%
summarise(ssb = sum(effort)) %>%
ggplot(aes(year, ssb)) +
geom_line() +
geom_point()

length_and_age_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age)


length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = 2
)

length_at_age_key <- length_at_age_key %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_and_age_comps <- length_and_age_comps %>%
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'population',
percent_sampled = 1,
time_step = fish$time_step
)
))

length_and_age_comps <- length_and_age_comps %>%
mutate(catch_ages = map(
catch_length,
~ length_to_age(
length_samples = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
max_age = fish$max_age,
min_age = fish$min_age,
time_step = fish$time_step
)
))

length_and_age_comps %>%
filter(year == max(year)) %>%
select(catch_length) %>%
unnest() %>%
filter(length_bin < 150) %>%
ggplot() +
geom_col(aes(length_bin, numbers)) +
geom_vline(xintercept = fish$linf) +
geom_vline(xintercept = fish$length_at_age)
```

Not bad. Might need to go back and double check some of your aging ideas, but it mostly works. If you were catually that close to the true age structure in real life you'd be very excited. 

Interesting, something is going wrong in the length-to-ages at the far right end. Let's take a brief look at this some more. 

OK fixed somewhat, but going to ignore the problems that persist there since who really cares about age comps for this assessment: it's length based. 


## Testing stan fit

```{r}

scrooge_data <- list(
  nt = 20,
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
)

test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1)

check<- rstan::extract(test_scrooge)

test_numbers <- array_branch(check$n_ta,1)

test_numbers <- data_frame(thing = map(test_numbers, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_numbers <- test_numbers %>% 
  filter(iteration == 1) %>% 
  select(-iteration)

test_ssb <- array_branch(check$ssb_ta,1)

test_ssb <- data_frame(thing = map(test_ssb, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_ssb <- test_ssb %>% 
  filter(iteration == 1) %>% 
  select(-iteration)

test_catch <- array_branch(check$cn_ta,1)

test_catch <- data_frame(thing = map(test_catch, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_catch <- test_catch %>% 
  filter(iteration == 1) %>% 
  select(-iteration)


total_ssb <- rowSums(test_ssb)

recruits <- test_numbers$V1

plot(total_ssb, lead(recruits))

test_catch %>% 
  slice(20) %>% 
  gather(age, numbers) %>% 
  mutate(age = str_replace_all(age,"\\D","") %>% as.numeric()) %>% 
  ggplot(aes(age, numbers)) + 
  geom_col()


test_length <- array_branch(check$n_tl,1)

test_length <- data_frame(thing = map(test_length, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_length <- test_length %>% 
  filter(iteration == 1) %>% 
  select(-iteration)




a = sim %>% 
  filter(year == min(year)) %>% 
  select(age, numbers_caught)

b = t(a$numbers_caught) %*% as.matrix(length_at_age_key)

test_catch %>% 
  slice(1) %>% 
  gather(age, numbers) %>% 
  mutate(true_numbers = as.numeric(a$numbers_caught)) %>% 
  mutate(age = str_replace_all(age,"\\D","") %>% as.numeric()) %>% 
  gather(source, numbers,-age)  %>% 
  group_by(source) %>% 
  mutate(snumbers = numbers / sum(numbers)) %>% 
  ggplot(aes(age, snumbers, fill = source)) + 
  geom_col(stat = "identity", alpha = 0.5, position = "dodge")



```

God damn dude, nicely done. Now, add in generation of length comps. This is going to be fuuuuun. 

### generating length comps

For now, don't worry about survey selectivity of the catch itself. For now, just focus on generating a length distribution from the catch. In theory, you're going to pass samped data, which should be the "true" length distribution of the catch. You're now going to try and match those length comps with the length comps of the catch that would result from your model. 


So, the simplest thing to do would be to generate in R a matrix that is rows age, columns probability in each length bin at age. 

Then, you just take the catch at age, and multiply by the proportion in each age at eathc length bin, then sum the columns. 

```{r}

set.seed(42)
n_at_a <- matrix(rnorm(31,100))

d =  t(n_at_a) %*% (as.matrix(length_at_age_key))


b =  t(as.matrix(length_at_age_key)) %*% n_at_a


as.matrix(length_at_age_key) %*% t(n_at_a)

```

## Setting up likelihoods

OK, let's pass in the lengths!


```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

rstan_options(auto_write = TRUE)

fish <-
create_fish(
query_fishlife = T,
mat_mode = "length",
max_age = 30,
time_step = 1
)

fleet <- create_fleet(fish = fish)

manager <- create_manager(mpa_size = 0)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
fish = fish,
fleet = fleet,
manager = manager,
num_patches = 1,
sim_years = 25,
burn_year = 50,
time_step = fish$time_step
)


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
estimate_recruits = 0,
length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year)) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")


# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison
  
  
```

hell yes. model now nails F perfectly in a perfect world with perfect data. 

Let's see how much of that was just the warmup...

```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

rstan_options(auto_write = TRUE)

fish <-
create_fish(
query_fishlife = T,
mat_mode = "length",
max_age = 30,
time_step = 1
)

fleet <- create_fleet(fish = fish)

manager <- create_manager(mpa_size = 0)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 10,
      cost = 2,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )

sim <- spasm::sim_fishery(
fish = fish,
fleet = fleet,
manager = manager,
num_patches = 1,
sim_years = 25,
burn_year = 50,
time_step = fish$time_step
)


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
  estimate_recruits = 0,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <-
  rstan::stan(
  file = here::here("scripts", "old-scrooge.stan"),
  data = scrooge_data,
  chains = 1,
  refresh = 10,
  cores = 1,
  iter = 4000,
  warmup = 2000
  )
  
check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year)) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")


# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison
  

```


IN the meantime, best damn idae I've had in a while: train a model to predict accuracy for F, SPR, trend in F, etc, across the range of scenarios that you try!!!!!

So, you'd plug in your fishery characteristics, and the model would give an idea of how accurate it is likely to be!!

Awesome idea. 

## Adding in recruitment estimation

```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

rstan_options(auto_write = TRUE)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1
  )



manager <- create_manager(mpa_size = 0)

fleet <- create_fleet(fish = fish, sigma_effort = 0)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      length_50_sel = 50,
      theta_tuner = 0.1,
      beta = 1.3
    ),
    fish = fish
  )


set.seed(42)

sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = manager,
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(tb = sum(biomass)) %>% 
  ggplot(aes(year, tb)) + 
  geom_point()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")




```


## adding in econ

OK, here we go. Let's get the econ module up and running. 

Now, one slightly weird thing in here. You need to think about what to do about effort. If you're passing in p,c,q, and taking the catch from the model, profits will then depend on effort....

But you need to rationalize the effort based on the other parameters to prodice the catch. 

So, what if you do something like this. 

Take as your starting point the current model, with no other information. 

With that, the most reasonable prior certainly seems like fishing mortality this year will be similar to fishing mortality last year. 

That seems more reasonable that imposing a random oscilation on it. 

Now, suppose that you had the complete price, cost, and technological history of the fishery in hand, as you do here. What woud your prior be then? In the current case, you could of course feed it in, in which case your prior on the evolution of the fishery would depend on the initial profits, the degree of F, etc. In other words, it seems entirely plausible that if you just feed in a bioeconomic model, your prior could very easily be off sync with the truth, e.g. if you catch the fishery on a downswing in profits but you start it assuming an upswing?

But let's follow that thought exercise, what would it look like?

if you have p,c,q over time (and assume for now they are constant), is there value in making the prior the bioeconomic delta? to get the whole cycle going, you need to make some judgement about initial profits. If you assume zero profits from the getto, then the delta is 1, and your prior is back to the current prior (f is just f last year). 

If you assume some positive initial profits, then the cycle can get going until it hits equilibrium. 

So, the question of what those initial profits are is of course critical: are they going up or down? If ther are profits, then you predict fishing pressure to go up next year. if negative, you expect fishing pressure to decline. 

Under your old idea, you would pick a totally arbitrary effort level and stick with that, with the idea being that you're only focusing on the change in fishing mortality. But, that still requires an assumption about how profitable things are. 

one idea there would be to pick a totally arbitrary effort level, apply your data, and calculate catch and profits based on the biomass from the model and those thing. So, in that world, the effort (and f) is independent of the f's estimated by the model. 

Which is actually better right? Otherwise it's a little weird, your effort is now coming from your latent variable, not vice versa, i.e. f is a prior on itself. Which is actually what you're already doing, but with more of a functional form. 

Another idea would be to assumine something about intiial profitability, say by profits/effort, where we call effort a day of fishing or something. 

You could divie across, to get p/e, and then solve for e such that p/e is some number that you like. In that world you'd get p/e, price for a given unit of catch, cost per day of fishing, q.... which no one would be able to tell you q, so you'd have to be back to f....  you could play with it. 

A third idea is to make everything in terms of deltas: your prior is that f is last years f unless something changes in p,q, or c, or E. Under that model, you'd calculate the effort in a given time step using the last time steps p,q,c,E, and then calculate it using the changed p,q,c,E, and apply that change to f (up by 10%, down by 25%, etc.)

To summarize: 

1. Take your "data" on p,q,c, along with a random effort, and project the evolution of effort using the biomass from the model, and use the deltas from this to inform your prior on f. has the appeal of being more independent of f, though not totally through the biomass link, and makes it pretty robust to just plugging and chugging: add your timeseries of "data" and let it go to down

2. Bring the f's a bit more specifically into the equation. Assuming something about q, find the E that at least gives you the observed catch. Hmm though by that logic E is just a linear transformation of F in each time step, and as such is useless. You could "seed" the system there to get the scale right and then let it evolve? COuld also sove for profits per unit effort, instead of catch

3. Make it all about change: your prior is the default prior unless there is a shock to prices, q, or c. i.e. if price doubles youd expect effort to double relative to what it would have been without the price increase. This seems like the cleanest idea.... you still use the model to get at joint changes and non-linear effects, but you're not assuming much about where you are in the cycle. 

Think of it this way, if I tell you the p, c, q, and they are all constant, and ask me what I think is happening to effort, the honest answer at any given timestep is that I don't know. It could be going up, it could be going down, it could be constant, depending where on the equilibrium path I think I am. The only real "prior" that I have then is that it oscilates, so I could pass those to the bioeconomic model and hope that I get the osscilations right. The only way this really adds value then is with some other piece of information, like an idea of profits per trip. Or, maybe through some linkage through the evolution of the system, but that seems more complicated. 

So, given just that above information, it seems more logical to go with the shock-based prior: If you tell me that price doubled last year, that does give me some idea of what might be happening to effort (though need to think through supply/demand dynamics here). It would be nice to generalize the case here, which actually I think you can do. If you evolve the system and just calculate the counterfactual difference every year, it should be 1 if nothing changes, so your prior defaults back to the basic one, but change when there is a shock. Let's explore this one for the week. To put it more clearly, I think you have a stronger prior on how thins should change in response to schocks than you do on how effort should evolve per-say. 

Suppose you knew nothing else, but that price had doubled. What would you expect to happen?


```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

set.seed(42)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    linf = 100,
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1,
    price_cv = 2,
    price_ac = 0
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0.75,
  cost_ac = 0.2,
  q_cv = .1,
  q_ac = .85
)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0),
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(f)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(cost)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")




```


booyakasha. So what do to do with this. 

I like the idea of starting with the delta model. So there, you want to quantify the change in F relative to the F that would have been expected if prices/costs hadn't changed. 

How to do that?

In each time step, you're calculating profits in that time step. you could calculate profits in that time step based on the conditions on the current year, and the conditions in the last year. 

When does that affect your prior?

Under the bioeconomic model you're working with, effort in the current year responds to profits in the prior year. Or specifically, $f_{t + 1} = f_t + profits_t$

```{r}


```

Right, so this is informative if you have an idea of the effort and biomass. But, if you're going with the shock model, suppose now that you have the case you put above, where things change. How do you incorporate that. I think the answer is now you need three years of data. 

Effort in year t is delta percent different than the delta in year t that would have been expected without the shock. 

$$f_t + 1 \sim normal(f_t * \Delta)$$


so what is $\delta$?

delta is new f resulting from the current prices and costs / new f resulting from the last prices and costs. 


in practice then. 

$$\frac{f_t + \theta(p_t^sq_tE_tB_t  - c_t^sE_t^\beta)}{f_t + \theta(p_tq_tE_tB_t  - c_tE_t^\beta)}$$

Then $f_t + \theta$ drops out, as does anything els that doesn't change (I think), doesn't really matter, since the nice thing is if things don't change this is 1

So to operationalize this, you need to track changes in the things of interest and calculate two profits in each time step: the profits using the current values and the profits using last time steps values, focusing in on profits and costs at the moment. 

For now, assume a beta of 1, since if you do that then effort drops out, and the change is all dependent 

```{r}
library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

set.seed(42)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    linf = 100,
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1,
    price_cv = 2,
    price_ac = 0
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0.75,
  cost_ac = 0.2,
  q_cv = .1,
  q_ac = .85
)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0),
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(f)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(cost)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

price_and_cost_history <- sim %>% 
  group_by(year) %>% 
  summarise(price = unique(price),
            cost = unique(cost)) %>% 
  gather(variable, value, -year) %>% 
  group_by(variable) %>% 
  mutate(lag_value = lag(value,1)) %>% 
  ungroup() %>% 
  mutate(lag_value = ifelse(is.na(lag_value), value, lag_value))

price_history <- price_and_cost_history %>% 
  filter(variable == "price")

cost_history <- price_and_cost_history %>% 
  filter(variable == "cost")


scrooge_data <- list(
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  price_t = price_history %>% select(value, lag_value),
  cost_t = cost_history %>% select(value, lag_value),
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")
```

```{r}

p1 <-  1
p2 <- 3.46

c1 <- 1
c2 <- 1

q <- .0001

beta <- 1.3


test <- expand.grid(effort = 1:100, biomass = seq(1000,10000, length.out = 100)) %>% 
  mutate(prof1 = p1 * q *effort * biomass - c1 * effort ^ beta,
         prof2 = p2 * q * effort * biomass - c2 * effort ^ beta) %>% 
  mutate(delta = prof2 - prof1)



test %>% 
  ggplot(aes(effort, biomass, fill = delta) ) +
  geom_raster() + 
    scale_fill_viridis_c()

```

OK the net different has much better properties than the damn delta percent. So, you're back to to modifying by addition. hooray. take a look back at SSCOM and see how he dealt with that. 

What you can at least do is back out the efforts based on the q... and to some extent this should just give a fucntional form to the evolution. Let's try it and see what happens. Where "it" refers now to a max/min absolute change in f, calculated from the bioeconomic model. 

See how that goes, and if the answer is badly, so be it, back to the drawing board. 

Could think about some kind of distribution to draw from based on plausible states of the world.... ugh. well this is why it hasn't been done!

So, where are we now?

One idea: Start with a seed effort that rationalizes the initial catch (which is made up anyway). 

Now, in one world, you have a sort of "parallel" stock trajectory, where you let effort and F, and subsequently biomass evolve on their own according to the profit function you provide. 

You then calculate the shocks and update your priors on F, which affect the "actual" stock dynamics. 

In the other example, you utilize the estimated F's by the model, and just keep track of the change in F that would have been expected under a shock. So, if in time step 2, price doubled, and you think that means a doubling in F, updating that delta. 

So what that means is that you have a vector of deltas, and in each time step, you're simply estimating that delta. 

That seems like the slightly simpler and approach, and the one that seems less prone to just being way off by virtue of model misspecification. With the other approach, if you're a) adding in a lot of computation time by running a literal parallel population model and b) since all you're utilizing is the shocks,but since the degree of the shock depends on E and B, if your E and B are wildly off from the population model expected E and B

so, the problem now is how to get E to rationalize all this.... 

poop. If you pick a random R, then your catches don't match up. If you stick with F, then you need to translate cost per unit F instead of cost per unit E.

So in a perfect world in every time step you need the E that produces the observed catch... BALLS. Simplest would be a bisection method?

What about estimating Effort and effort devs and translating to F to save time and effort... intereating. 

So what would that look like. THe only reason you're working with F right now is that it makes the search space a bit neater, between 0 and 10. 

What if you worked in raw effort instead. Fiven that you're fixing the size of the population at some arbitraty level, and given that you're going to specify q as well, it wouldn't be that hard to 

So in that world, you estimate effort_t, with lower of 0 and upper of some much bigger number. 

Alternatively, and this would probably be numerically simpler, you estimate a mean effort, and then a vector of effort_dev_t that are deviates from that effort. with mean 0 and some sigma. So you're just shifting the model over to effort based instead of F based. 

So what would that look like

in the simpler case....

 $$effort_t \sim unif(effort^{min}, effort^{max})$$
 
 AHA, no, here's what you do, you keep the uniform multiplier from 0 to like 10, and you just multiply that by your base effort factor, that instead of estimating (for now) you set at some arbitrary level, like the effort that makes f = m, which is just m / q!!!! BOOM. you could make it a free parameter later, but doesn't make a lot of sense since it's just helping scale the system. 
 
 $$ effort_t \sim normal(effort_{t-1}, \sigma_{effort}) $$




If that for some reason doesn't work, then you stick with estimating F's, and have to find the E that produces the catch given the other parameters using probably a bisection method. 


```{r}
library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

set.seed(42)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    linf = 100,
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1,
    price_cv = .1,
    price_ac = 0
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0.75,
  cost_ac = 0.2,
  q_cv = .1,
  q_ac = .85
)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0),
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(f)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(cost)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

price_and_cost_history <- sim %>% 
  group_by(year) %>% 
  summarise(price = unique(price),
            cost = unique(cost),
            q = fleet$q) %>% 
  gather(variable, value, -year) %>% 
  group_by(variable) %>% 
  mutate(lag_value = lag(value,1)) %>% 
  ungroup() %>% 
  mutate(lag_value = ifelse(is.na(lag_value), value, lag_value))

price_t <- price_and_cost_history %>% 
  filter(variable == "price")

cost_t <- price_and_cost_history %>% 
  filter(variable == "cost")

q_t <- price_and_cost_history %>% 
  filter(variable == "q")


scrooge_data <- list(
  economic_model = 1,
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  price_t = price_t %>% select(value, lag_value),
  cost_t = cost_t %>% select(value, lag_value),
  q_t = q_t %>% select(value, lag_value),
  beta = 1.3,
  base_effort = fish$m / mean(q_t$value),
  length_50_sel_guess = fish$linf / 2,
  delta_guess = 2,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

econ_fplot <- fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")

profit_shocks <- array_branch(check$profit_shock_t,1) 

total_effort <- array_branch(check$total_effort_t,1) 


profit_shocks <- data_frame(thing = map(profit_shocks, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest() %>% 
  mutate(type = "profits")

total_effort <- data_frame(thing = map(total_effort, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest() %>% 
  mutate(type = "effort")

thing <- profit_shocks %>% 
  bind_rows(total_effort) %>% 
  spread(type, value) %>% 
  mutate(relative_thing = profits / effort)

thing %>% 
  ggplot(aes(x = year,y = profits / sd(profits))) +
  geom_point()




```

OK! Time to set this thing up, concept is up and running. 

# check in with Jason

Damn good point, this thing actually provides estimated catch histories based on the lengths. Catch-only model actually estimates SPR (simple stock synthesis, since it's age structured catch dynamics), but SPR doesn't provide catch

measure performance as a function of bias (i.e. identify probability of estimating F that's waaaay too low)

fold LIME into SS, a future thing would be to take this concept and fold into SS

Check with an economist or two on the shock idea 

# check in with merril

Multifleet stuff is the the biggest sticking point so far

Incorporating uncertainty in life history 

Seems like my fits 

# 2018-03-22 Thoughts

SO where to now? You have a model that fits and seems to do pretty well under some circumstances. 

You can obviously tinker till the cows come home, what's an effective strategy to move forward?


Jason had two important outputs that would be good to build in: estiamted catch trajectory and bias (how often do you think you're overfishing when you're under, and vice versa)


It would be good to have something to benchmark performance against. 

To that end, I think the best option to add in at this point would be scrooge-lite (i.e. knock out the economic prior), LIME, and LBSPR, and measure performance of those and compare. 

That gives you something to push against in terms of performance: If you're already doing way better than the others one no need to go too crazy. 

From there, try and fit your performance model. 

That gives you a nice complete draft of results to work with over the next month. 


# Testing LIME

from lime::LIME_example.R

```{r}
library(LIME)

fish <- vfo$prepped_fishery[[1]]$fish

fleet <- vfo$prepped_fishery[[1]]$fleet

scrooge_lh <- create_lh_list(vbk= fish$vbk,
					 linf= fish$linf,
					 t0= fish$t0,
					 lwa= fish$weight_a,
					 lwb= fish$weight_b,
					 S50= fleet$length_50_sel,
					 S95=fleet$length_95_sel,
					 selex_input="length",
					 selex_type=c("logistic"),
					 M50= fish$length_50_mature,
					 M95= fish$length_95_mature,
					 maturity_input="length",
					 M= fish$m,
					 binwidth=1,
					 CVlen= fish$cv_len,
					 SigmaR= fish$sigma_r + .001,
					 SigmaF= fleet$sigma_effort + .001,
					 SigmaC=0.2,
					 SigmaI=0.2,
					 R0= fish$r0,
					 qcoef=1e-5,
					 start_ages=0,
					 rho=0,
					 nseasons=1)

true <- generate_data(modpath=NULL,
					  itervec=1,
					  Fdynamics="Endogenous",
					  Rdynamics="BH",
					  lh=scrooge_lh,
					  Nyears=20,
					  Nyears_comp=20,
					  comp_sample=200,
					  init_depl=0.5,
					  seed=123)

temp_LF_matrix <- vfo$prepped_fishery[[1]]$length_comps


LF_matrix <- temp_LF_matrix %>% 
  as.matrix()

rownames(LF_matrix) <- LF_matrix[,"year"]


LF_matrix <- LF_matrix[, -c(1)]

scrooge_data_LF <-
  list("years" = 1:nrow(LF_matrix), "LF" = LF_matrix)
  

start <- Sys.time()
res <- run_LIME(modpath=NULL,
				lh=scrooge_lh,
				input_data=scrooge_data_LF,
				est_sigma="log_sigma_R",
				data_avail="LC",
				newtonsteps=3)
end <- Sys.time() - start


check <- res$df

## check for other issues
issues <- res$issues

## check TMB inputs
Inputs <- res$Inputs

## Report file
Report <- res$Report

## Standard error report
Sdreport <- res$Sdreport

##----------------------------------------------------
## Step 4: Plot fits
## ---------------------------------------------------
## plot length composition data
plot_LCfits(LFlist=list("LF"=LF_matrix),
			Inputs=Inputs,
			Report=Report)

# plot_output(Inputs=Inputs,
#             Report=Report,
#             Sdreport=Sdreport,
#             lh=scrooge_lh,
#             True=1,
#             plot=c("Fish","Rec","SPR","ML","SB","Selex"),
#             set_ylim=list("Fish" = c(0,1), "SPR" = c(0,1)))

true_f <- vfo$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(f = unique(f))

true_f$predicted_f <- Report$F_y

# reportnames <- Sdreport$value %>% names()

# Sdreport$sd[str_detect(reportnames,"F_t")]

predicted_f <- data_frame(f = Report$F_y)

true_f %>% 
  ggplot() +
  geom_point(aes(year,f,color = "True")) + 
  geom_line(aes(year, (predicted_f),color = "Predicted"))

temp_LF_matrix %>% 
  gather(length, numbers, -year) %>% 
  mutate(length = as.numeric(length)) %>% 
  ggplot(aes(length, numbers, fill = factor(year))) + 
  geom_density(stat = "identity", alpha = 0.5)
```

Innnnteresting that doesn't work well at all. Let's try and flip it around and use LIME to predict F from SCROOGE


```{r}


lh <- create_lh_list(vbk=0.21,
                     linf=65,
                     t0=-0.01,
                     lwa=0.0245,
                     lwb=2.79,
                     S50=20,
                     S95=26,
                     selex_input="length",
                     selex_type=c("logistic"),
                     M50=34,
                     M95=NULL,
                     maturity_input="length",
                     M=0.27,
                     binwidth=1,
                     CVlen=0.1,
                     SigmaR=0.7,
                     SigmaF=0.25,
                     SigmaC=0.2,
                     SigmaI=0.2,
                     R0=1,
                     qcoef=1e-5,
                     start_ages=0,
                     rho=0,
                     nseasons=1)

true <- generate_data(modpath=NULL,
					  itervec=1,
					  Fdynamics="Endogenous",
					  Rdynamics="BH",
					  lh=lh,
					  Nyears=20,
					  Nyears_comp=20,
					  comp_sample=200,
					  init_depl=0.5,
					  seed=123)

LF_matrix <- true$LF

data_LF <- list("years"=1:true$Nyears, "LF"=LF_matrix)


start <- Sys.time()
res <- run_LIME(modpath=NULL,
				lh=lh,
				input_data=data_LF,
				est_sigma=c("log_sigma_R"),
				data_avail="LC",
				newtonsteps=3)
end <- Sys.time() - start


fish <-
  create_fish(
    scientific_name = NA,
    min_age = 0,
    max_age = lh$AgeMax,
    linf = lh$linf,
    vbk = lh$vbk,
    t0 = lh$t0,
    weight_a = lh$lwa,
    weight_b = lh$lwb,
    length_50_mature = 34,
    length_95_mature = 36,
    length_mature = 34,
    m = lh$M,
    cv_len = lh$CVlen,
    query_fishlife = F,
    mat_mode = "length",
    time_step = 1,
    sigma_r = lh$SigmaR,
    price = 1,
    price_cv = 0,
    price_ac = 0,
    r0 = 100000
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0,
  cost_ac = 0,
  q_cv = 0,
  q_ac = 0,
  fleet_model = 'constant-effort',
  target_catch = 10,
  length_50_sel = lh$S50,
  delta = lh$SL95 - lh$S50
)

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = 1.5
) %>%
  ungroup() %>%
  select(age, length_bin, p_bin) %>%
  spread(length_bin, p_bin) %>%
  select(-age)

length_comps <- LF_matrix %>% 
  as.data.frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  select(year, everything())

length_comps %>% 
  gather(length,numbers, -year) %>% 
  mutate(length = as.numeric(length)) %>% 
  ggplot(aes(length,numbers, fill = factor(year))) + 
  geom_density(stat = "identity", show.legend = F)

price_t = data.frame(value = rep(2,nrow(length_comps)), lag_value = rep(2,nrow(length_comps)))
  
cost_t = data.frame(value = rep(1,nrow(length_comps)), lag_value = rep(1,nrow(length_comps)))

q_t <- data.frame(value = rep(0.001,nrow(length_comps)), lag_value = rep(0.001,nrow(length_comps)))

scrooge_data <- list(
      economic_model = 1,
      estimate_recruits = 1,
      length_comps = length_comps %>% select(-year),
      length_comps_years  = length_comps$year,
      price_t = price_t %>% select(value, lag_value),
      cost_t = cost_t %>% select(value, lag_value),
      q_t = q_t %>% select(value, lag_value),
      beta = 1.3,
      length_50_sel_guess = fish$linf / 2,
      delta_guess = 2,
      n_lcomps = nrow(length_comps),
      nt = length(length_comps$year),
      n_ages = fish$max_age + 1,
      n_lbins = ncol(length_at_age_key),
      ages = 1:(fish$max_age + 1),
      mean_length_at_age = fish$length_at_age,
      mean_weight_at_age = fish$weight_at_age,
      mean_maturity_at_age = fish$maturity_at_age,
      m = fish$m,
      h = fish$steepness,
      r0 = fish$r0,
      k = fish$vbk,
      loo = fish$linf,
      t0 = fish$t0,
      length_at_age_key = as.matrix(length_at_age_key)
    )

  fit <-
    rstan::stan(
      file = here::here("src", "scrooge_v3.0.stan"),
      data = scrooge_data,
      chains = 1,
      refresh = 25,
      cores = 1,
      iter = 4000,
      warmup = 2000,
      control = list(
      adapt_delta = 0.8
    ))
  
  processed_fit <- process_scrooge(fit = fit)

  true_f <- data_frame(year = 1:length(true$F_t), true_f = true$F_t)
  
processed_fit$f_t %>% 
  left_join(true_f, by = "year") %>% 
  ggplot() + 
  geom_boxplot(aes(year,value, group = year)) +
  geom_point(aes(year, true_f), color = "red") + 
  lims(y = c(0,2))


```

Hell yes! scrooge predicts LIME. 

# Getting scrooge to work on VFO

OK, so you can see that scrooge works when it works, but gets waaay off track sometimes. 

Taking the VFO example, there's a few telltale signs: the effort distributions are crazy right tailed, but the effor deviations are getting stuck near zero. it's also near efforts of zero where the divergences are happening  

```{r}
sel_at_age <- vfo$prepped_fishery[[1]]$fleet$sel_at_age 

length_at_age <-  vfo$prepped_fishery[[1]]$fish$length_at_age 

plot(length_at_age, sel_at_age)

```

It's nailing selectivity at least. 

So, there's a few potential culprits here. Easiest answer is that it's the starting guess, and the model would do better if you let it run out longer. 

You could test that by feeding it the "right" starting guess out the bat and see if that gets it. If it does, cool, then you can work on helping the model find that starting guess. 

If that doesn't work, then you need to revisit the model structure itself, maybe by testing estimating F instead of effort madness. 
```{r}

true_effort_devs <- vfo$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(effort = unique(effort))

true_effort_devs %>% 
  ggplot(aes(year, effort)) + 
  geom_point()


true_effort_devs <- true_effort_devs %>% 
  mutate(effort_devs = effort / vfo$prepped_fishery[[1]]$scrooge_data$base_effort)

true_effort_devs %>% 
  ggplot(aes(year, effort_devs)) + 
  geom_point()

```

One thing I really don't like at this point is that the magnitude of the deviations is tied to the appropriateness of the base effort parameter. Though to some extent that doesn't really matter since it's scaleess, except for the case of the supplied catch. So so long as Emsy is an roughly OK effort, then your multiplier shouldn't be that bad. Could use some improvement but it's not the worst thing. 


Another thing you could try: estimate effort deviates in the same manner that you estimate recruitment deviates right now: with mean from the bionomic model and deviations from that. So in that model, you're estimating deviations from bionomic equilibrium, and probably theta as well. You could still update your prior on the deviate by the economic shock. Seems a reasonable defense is that we put a lot of stock in beverton holt with limited evidence, why not this as well?

This seems like a different enough idea that it warrants a separate model... which if you can't fix this one might be the way to go

poop, feeding it the right starting guess doesn't seem to solve it... 

Let's see how good the fit is is to the length comps: if it's getting it pretty close then that means there's something abotu the explanation that is more satisfactory to the model... which poop. 

One possibility, also try really constraining sigma effort a bit more, I wonder if it's too big at the moment. Part of the problem is that the likelihood actually is going up at those crazy low efforts.... 

To an exte

```{r}

a <- vfo$processed_scrooge[[1]]$n_tl

predicted <- a %>% 
  gather(length,numbers, contains('V')) %>% 
  mutate(length = str_replace(length,'\\D','') %>% as.numeric()) %>% 
  group_by(year, length) %>% 
  summarise(numbers = median(numbers)) %>% 
  mutate(source = "predicted")
 

observed <- vfo$prepped_fishery[[1]]$length_comps %>% 
  gather(length, numbers, -year) %>% 
  mutate(length = as.numeric(length)) %>% 
  mutate(source = "observed")

observed %>% 
bind_rows(predicted) %>% 
 ungroup() %>% 
  group_by(source, year) %>% 
  mutate(numbers = numbers / sum(numbers)) %>% 
  ungroup() %>% 
  ggplot(aes(length,numbers, fill = source)) + 
  geom_col(position = "dodge") + 
  facet_wrap(~year) + 
  theme_minimal()

```

Innnnteresting, those fit's aren't very good at all. Let's inspect the actual likelihood a bit here and make sure there's not just an error in there. 

Aha, sigma effort is part of what' causing such a problem in here

I wonder if the core problem here is that duh, recruitment and F are really hard to tease appart, and so given variation in both, the model prefers to pack the variation into one of the things, like recruitment in thise case

OK, digging into PFO a bit, even there, where the fit is "good", it's not great. I't missing the length comps a decent bit in some years (though you obviuosly also need to burn for a bit). In the PFO example, the likelihood  gets better the lower sigma_r is, which is correct, but divergences also start to pile up the lower sigma R gets. 

Could explore some transformations here: simple one would be to estimate log sigma r / log sigma effort. There's definitely some hierarchichal funneling going on... read up on that some more. As a starting point KISS, let's try log transforming. 

Alternatively, you could estimate one, and estimate and or pass a ratio of the variation in recruitment relative to the variaion in effort

Also see neil's funnel in the stan documentation page 341

Well loggint at least helped with the divergences

With little data (aka here), non-centering can help a lot. Let's think about how to do that in the context of this model 

Starting with the recruitment, you're making a strong assumption that there's no bias in the recruitment, therefore mean_beta is 0. So, in that case you're now estimating sigma_r and a vector of rec_devs ~ normal(0,1)

the actual rec devs in a given time step will then be sigma_r * rec_devs

What about for F? 

Following the example from the stan documentation, the staight forward translation would be


effort_devs  = mean_effort + sigma_effort * ed

ed ~ normal(0,1). 

So what if you went off script a bit here on the f's saying that f in the current year is most likely F last year, i'm unclear how that differs from saying that sigma effort should be really small. 

so, you have base effort, and you want to multiply that by a thing with mean 1. 

So, base_effort * exp(deviation)

Where deviation is  mean 0. 

Now, you could say that the deviation in any given time step 

`total_effort_t = base_effort * exp(effort_devs * sigma_effort)`

So how do you build the economic shocks in there?

`effort_devs[t] ~ normal(profit_shocks[t],1)`

where profit_shock is usualyl 0, but sometimes is very positive


AHA, wait, I think there's a problem with the low profit shock scenario; Suppose that sd is really low, isn't that going to crazy inflate your effort deviation? Check on this. 

Also, can use this formulation to actually estimate mean effort a bit easier

DOUBLE CHECK that you're not estimating parameters with zero information (i.e. recruitment in the final year)

Given the near fact that the model is working pretty damn well for the simpler cases, I'm wondering if there is something systematically wrong with the way that you're setting up that simulation, or if it's simply that you can't possibly estimate two random processes with the same variance. 

# ideas to improve scrooge

The more I dig into it, it seems likely that the model isn't really capable of dealing 
substantial variaion in both recruitment and effort dynamics. To an extent that's logical: without a prior 

# check in with cody

which one is in the drivers seat: effort or recruitment. So, add in to your simulation testing steepness of 1 and autocorrelated recruitment variation vs steepness of like 0.6 and effort following that

estimate rho (autocorrelation in recruitment)

address szuwalski & thorson 2017

# What's the end game with scrooge?

So, you know you need to run lots of versions, but it's going to be an expensive run so let's think about it carefully. 

Before hitting massive go I'd like the model to be a bit better at dealing with both recruitment and economic noise. 

The most obvious solution to that is this idea of a prior on the ratio of the recruitment to effort variation. It makes intuitive sense on some levels, but would depend a lot on how sensitive it is to model specification. 

Talk with jim tomorrow about the idea and see where he would push back, and how far you can push this thing. The goal should be a full run setup for the weekend, at the very least s

# culprits of spasm and lime divergence

- Recruitment variation

- initial depletion

- problem with length-at-age
  - Its not that, you generate identical length at age keys, unless you're sampling from them incorrectly?

- burn-in problems

# solving the fleet expansion

The only reasonable thing is a prior on the % change in effort in any one year. So, I want to put a prior on that, but then tune theta to achieve that. Can you calculate the mean/max effort change and use that somehow?

You need some mechanism for translating that change into a prior 

here's an idea. store the profit driven change in effort. Calculate max (absolute change - 1). Say that that comes from a distribution with mean mean_max_change and some low sigma. 

# check in with jason

THey almost never actually estimate sigma r

fix sigma r and depending on life history, 0.5, 0.6 standard deviation in log space 1.2 1.5 for hake

check in with merril on whether sigma_r and sigma_f are actually being estimated

try using a uniform prior on f instead of a centered sigma

check on technical documentation for stock synthesis 

report on estimated catches; compare to catch histories from CDFW

test how thosedynamics that aren't included in other operating models affect how we do business

inducing different relevant dynamics 

we need to be including that type of thought when we. If we have this effort data we can try and recover effort data for use in other assessments. 

So here's a compromise between those: make the priors on sigma_r REALLY tight around the life history for that species, so that the model has a little leeway, but will be pulled towards that. 

```{r}
library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

set.seed(42)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    linf = 100,
    max_age = 30,
    time_step = 1,
    sigma_r = 0,
    price = 1,
    price_cv = 0.75,
    price_ac = 0.75,
    steepness = 0.6
  )

fleet <- create_fleet(
  fish = fish,
  cost = 3,
  cost_cv =  0.75,
  cost_ac = 0.75,
  q_cv = .5,
  q_ac = .95,
  profit_lags = 0
)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 10,
      cost = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = .75
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0),
  num_patches = 1,
  sim_years = 100,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(ssb = sum(ssb)) %>% 
  ggplot(aes(year, ssb))  +
  geom_point()


sim %>% 
  group_by(year) %>% 
  summarise(effort = mean(effort),
            price = unique(price),
            cost = unique(cost),
            profits = sum(profits),
            revs = sum(biomass_caught * price)) %>% 
  ungroup() %>% 
  mutate(delta_price = price - lag(price),
         delta_cost = cost - lag(cost)) %>% 
  gather(variable, value, -year, -effort) %>% 
  ggplot(aes((value), effort, color = variable)) +
  geom_point(show.legend = F) +
  facet_wrap(~variable, scales = "free")


```

So that's the problem at it's core. Well not sure if it's a problem, but reality. The relationship between price and profits is only loose. Wonder if part of that is an artifact of fishing down the initial stock...

A little bit, the core fact is tshat price and costs themselves are weak signals of effort, even in this stylized world. This goes back to your original ideas of caculating dE/dprice, and of course realizing that it is completely state dependent. So, it's no surprise that prices and costs themselves are not informative. 

At its core then, I think you need to abandon shock scrooge; it has to have some structural parameters to it... unless you did soemthing interesting like lags of prices and costs... takens theorem? Or should you focus more on integrating effort priors themselves, thinking that prices and costs themselves don't tell you much..., at least if you want to stick with a classic model of open access fisheries, which seems reasonable. 




parameterize price shocks on the scale of Tyler's database!


```{r}
  fish_prices <-
    readr::read_csv("~/Box Sync/Databases/Exvessel Prices/Exvessel Price Database.csv") 

price_cv <- fish_prices %>% 
  group_by(scientific_name) %>% 
  summarise(cv = sd(exvessel, na.rm = T) / mean(exvessel, na.rm = T)) 

price_cv %>% 
  ggplot(aes(cv)) + 
  geom_histogram()

```


OK, I think shock scrooge is dead; just loooking at the simulated scenario there's not really a good reason to have a particular prior on price, though that does seem a little odd: good thing to talk over with Chris. But for now, seems like you've got bioeconomic scrooge working again. Problem was largely the multiplicative effort deviates. Still similar problem separating out recruitment from effort, and constraining effort enough, but seems like it works-ish at least. 

One problem though is that the model is clearly really good at nailing the length comps, even when the other stuff is off, which duh, there's a lot of ways to get the length comps if you can play with recruitment and with f, and something about the way you have it set up at the moment prefers to use reruitment to dial the noise in. i.e your model isn't broken: you're not getting divergences or treedepth problems anymore, and it actually is pretty close to perfect on the length comps. So the only way to "improve" the fit is to make priors more informative. 

From the effort side, the prior could at least by the year-to-year CV on the effort deviates, and then there's your old friend p_response. 

From there, you need to get the initial depletion thing working. Works!


# Pre-meeting activities

I think you may have finally settled on a working version of the model... so what are you rnext steps. 

Week of may 9th: Get a "complete" run working to see the model end-to-end

- Grab the length comps at a few different places in the fishery history: early, middle, late

- try and get a more "economicy" looking period to fit to

- add in LBSPR and LIME fits 

- Add in LIME-lite


Week of may 16: Do any new runs that need doing as a result of may 9th

May 23: Write presentation

# meeting with chris

endogenous vs exogenous shocks

diffuse price 

if you don't have any information 

unemployment, labor 

if profit goes up by 10%, by what percent does effort go up

look at data from ram what's the biggest increase in F/Fmsy that we ever see 

then, calculate thetat as the thing that prodiuces that change in f/fmsy at b/bmsy = 1

So the two to-do's on this. First, you need to tune the "default" price and cost parameters to something sane, such that at EQ the population is at let's say 0.5 B/Bmsy. 

Second, make theta in units of p/pmsy.  you also need to build it into scrooge for the simulation generation step of things. Set price at someting, find costs that get you to open access level of fishing given theta.

The assumption here is that profits will increase the most when profits are at their highest, so this should be the biggest change in F/Fmsy that we would reasonably expect

```{r}
ram <- read_csv(here::here("processed_data","ram_data.csv"))

delta_f_v_fmsy = ram %>% 
  filter(is.na(udivumsypref) == F,udivumsypref < 4) %>%  
  select(stockid, year, udivumsypref) %>% 
  arrange(stockid, year) %>% 
  group_by(stockid) %>% 
  mutate(delta_year = year - lag(year),
         delta_u = udivumsypref - lag(udivumsypref)) %>% 
  ungroup() %>% 
  filter(delta_year == 1, is.na(delta_u) == F, delta_u > 0)

max_delta_u <- delta_f_v_fmsy %>% 
  group_by(stockid) %>% 
  summarise(max_delta_u = max(delta_u))
  
mean(max_delta_u$max_delta_u)

max_delta_u %>% 
ggplot(aes(max_delta_u)) +
  geom_histogram()


```


# integrating effort data

The main question is is it data or is it a prior? On the data side, you treat the vector of relative effort over time as data or as a prior. If it comes in as data, the idea would be that you put it in the likelihood. The advantage there is that it's actual data means that the entire likelihood isn't explained by the length comps. The disadvantage is what to do if the effort data isn't really data but more of a prior.

Fuck it, let's try both. 

On the likelihood side, scale the effort relative to something, treat it as normally distributed for now. Philisophically, you have priors based on cost and tech, and you confront those priors with the effort and length data

On the prior side, philosphically you're saying you don't have effort data per say but an idea around effort. So where would those come in?

The prior would come in on effort_shock_t? though technicaly speaking, effort_shock is deviations from the open-access model. So, suppose that you sampled from a perfect open access scenario. Then, the prior on the effort shock should in theory be zero. So, it's kind of hard to think about how you would construct a prior around the deviations, when your prior is already that it is open access. 

So in that case, I think your prior would have to actually sub in for the open-access model. In that you have some prior that effort has been flat, increasing, whatver, and then you still estimate the shocks around that model. 


```{r}

    fisheries_sandbox <-
      purrr::cross_df(
        list(
          sci_name = c("Lutjanus campechanus"),
          fleet_model = c(
            "open-access"
          ),
          sigma_r = c(0),
          sigma_effort = c(0),
          price_cv = c(0),
          cost_cv = c(0),
          price_ac = 0,
          cost_ac = 0,
          q_cv = 0,
          q_ac = 0,
          economic_model = c(1),
          steepness = c(0.6),
          obs_error = c(0),
          b_v_bmsy_oa = c(0.5)
        )
      )

   fleet_model_params <- data_frame(
      fleet_model = c(
        "constant-catch",
        "constant-effort",
        "supplied-catch",
        "open-access"
      ),
      fleet_params = list(
        list(target_catch = 10000),
        list(initial_effort = 200),
        list(catches = cdfw_catches$catch[cdfw_catches$sci_name == "semicossyphus pulcher"]),
        list(theta = 0.5, initial_effort = 10)
      )
    )
   
   fisheries_sandbox <- fisheries_sandbox %>%
      left_join(fleet_model_params, by = "fleet_model") %>%
      # filter(fleet_model == "open-access", b_v_bmsy_oa == 0.5) %>%
      # arrange(b_v_bmsy_oa) %>%
      # slice(1) %>%
      mutate(prepped_fishery = pmap(
        list(
          sci_name = sci_name,
          fleet_model = fleet_model,
          fleet_params = fleet_params,
          sigma_r = sigma_r,
          sigma_effort = sigma_effort,
          price_cv = price_cv,
          cost_cv = cost_cv,
          price_ac = price_ac,
          cost_ac = cost_ac,
          steepness = steepness,
          obs_error = obs_error,
          b_v_bmsy_oa = b_v_bmsy_oa,
          q_cv = q_cv,
          q_ac = q_ac
        ),
        safely(prepare_fishery),
        sim_years = 50,
        burn_years = 25,
        price = 10,
        cost = 5,
        profit_lags = 1,
        query_price = F,
        use_effort_data = 1
      ))
   
      no_error <- map(fisheries_sandbox$prepped_fishery,'error') %>%
      map_lgl(is.null)

    fisheries_sandbox <- fisheries_sandbox %>%
      filter(no_error == T)

    fisheries_sandbox <- fisheries_sandbox %>%
      mutate(prepped_fishery = map(prepped_fishery,"result")) %>%
      mutate(summary_plot = map(prepped_fishery, plot_simmed_fishery))



      tester <- fisheries_sandbox %>%
    slice(1) %>%
    mutate(prepped_fishery = map(prepped_fishery, subsample_data, window = 10, period = "end")) %>%
    mutate(scrooge_fit = pmap(
      list(
      data = map(prepped_fishery, "scrooge_data"),
      fish = map(prepped_fishery, "fish"),
      fleet = map(prepped_fishery, "fleet")),
      fit_scrooge,
        iter = 4000,
        warmup = 2000,
        adapt_delta = 0.8,
        economic_model = 1,
       use_effort_data = 1,
        scrooge_file = "new_scrooge",
        in_clouds = F,
      experiment = "pfo",
      max_f_v_fmsy_increase = 0.25
      )
    )
      
       tester <- tester %>%
    mutate(processed_scrooge = map2(
      scrooge_fit,
      map(prepped_fishery, "sampled_years"),
      process_scrooge
    )) %>%
    mutate(observed = map(prepped_fishery, "simed_fishery")) %>%
    mutate(scrooge_performance = pmap(
      list(observed = observed,
           predicted = processed_scrooge),
      judge_performance
    )) %>%
 mutate(
      scrooge_rec_performance = pmap(
        list(observed = observed,
             predicted = processed_scrooge),
        judge_performance,
        observed_variable = rec_dev,
        predicted_variable = "log_rec_dev_t"
      )) %>% 
    mutate(lcomps = map2(prepped_fishery, processed_scrooge, ~process_lcomps(.x, .y$n_tl))) %>%
    mutate(
      rmse = map_dbl(scrooge_performance, ~ .x$comparison_summary$rmse),
      bias =  map_dbl(scrooge_performance, ~ .x$comparison_summary$bias)
    ) %>%
    arrange(rmse)

  tester$scrooge_performance[[1]]$comparison_plot + 
    lims(x = c(35, NA))
  
    tester$scrooge_rec_performance[[1]]$comparison_plot+ 
    lims(x = c(35, NA))

  tester <- tester %>%
    mutate(lime_fit = pmap(list(
      data = map(prepped_fishery, "scrooge_data"),
      fish = map(prepped_fishery, "fish"),
      fleet = map(prepped_fishery, "fleet")
    ), fit_lime))

  true_f <- tester$prepped_fishery[[1]]$simed_fishery %>%
    group_by(year) %>%
    summarise(f = unique(f)) %>%
    filter(year >=64)

 a <- tester$lime_fit[[1]]$Sdreport %>%
    summary() %>%
   as.data.frame() %>%
   mutate(variable = rownames(.)) %>%
   filter(variable == "lF_y") %>%
   mutate(mean = exp(Estimate),
          upper = exp(Estimate + 1.96 * `Std. Error`),
          lower = exp(Estimate -  1.96 * `Std. Error`)) %>%
   mutate(year = 1:nrow(.)) %>%
   ggplot() +
   geom_pointrange(aes(year, mean, ymin = lower, ymax = upper)) +
   geom_point(data = true_f, aes(1:11, f), color = "red")

    
```

# Final tweeks to scrooge

log sigma effort is a problem. The estiamtes are literally just the prior, with a SLIIIIIIIGHT boost to the right. Is it better to fix it at something, like 1?

Your recruitment devs aren't particularly well mixed either

Also, are you actually estimating length sel or is that just spitting back the prior? Check on that. 

p_response is looking a bit goofy as well. 

length_50_sel is 24cm, is that right?

```{r}

a = compare_sim$scrooge_fit[[1]]

b <- rstan::extract(a, "p_length_50_sel")

traceplot(a, pars = "p_length_50_sel", inc_warmup = TRUE)
```

Huh, it's starting off at the "right" level, but then heading south pretty quick... 

```{r}

sel <- rstan::extract(a, "mean_selectivity_at_age")$mean_selectivity_at_age

sel <- colMeans(sel)

compare <- data_frame(pred = sel, obs = compare_sim$prepped_fishery[[1]]$fleet$sel_at_age) %>% 
  mutate(age = 1:nrow(.)) %>% 
  gather(source, selectivity, -age)

compare %>% 
  ggplot(aes(age, selectivity, color = source)) + 
  geom_jitter()


```


```{r}
a = tester$prepped_fishery[[1]]$simed_fishery

b <- a %>% 
  group_by(year) %>% 
  summarise(effort = mean(effort), profits = sum(profits)) %>% 
  mutate(p_per_e = profits / effort)

b %>% 
  ggplot(aes(year, p_per_e)) + 
  geom_point()

b <- a %>% 
  group_by(year) %>% 
  summarise(biomass = sum(biomass),
            price = mean(price),
            cost = mean(cost),
            q = mean(f) / mean(effort),
            effort = mean(effort),
            profits = sum(profits),
            catch = sum(biomass_caught)) %>% 
  ungroup() %>% 
  mutate(lead_effort = lead(effort),
         p_per_e = profits / effort,
         c_per_e = catch / effort)

b %>% 
  select(-effort) %>% 
  gather(variable, value, -year, -lead_effort) %>% 
  ggplot(aes(value, lead_effort, color = variable)) + 
  geom_point(show.legend = F) + 
  facet_wrap(~variable, scales = "free_x")




```

oh wait, this is interesting. Conditional on the other crap, biomass us proportional to profit per unit effort. So does that mean that you could do something like profit per unit effort in the model enters the model by being fit to the biomass output by the model?

Annnnd the relationship holds even outside of the open access model. woah. Now, that relationahips could be distorted by changes in price, cost, and technology, which if you have data on can help. 

```{r}
b %>% 
  ggplot(aes(profits, lead_effort )) + 
  geom_point()
```


# summary of candidate economic models

OK, it's time to make some decisions. 

1. Effort in this time step is proportional to effort in the last time step. This is just the LIME model 

2. Open access + process error. Effort is a latent variable with mean open access model and some noise around it

3. Open access + process error + observation error. You actually fit to the timeseries of effort data that comes out of this process. 

4. profit_per_unit_effort ~ biomass. No more open access structure. The theory here is that given data on the other stuff, effort will get tuned to make this work out. 

5. profit_per_unit_effort ~ hat(profit_per_unit_effort) prevents the need for a translator between biomass and profit per unit effort and should have a similar effect. 

6. Just effort ~ hat(effort)


So here's a simple question. What does profit per unit effort tell you that catch per unit effort doesn't? If I'm being honest, getting people to report their average catches per fishing trip seems easier than profits. One advantage is that it gives you something like a reference point: the closer profits per unit effort get to 0, the closer you are to open access. But what if you just observe a bunch of p per unit e at levels above 0, why is that any more or less informative than just CPUE? At least from the perspective of the model, there isn't anything. bummer. The only real advantage there would be if you felt that profit per units effort was easier to measure than catch per unit effort. 



# Sea Grant feedback

- Think about dynamics of food motivated fisherie


# Final steps

So at the moment, are are providing the same data in every year: all the economic priors are doing is help you separate out F and recruitment. And, it turns out that's not all that useful. Not really surprising there in any real way given that in order to stop total chaos youre not letting F change all that much year to year, so the generic prior on that isn't a bad idea. You could mess with the sigma priors a bit here to try and tighten things up there, but I'm not really happy with results being that sensitive to that esoteric of a choice. 

You should also explore though why it is that the first year of effort is so precisely estimated, unlike first year of recruitment. 

So, the story here is that if you have 10-20 years of highly representative length data, you do a decent job. Maybe economic priors tighten up the intervals a bit. 

That's not really all the common of a situation though. So, what you should be testing now is what if you only have say 1-5 years of length data, but a few years of economic priors. This is a *MUCH* more realistic scenario. You are already set up to only have a few years of length comps, so I think this should only take a little wrangling on the data-input side to get this to work properly. 

One other thing you should consider. Aside form letting effort be just a prior on F, what if it's data? Still need to estimate q, and then fit observed vs predicted effort. So this would be for a case where you think you have a handle on the scale of total effort in the fishery, and you want to translate that effort into an F. Interesting for a few reasons. Allows the q to help translate the relative impact of that effort on the population (could have the trend right but the magnitude wrong), e.g. if you are sampling representative effort from a fishery but only a small portion of it. But that would actually back out estimated catch which would be very cool (whoop no since that would require something about r0 in there). So now this would be a joint likelihood with the effort and lengths... 

In that way, you could use the economic priors to help guide F, and then confront those priors with data from the fishery. The weighting could then come down to how much creedence you want to give to the effort data itself, instead of just in the in the prior itself. 

Seems like a good discussion to have with Jason; don't scrap the current method, but definitely build this in. For that, sample the effort data through some process, probably negative binomial to allow discrete but overdispersed?

Phase 1: More econ prior time series than length data

Phase 2: Turning effort into data instead of just priors


## phase 1

Easiest option is to build it into subsample_data

Ah interesting, I think steepness is where the bias is coming in from! Seems like the model is unbiased when steepness is low, but when sigma R is high and steepness is high, things become biased. Check and see what your default steepness is. So in general,probably shouldn't be passing steepness as data, but rather as a guess. 


```{r}


case_studies <- purrr::cross_df(list(scenario = c("simple","complex"), 
                                     economic_model = c(1,0),
                                    effort_data_weight = c(1,0))) %>% 
  filter(!(economic_model == 0 & effort_data_weight > 0))



simple <- fisheries_sandbox %>%
  filter(
  sci_name == "Lutjanus campechanus",
  steepness == 0.6,
  sigma_r == min(sigma_r),
  sigma_effort == min(sigma_effort),
  price_cv == min(price_cv),
  cost_cv == min(cost_cv),
  q_cv == min(q_cv),
  q_ac == min(q_ac),
  obs_error == min(obs_error),
  fleet_model == "constant-effort"
  ) %>%
  mutate(scenario = "simple") 


complex <- fisheries_sandbox %>% 
  filter(sci_name == "Lutjanus campechanus",
         steepness == max(steepness), 
         sigma_r == max(sigma_r),
         sigma_effort == max(sigma_effort),
         price_cv == max(price_cv),
         cost_cv == max(cost_cv),
         q_cv == max(q_cv),
         q_ac == max(q_ac),
         obs_error == min(obs_error),
         fleet_model == "open-access") %>% 
  mutate(scenario = "complex")

case_studies <- case_studies %>% 
  left_join(simple %>% bind_rows(complex) %>% select(-economic_model) %>% nest(-scenario), by = "scenario")
  



case_studies <- fisheries_sandbox %>%
  mutate(assessment = pmap(
    list(
      fishery = data,
      experiment = scenario,
      economic_model = economic_model,
      effort_data_weight = effort_data_weight
    ),
    run_and_judge_scrooge,
    prop_years_lcomp_data = .1,
    window = 10,
    period = "middle",
    max_f_v_fmsy_increase = 3,
    cv_effort = 0.25
  ))


case_studies <- case_studies %>% 
  mutate(f_t_hat = map(assessment, ~map_df(.x$processed_scrooge,"f_t")))



case_studies <- case_studies %>% 
  mutate(f_t = map(data, ~ map_df(.x$prepped_fishery,"simed_fishery") %>% 
                     group_by(year) %>% summarise(f = unique(f))))


true_f <- case_studies %>% 
  select(scenario, f_t) %>% 
  unique() %>% 
  unnest()
  

f_performance <- case_studies %>%
  select(-data, -assessment,-f_t) %>%
  unnest() %>%
  mutate(effort_data_weight = if_else(economic_model == 0, 99, effort_data_weight)) %>% mutate(effort_data_weight = factor(effort_data_weight))

f_summary <- f_performance %>%
  group_by(effort_data_weight, scenario, year) %>%
  summarise(
  lower = quantile(value, 0.1),
  upper = quantile(value, 0.9),
  mean_f = mean(value),
  median_f = median(value)
  ) %>%
  ungroup() %>% 
  mutate(effort_data_weight = factor(effort_data_weight))



f_performance_plot <- f_summary %>%
  ggplot()  +
  geom_ribbon(aes(
  year,
  ymin = lower,
  ymax = upper,
  fill = effort_data_weight
  ),
  alpha = 0.25) +
  geom_line(aes(year, mean_f, color =effort_data_weight )) +
  geom_point(data = true_f %>% filter(year %in% unique(f_performance$year)), aes(year, f)) +
    geom_line(data = true_f %>% filter(year %in% unique(f_performance$year)), aes(year, f)) +
  facet_wrap(~ scenario) 

```


#phase 2


So now, let's try passing the actual effort data in

```{r}


case_studies <- purrr::cross_df(list(scenario = c("complex"), 
                                     economic_model = c(0),
                                    effort_data_weight = c(0))) %>% 
  filter(!(economic_model == 0 & effort_data_weight > 0))




update_data <- function(prepped_fishery) {
  prepped_fishery$scrooge_data <-
  prepare_scrooge_data(
  fish = prepped_fishery$fish,
  fleet = prepped_fishery$fleet,
  sim = prepped_fishery$simed_fishery,
  bias = 0,
  obs_error = 0,
  economic_model = 0,
  cv_effort = 0
  )
  
  return(prepped_fishery)
  
}

simple <- fisheries_sandbox %>%
  filter(
  sci_name == "Lutjanus campechanus",
  steepness == 0.6,
  sigma_r == min(sigma_r),
  sigma_effort == min(sigma_effort),
  price_cv == min(price_cv),
  cost_cv == min(cost_cv),
  q_cv == min(q_cv),
  q_ac == min(q_ac),
  obs_error == min(obs_error),
  fleet_model == "constant-effort"
  ) %>%
    mutate(scenario = "simple") %>% 
    mutate(prepped_fishery =  map(prepped_fishery, update_data))



complex <- fisheries_sandbox %>% 
  filter(sci_name == "Lutjanus campechanus",
         steepness == max(steepness), 
         sigma_r == max(sigma_r),
         sigma_effort == max(sigma_effort),
         price_cv == max(price_cv),
         cost_cv == max(cost_cv),
         q_cv == max(q_cv),
         q_ac == max(q_ac),
         obs_error == min(obs_error),
         fleet_model == "open-access") %>% 
  mutate(scenario = "complex") %>% 
  mutate(prepped_fishery =  map(prepped_fishery, update_data))

case_studies <- case_studies %>% 
  left_join(simple %>% bind_rows(complex) %>% select(-economic_model) %>% nest(-scenario), by = "scenario")
  

case_studies <- case_studies %>%
  slice(1) %>% 
  mutate(assessment = pmap(
    list(
      fishery = data,
      experiment = scenario,
      economic_model = economic_model,
      effort_data_weight = effort_data_weight
    ),
    run_and_judge_scrooge,
    prop_years_lcomp_data = .1,
    window = 10,
    period = "middle",
    max_f_v_fmsy_increase = 3,
    cv_effort = .5,
    scrooge_file = "scrooge_v2"
  ))

case_studies <- case_studies %>% 
  mutate(f_t_hat = map(assessment, ~map_df(.x$processed_scrooge,"f_t")))


case_studies <- case_studies %>% 
  mutate(f_t = map(data, ~ map_df(.x$prepped_fishery,"simed_fishery") %>% 
                     group_by(year) %>% summarise(f = unique(f))))


true_f <- case_studies %>% 
  select(scenario, f_t) %>% 
  unique() %>% 
  unnest()
  

f_performance <- case_studies %>%
  select(-data, -assessment,-f_t) %>%
  unnest() %>%
  mutate(effort_data_weight = if_else(economic_model == 0, 99, effort_data_weight)) %>% mutate(effort_data_weight = factor(effort_data_weight))

f_summary <- f_performance %>%
  group_by(effort_data_weight, scenario, year) %>%
  summarise(
  lower = quantile(value, 0.1),
  upper = quantile(value, 0.9),
  mean_f = mean(value),
  median_f = median(value)
  ) %>%
  ungroup() %>% 
  mutate(effort_data_weight = factor(effort_data_weight))


f_performance_plot <- f_summary %>%
  ggplot()  +
  geom_ribbon(aes(
  year,
  ymin = lower,
  ymax = upper,
  fill = effort_data_weight
  ),
  alpha = 0.25) +
  geom_line(aes(year, mean_f, color =effort_data_weight )) +
  geom_line(data = true_f %>% filter(year %in% unique(f_performance$year)), aes(year, f)) +
  facet_wrap(~ scenario)

```

idiot. no wonder the prior f thing is working so well: you're giving the model q!!!!! it needs to estimate or fix it. Whoops no that's not entirely right. At the moment, you hard wired 0.1 as q into prepare scrooge data. But, it is true that that the way you have it coded right now, if you were to pass it a time series of q, the non-economic model assessment would gain access to that information. So broadly, make sure that's not happening. 

Regardless, at the moment, by fixing q at a number, you slide effort up and down to translate those qs into F, since the actual scale of effort doesn't really matter to you. 

That presents a problem now though in the case where you are estimating the actual effort, since now q matters. So, you need to either estimate q, or estimate the trend in effort instead of the total effort itself. That seems like it might do the trick, since the idea is simply to say the effort data helps you with the trend, and the length data help you understand is that a lot or a little of effort.

Once you have effort data, treating it as anything other than data doesn't make much sense. It can be used to inform a prior on a latent variable, but you're not really confronting a model with data if you try and fit to the effort, unless you think about as estimating observation error in the effort 

So, without effort data, you're in friendly territory of using incentive priors. With effort data, you're faced with two general choices. You can treat it as data, and use it to improve your estimates of F. In that case, you'd need to translate between the effort to F by estimating q. Alternatively, you could do what you're doing now and use the effort data as another part of the likelihood, where you're saying given the length data and priors, what is the likelihood of the effort data? You'd still need to estimate q to make this work, and now you'd be estimating observation error in the effort data. Not really a big fan of this for a few reasons. Instead of using the effort data to guide your model, you're almost using the effort data to ask more of the length data alone. "given effort, predict effort" is a pretty silly statement. 

So, you're back to what to do with the effort data that are given to you. At the moment, it's coming as a prior on the rate of change of effort. So, the effort you estimate, given the fixed q's dictates F, and this just helps guide where F should go year to year. 

An alternative would be to say that effort data are effort data. Now, you need to translate those efforts into Fs, which you'd need to do by a q. So you'd need to either estimate a q, or a multiplier on effort to scale it relative to the q. Not bad, but gets comlicated to think about how to code that in, since you can't have if statements in the parameter block, so hard to say estimate q sometimes, and not others. 

One option would be to set sigma effort to be very very low if you want to treat the percent change effort as closer to "data". That would really force effort to do exactly what you want it do to


# Last steps

OK, no more wheel spinning, what are the last steps to do here?

Everything seems to fit. The main time consuming part is running all the damn simulations for the mass performance review. So let's get that going this week so you can have it running while you're in wisconsin and have it ready to write about. 


So what are the steps you need to make that happen? Let's explore a few improvements to the operating model open-access scenarios to see if you can allow them to be more responsive to price/cost/q (not automatically, but have that be a parameter). Is it worth also varying which data you include in the assessment? On one hand, the interesting comparison is all data vs none, on the other hand gradients of value of information are interesting. Histograms of RMSE as a function of which historic data you include. Lastly, double check that the effort thing is working well. Can explore as a later project using total effort data, but just seems like an esoteric use case. 

## Making effort more responsive to profits

So as a general case, if you think of change in effort as additive function of profits, 

then 

$$dE/dp = qEB$$

$$dE/dc = -E^{\beta}$$

and 

$$dE/dq = pEB$$

Just eyeballing these, all else equal q is going to be more impactful than p, since holding E and B constant p is going to be much bigger than q. 

```{r}

effort <- 0:100

beta <- 1.3

price <- 10000

biomass <- 100

dc <- -effort^beta

dq <- price * effort * biomass

data_frame(effort = effort, dc = dc, dq = dq) %>% 
  gather(variable, marginal_effect, -effort) %>% 
  ggplot(aes(effort, abs(marginal_effect), color = variable)) + 
  geom_line()

```



So, for the range of levels for effort, price, biomass, etc q is going to matter a LOT more than important than either price or cost, so basically it goes q > c > p. 

So, broadly you could play a bit with the max increase, with an idea of what a reasonable max increase in F/Fmsy might be. From there though, might be worth thinking about the relative magnitude of things. Rather than totally making things up, let's try and get some reasonable units using RAM and the price database. 

get price per kg from melnychuck. Get r0 from RAM? if not, get a sense of the scale that you're talking about, and get weight in the right units to again make some kind of sense. So that gives you a sense of scale for price and r0, you can get a sense of q from RAM maybe? That leaves cost per unit effort as something to play with. I really like this idea just to stop making things up. Goal is getting the full run going by the time you get on the plane Wednesday. 

So that's on the simulation side. On the fitting side then, what you want to do is carefully construct a pretty generic prior, with some ability to control things. e.g. set price a reasonably price per kg, and set the scale of the population at a reasonable thing. Then, set q to a reasonable level to get efforts in a range you want them, then tune costs to get you an open access biomass. 

YOu're doing all this because scale matters in here: if effort is really big then costs can start to matter, but for the small efforts that you're talking about here then really the only thing that comes into play is q


```{r}
load("~/Dropbox/Prelim Database Files/Versions/RAM v4.40 (6-4-18)/DB Files With Model Fit Data/DBdata.RData")

```

 ## what are reasonable prices?
 
```{r}
 
fish_prices <-
    readr::read_csv("~/Box Sync/Databases/Exvessel Prices/Exvessel Price Database.csv") %>%
    filter(Year > 2010) %>%
    group_by(scientific_name) %>%
    summarise(mean_exvessel_price = mean(exvessel, na.rm = T)) %>% 
  mutate(price_units = "usd_per_ton") %>% 
  mutate(price_per_kg = mean_exvessel_price / 1000)

fish_prices %>% 
  ggplot(aes(price_per_kg)) + 
  geom_histogram() + 
  scale_x_continuous(labels = dollar)

  
```

## what are reasonable ssb0?

looks like you can use bioparams == SSB), and units  = MT to get tons of unfished SSB, see what that looks like. 

Could also use R0 and units = E00

```{r}


r0 <- bioparams %>% 
  filter(bioid == "R0-E00") %>% 
  mutate(biovalue = as.numeric(biovalue))


r0 %>% 
  ggplot(aes(biovalue)) + 
  geom_histogram() + 
  scale_x_log10()

```

 
Cool, good as any, goes from small to bonkers big. 


## q

```{r}


f <- er.data %>% 
  mutate(year = rownames(.)) %>% 
  select(year, everything()) %>% 
  gather(stock, f, -year)

effort <- effort.data %>% 
  mutate(year = rownames(.)) %>% 
  select(year, everything()) %>% 
  gather(stock, effort, -year)


e_and_f <- f %>% 
  left_join(effort, by = c('year','stock')) %>% 
  filter(is.na(f) == F & is.na(effort) == F) %>% 
  mutate(q = f / effort) %>% 
  mutate(year = as.numeric(year))

e_and_f %>% 
  ggplot(aes(year, q, color = stock)) + 
  geom_line(show.legend = F) 

e_and_f %>% 
  ggplot(aes(q)) + 
  geom_histogram()
```


## max expansion


```{r}

u <- divupref.data %>% 
  mutate(year = rownames(.)) %>% 
  select(year, everything()) %>% 
  gather(stock, u_v_umsy, -year) %>%
  mutate(year = as.numeric(year)) %>% 
  arrange(stock, year) %>% 
  group_by(stock) %>% 
  mutate(lead_u = lead(u_v_umsy,1)) %>% 
  filter(is.na(u_v_umsy) == F & is.na(lead_u) == F) %>% 
  mutate(delta_u = lead_u - u_v_umsy)

u %>% 
  group_by(stock) %>% 
  summarise(max_delta_u = max(delta_u, na.rm = T)) %>% 
  ggplot(aes(max_delta_u)) + 
  geom_histogram() + 
  scale_x_log10()


```

Boom, these are all just great. So then one option will be to draw randomly from these, another to bin?

Binning seems more efficient. You're going to waste a lot of iterations pulling say 0.1 and 0.12 when the comparison isn't really all that interesting. 


# fixing spasm

AHA. finally figured out why your fleet model is so out of whack. YOu're setting the seed before each fit and so that's making the results stable. But, that's one seed, and with stochasticity, you're just capturing the MSY, and the prices, costs etc. at a fairly arbitrary level. Whihc, hen you start simulating things out, doesn't translate. You get a totally different MSY and hence prices and costs. From the open access perspective, of course you end up in crazy new places. 

With stochasticity in p,c,q, e_msy will change but MSY itself won't, though with sigma_r in place that of course changes that, since the MSY you estimate could be just a function of a light recruitment pulse. 

So, what's the solution

For MSY, you'd need to stop setting the seed, and then calcuate MSY a bunch of times to get a distribution of MSY and E_MSY and B_MSY. You could then find the biggest MSY, the smallest E_MSY (corresponding to the biggest q). So, that would give you everything you need except price. You could take price, and then as a buffer multiply it by some big value and then that would mean that c to r ratio under the most optimistic scenario would be something. 


One option would be to simulate things out and take MSY as the mean value at EQ. 




```{r}
a <- fisheries_sandbox %>% 
  mutate(experiment = 1:nrow(.)) %>% 
  mutate(sim = map(prepped_fishery,'simed_fishery') ) %>% 
  select(experiment, sim) %>% 
  unnest()

a %>% 
  group_by(experiment, year) %>% 
  summarise(price = unique(price),
            cost = unique(cost),
            q = unique(f / effort),
            rec_dev = unique(rec_dev)) %>% 
  gather(variable, value, -experiment, -year) %>% 
  ungroup() %>% 
  ggplot(aes(year, value, color = factor(experiment))) + 
  geom_line(show.legend = F) + 
  facet_wrap(~variable, scales = "free_y")

b <- fisheries_sandbox %>% 
  arrange(desc(price_cv))

b$summary_plot[[1]]
```

that looks better. 

# profit per unit effort

Interesting though motivated by @Carozza2017, moving the fleet model to profit per unit effort instead of total profits. This has a few potential advantages. First, it seems plausible that this might be easier to tune and subject to less wild swings than pure profits, especially in the early years of the fishery as profits go up but effort increases at the same time. 

In addition, gives you another thing to build into scrooge...

Actual profits in the fishery, no way in hell. But what about profits per unit effort? That seem reasonable actually.

Where would that come in? That you might actually be able to fit intot he likelihood. It's an outcome of the model itself that you could confront with data. 

Could adjust the importance of it by weighting or by the prior on sigma. That's really the only part that makes much sense.... 

Unless you took profit per unit effort, calculated profits based on the model, and then backed out effort and used that effort as a prior on the change in effort? seems a bit bass ackwards. 

Another thing: Why don't you just feed the effort data in? Ah right you sort of can. If you feed in the percent change effort and set the prior on sigma_effort to be really tight, and the weighting all in the effort, that basically forces effort to be the effort you say it is, and just scales the magnitude of effort up and down. Let's play with this a bit. 




Aha, it does make a difference. When things are crashing profits per unit effort and and profits per p_msu mirror each other. But, when the fishery is developing, there's a big difference. Profit per pmsy just mirrors profits and so increases exponentially, causing chaos. But, profit per unit effort actually slows down, dampening things. So it's worth doing. I think. Now, what would it actually entail... 


so one option would be to simply skip alllllll the msy nonsense, just set a cost parameter that is in units of cost per unit effort. Or maybe something like.... suppose you started fishing at E ~ M right off the bat, see what reveunes are then, and then tune costs accordingly to get your cost to revenue ratio right, and then just use that one... but that still leaves the expansion parameter question up for debate; for a one unit increase in profit per unit effort how much does effort change? could just tool around with a range... all of that to basically speed up the simulation process a ton. Or maybe just something like an upper bound... if p per unit E at SSB0 is something, set cost so that holding new profits stable how long does it take to adjust to that level of profits. i.e. tune it so that it takes about ten years for effort to stabalize to zero 

aha what abotu setting it in terms of cost per unit F, which is pretty stable, then converting things to effort

Alternatively, you could say hey, it works for the most part, let's focus on getting scrogoe itself to converge better. that seems like a better use of your time since that's what needs to get run a lot more than the operatin models themselves. 

# what's the value of ppue?



```{r}

library(tidyverse)
library(FishLife)
library(spasm)
library(ggridges)
set.seed(42)
fish <-
  create_fish(
    scientific_name = "Lutjanus campechanus",
    query_fishlife = T,
    mat_mode = "length",
    time_step = 1,
    sigma_r = 0.25,
    price = 100,
    price_cv = 0.25,
    price_ac = 0.5,
    steepness = 0.6,
    r0 = 1000,
    rec_ac = 0.4,
    density_movement_modifier = 0
  )


fleet <- create_fleet(
  fish = fish,
  cost_cv =  0.25,
  cost_ac = 0.5,
  q_cv = 0.25,
  q_ac = 0.5,
  fleet_model = "open-access",
  sigma_effort = 0,
  length_50_sel = 0.25 * fish$linf,
  initial_effort = 100,
  profit_lags =  0,
  beta = 2,
  max_cp_ratio = 0.25,
  max_perc_change_f = 0.25
)


sim_noad <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0.5),
  num_patches = 1,
  sim_years = 100,
  burn_year = 50,
  time_step = fish$time_step,
  est_msy = F,
  random_mpas = F
)


 a = sim_noad %>%
  group_by(year) %>%
  summarise(te = unique(effort),
            f = unique(f),
            profits = sum(profits),
            biomass = sum(biomass),
            catch = sum(biomass_caught)) %>%
  ungroup() %>%
   arrange(year) %>% 
  mutate(ppue = profits / te,
         cpue = catch / te) %>%
   mutate(lag_ppue = lag(ppue),
          lag_profits = lag(profits),
          delta_f = f - lag(f),
          delta_effort = te - lag(te)) 
 
  
 wtf <- a %>% 
   filter(profits <0 & delta_effort > 0 )
 
 a %>% 
   ggplot(aes(lag_ppue, delta_effort)) + 
   geom_point()

   
   a %>% 
  gather(metric, value, -year) %>% 
  ggplot(aes(year, value, color = metric)) +
  geom_line(show.legend = F) +
    facet_wrap(~metric, scales = "free_y")




```

profit per unit effort is directly proportional to the increase in F. So, that's where it can become useful in the likelihood. In the same way that qCPUE is useful, dPPUE can be useful then, where you estimate q to translate CPUE to abundance, and you estimate d to translate PPUE. 

Or really, the reason that you don't just include the CPUE as data is that you are more interested in estiamting the unobserved biomass. In your case, you are interested in the F itself. 

So one option would be to pass the profit per unit effort in as data, and just estimate the p_response to translate that into changes in effort. So, that would be explain the length composition data, given data on the relative change in profitability. 

Another would be to use your same priors, but now take the profit per unit effort data as part of the likelihood. So, find the F, recruitment, etc. that best explain the length compision and the history of profit per unit effort in the fishery. 

add an index on a random effect

as a discussion point 

DEFINITELY make effort or ppue data on the left hand side. BOOYA

methot schirripa

unique identifier for a different 

Factor model for effort dynamics along metiers of fleets to look into which metiers are explaining F patterns

multivariate gompertz model, in theory you could fit a two "species" model of effort and biomass for the species and see what that looks like. 

hazard model on assessments the time between a stock becomes fished and when the first assessment comes, you can take that hazard model and transfer that into an annual varying propensity score. 

# Final parameterization 

OK, seems like you got things working fairly well again. Per Jim's point, definitely worth thinking about incorporating things on the right-hand side. 

- If you only know price, cost, or some unit of
  - Then it goes in the effort priors
- If you **know** effort
  - Input as data using economic weight of 1
  - Fit to as data
- If you have profits per unit effort
  - Input as data 
  - Fit to as data
  
Can you turn off parts of the likelihood?

You can at least include an if statement there for different economic models... 

I'd like to get away from the weighting since it's so arbitrary: how would you possible pick anything except zero or 1?

One option is just to fit to the PPUE, and take the price, cost, q, etc. as data. That would be assuming pure observation error

The other way is observation and process error. That's what you're doing now more or less. The process error comes in between the vector of efforts and the priors, and the observation error comes in between the true effort and the "observed" effort. look at the state epace example from the schaefer model to convince yourself 

ah though there you're estimating deviances, which is a bit different, interesting idea. 

So your options. 

I don't think it's kosher to both use the price, cost, and q data in the priors, and then fit to the ppue which is derived from those. Though, if you think about it, that's basically what a surplus production model is doing, in using the catch as data and then fitting to an index derived from the catch, so maybe it's OK? In that the PPUE is not a direct outcome of the costs etc. But, the question remains, why would you use the raw stuff if you have the PPUE itself. Why not just use the PPUE as data, and then allow effort deviates. 

If you're including both priors and data, that is saying you're including both process and observation error on the economic side, so need to estiamte both. 

of course, you idiot, when you tnink about it as process error, it's not a CV on how much effort can change, but how much it can deviate from the open access model itself. So, you should fix sigma r, which you can say you have reasonable means to predict, and estiamte sigma_effort_process, and observation if you need to.   

- 1: p,c,q driven priors

- 2: p,c,q driven priors, fit to PPUE
  - either relative ppue or tune costs here to allow tht things to actually match up

- 3: ppue based priors

- 4:  p,c,q driven priors, fit to effort
  - relative effort or tune q here to translate that effort into the right F
  
- 5: effort based priors - relative effort is fixed with deviates


Whoops, locking in inital depletion with initial F is a problem, since it's not allowing you to have high F and high B, need a burn-in F. Fix that first. 

# Fitting to ppue

If you don't have any data on prices/cost etc, then you can estimate ppue hat has delta effort / theta, where theta is the profit expansion factor, and then estimate f and theta and confront with data from PPUE


whoops, also, f in the last year is completely unidentified given the order of operations as they stand. You calculate F, and then apply that F to next years population. So, there's no signal of the current F in the current years's length comps, so you need to subtract one year off of the estimated F's

```{r}

a = fisheries_sandbox$prepped_fishery[[3]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(profits = sum(profits),
            effort  = unique(effort),
            f = unique(f)) %>% 
  ungroup() %>% 
  mutate(delta_effort = lead(effort) - effort,
         delta_f = lead(f) - f) %>%
  mutate(ppue = profits / effort) 

lm(delta_effort ~ ppue, data = a)

a %>% 
  ggplot(aes(ppue, delta_f)) + 
  geom_point()

 b = fisheries_sandbox$prepped_fishery[[3]]$simed_fishery %>% 
   group_by(year) %>% 
   mutate(pn = numbers / sum(numbers)) %>% 
   ggplot(aes(age, pn, color = year, group = year)) + 
   geom_line()
   
 b
 
fisheries_sandbox$prepped_fishery[[3]]$scrooge_data$length_comps %>%
   gather(length, numbers, -year) %>%
   mutate(length = as.numeric(length)) %>%
   group_by(year) %>%
   mutate(pn = numbers / sum(numbers)) %>%
   ggplot(aes(length, pn, color = year, group = year)) +
   geom_line()
```


checks to run

1. is biomass increasing when F is zero

2. Copy data and life history from LIME example, see if you can recover when model is in LIME mode

```{r}

lime_inputs <- readRDS("~/PhD/scrooge/data/lime_inputs.RDS")

limecomps <- lime_inputs$LF %>% drop()

limedata <- data

limedata$length_comps <- limecomps

limedata$length_comps_years <- 1:nrow(limecomps)

limedata$n_lcomps <- nrow(limecomps)

limedata$nt <- nrow(limecomps)

limedata$n_ages <- length(lime_inputs$ages)

limedata$n_lbins <- ncol(limecomps)

limedata$ages <- lime_inputs$ages

limedata$m <- lime_inputs$M

limedata$h <- lime_inputs$h

limedata$r0 <- lime_inputs$R0

limedata$k <- lime_inputs$vbk

limedata$loo <- lime_inputs$linf

limedata$t0 <- lime_inputs$t0

limedata$mean_length_at_age <- lime_inputs$L_a

limedata$mean_weight_at_age <- lime_inputs$W_a

limedata$mean_maturity_at_age <- lime_inputs$Mat_a

length_at_age_key <- generate_length_at_age_key(
  min_age = min(lime_inputs$ages),
  max_age = max(lime_inputs$ages),
  cv = lime_inputs$CVlen,
  linf = lime_inputs$linf,
  k = lime_inputs$vbk,
  t0 = lime_inputs$t0,
  time_step = 1,
  linf_buffer = 1.25
) %>%
  ungroup() %>%
  select(age, length_bin, p_bin) %>%
  spread(length_bin, p_bin) %>%
  select(-age)

limedata$length_at_age_key <- length_at_age_key

data$length_50_sel_guess <-  lime_inputs$SL50

limedata$economic_model <- 0

limedata$sigma_effort_guess <- lime_inputs$SigmaF

length_at_age_key %>% 
  mutate(age = 1:nrow(.)) %>% 
  gather(length, prob, -age) %>% 
  mutate(length = as.numeric(length)) %>% 
  ggplot(aes(length, prob, color = age, group = age)) + 
  geom_line()

limedata$length_comps %>% 
  as_data_frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  gather(lbin, number,-year) %>% 
  mutate(lbin = as.numeric(lbin)) %>% 
  ggplot(aes(lbin, number)) + 
  geom_col() + 
  facet_wrap(~year)

saveRDS(limedata, "limedata.RDS")



```



```{r}

data <- readRDS("../limedata.RDS")

data$economic_model <- 0

data$n_lcomps <- nrow(data$length_comps)

data$perc_change_effort <- data$perc_change_effort[1:data$nt]

data$observed_effort <- data$observed_effort[1:data$nt]

data$ppue_t <- data$ppue_t[1:data$nt]

data$price_t <- data$price_t[1:data$nt]

data$cost_t <- data$cost_t[1:data$nt]

data$q_t <- data$q_t[1:data$nt]

data$sd_sigma_r <- 0.1

data$sigma_f <- 0.2

data$bin_mids <- as.numeric(colnames(data$length_at_age_key)) 

data$bin_mids <- data$bin_mids + ((data$bin_mids[2] - data$bin_mids[1])/2)

data$age_sel <- 5

chains <- 2

refresh <- 25

iter <- 2000

warmup <-  1000

seed <- 52

cores <- 2

adapt_delta <- 0.8

max_treedepth <- 12

scrooge_file <- "scrooge"

  inits <-
    map(
      1:chains,
      ~ list(
        burn_f = jitter(data$m/2),
        f_t = jitter(rep(data$m/2, data$nt)),
        sigma_r = jitter(.1),
        p_length_50_sel = jitter(0.25),
        log_p_response = log(.1),
        sigma_effort = 0.2
      )
    )
  
fit <-
  rstan::stan(
    file = here::here("src", paste0(scrooge_file, ".stan")),
    data = data,
    chains = chains,
    refresh = refresh,
    cores = cores,
    iter = iter,
    warmup = warmup,
    control = list(adapt_delta = adapt_delta,
                   max_treedepth = max_treedepth),
    init = inits,
    seed = seed
  )

# rstanarm::launch_shinystan(fit)

wtf <- rstan::get_inits(fit)

sigma_r <- tidybayes::spread_samples(fit, sigma_r)

hist(sigma_r$sigma_r)

rec_devs <- tidybayes::spread_samples(fit, rec_dev_t[year])

rec_devs %>%
  ggplot(aes(year,rec_dev_t, group = .iteration)) +
  geom_line(alpha = 0.25) + 
  geom_hline(aes(yintercept = 1), color = "red") 


f_t <- tidybayes::spread_samples(fit, f_t[year])

f_t %>%
  group_by(year) %>% 
  summarise(mean_f = mean(f_t)) %>% 
  ggplot(aes(year,mean_f)) +
  geom_line() + 
  ylim(c(0,.4))

f_t %>%
  ggplot(aes(year,f_t, group = .iteration)) +
  geom_line(alpha = 0.5) + 
  ylim(c(0,.4))

lcomps <- tidybayes::spread_samples(fit, p_lbin_sampled[year,lbin])

observed_lcomps <- data$length_comps %>% 
  as_data_frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  gather(lbin, numbers, -year) %>% 
  mutate(lbin = as.numeric(lbin)) %>% 
  group_by(year) %>% 
  mutate(numbers = numbers / sum(numbers))

lcomps %>%
  group_by(year, lbin) %>%
  summarise(mean_n = mean(p_lbin_sampled)) %>%
  group_by(year) %>%
  mutate(mean_n = mean_n / sum(mean_n)) %>%
  ggplot() +
  geom_line(aes(lbin, mean_n, color = year, group = year),show.legend = F) + 
  geom_point(data = observed_lcomps, aes(lbin, numbers), size = .5, alpha = 0.5) +
  facet_wrap(~year) + 
  theme_classic()

selectivity <- tidybayes::spread_samples(fit, mean_selectivity_at_age[age]) 

selectivity%>% 
  ggplot() +
  geom_line(aes(age, mean_selectivity_at_age, group = .iteration),alpha = 0.5) 


```


fing awesome. Now, let's try it with your data






```{r}

experiments <- expand.grid(period = c("end"), window = c(20),
                           economic_model = c(0),
                           fishery = 1:nrow(fisheries_sandbox), stringsAsFactors = F)

temp <- fisheries_sandbox %>%
  filter(fleet_model == "open-access") %>% 
  mutate(fishery = 1:nrow(.)) %>%
  slice(1) %>%
  left_join(experiments, by = "fishery") %>%
  mutate(prepped_fishery = pmap(list(
    prepped_fishery = prepped_fishery,
    window = window,
    period = period,
    prop_years_lcomp_data = 0.25), subsample_data))

temp$experiment <- 1:nrow(temp)

temp <- temp %>% filter(economic_model == 0)
```

```{r}
observed_lcomps <- temp$prepped_fishery[[1]]$scrooge_data$length_comps %>% 
  as_data_frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  gather(lbin, numbers, -year) %>% 
  mutate(lbin = as.numeric(lbin)) %>% 
  group_by(year) %>% 
  mutate(numbers = numbers / sum(numbers))

observed_lcomps %>% 
  ggplot() +
  geom_point(data = observed_lcomps, aes(lbin, numbers), size = .5, alpha = 0.5) +
  facet_wrap(~year) + 
  theme_classic()

```

huh, I mean it certainly doens't look that bad... possible culprits. 

1. the long tails on the right

2. the sample sizes

3. the timing of recruitment 

4. a parameter mismatch. 



```{r}
sfs <- safely(fit_scrooge)

doParallel::registerDoParallel(cores = n_cores)

foreach::getDoParWorkers()

set.seed(42)
fits <- foreach::foreach(i = 1:nrow(temp)) %dopar% {
  out <- sfs(
    data = temp$prepped_fishery[[i]]$scrooge_data,
    fish = temp$prepped_fishery[[i]]$fish,
    fleet = temp$prepped_fishery[[i]]$fleet,
    experiment = temp$experiment[i],
    economic_model = 0,
    scrooge_file = "scrooge",
    iter = 2000,
    warmup = 1000,
    adapt_delta = 0.8,
    max_treedepth = 12,
    max_perc_change_f = 0.2,
    in_clouds = in_clouds,
    cloud_dir = cloud_dir,
    chains = 1,
    cv_effort = 0.5,
    q_guess = mean(possible_q),
    r0 = 100,
    sd_sigma_r = 0.1,
    sigma_f = 0.1,
    cores = 1
  )

} # close fitting loop


fit <- fits[[1]]$result

rstanarm::launch_shinystan(fit)

sigma_r <- tidybayes::spread_samples(fit, sigma_r)

hist(sigma_r$sigma_r)

rec_devs <- tidybayes::spread_samples(fit, rec_dev_t[year])

true_recdevs <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_rec_dev = exp(unique(rec_dev))) %>% 
  filter(year >=179) %>% 
  ungroup() %>% 
  mutate(year = 1:nrow(.))

rec_devs %>%
  left_join(true_recdevs, by = "year") %>% 
  ggplot(aes(year,rec_dev_t, group = .iteration)) +
  geom_line(alpha = 0.25) + 
  geom_point(aes(year,true_rec_dev), color = "red") +
  geom_hline(aes(yintercept = 1), color = "red") 

f_t <- tidybayes::spread_samples(fit, f_t[year]) %>% 
  ungroup() %>% 
mutate(year = year - 1 + 180 )

true_f <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f))

f_t %>%
  left_join(true_f, by = "year") %>% 
  group_by(year) %>% 
  summarise(mean_f = median(f_t),
            true_f = mean(true_f)) %>% 
  ggplot() +
  geom_line(aes(year,mean_f)) + 
  geom_point(aes(year, true_f), color = "red")

f_t %>% 
  ggplot() + 
  geom_line(aes(year, f_t, group = .iteration),alpha = 0.1) + 
  geom_point(data = true_f, aes(year, true_f), color = "red")


lcomps <- tidybayes::spread_samples(fit, p_lbin_sampled[year,lbin])

observed_lcomps <- temp$prepped_fishery[[1]]$scrooge_data$length_comps %>% 
  as_data_frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  gather(lbin, numbers, -year) %>% 
  mutate(lbin = as.numeric(lbin)) %>% 
  group_by(year) %>% 
  mutate(numbers = numbers / sum(numbers))

lcomps %>%
  group_by(year, lbin) %>%
  summarise(mean_n = mean(p_lbin_sampled)) %>%
  group_by(year) %>%
  mutate(mean_n = mean_n / sum(mean_n)) %>%
  ggplot() +
  geom_line(aes(lbin, mean_n, color = year, group = year),show.legend = F) + 
  geom_point(data = observed_lcomps, aes(lbin, numbers), size = .5, alpha = 0.5) +
  facet_wrap(~year) + 
  theme_classic()

selectivity <- tidybayes::spread_samples(fit, mean_selectivity_at_age[age]) 

selectivity%>% 
  ggplot() +
  geom_line(aes(age, mean_selectivity_at_age, group = .iteration),alpha = 0.5) 

```



```{r}
sfs <- safely(fit_scrooge)

doParallel::registerDoParallel(cores = n_cores)

foreach::getDoParWorkers()

set.seed(42)
fits <- foreach::foreach(i = 1:nrow(temp)) %dopar% {
  out <- sfs(
    data = temp$prepped_fishery[[i]]$scrooge_data,
    fish = temp$prepped_fishery[[i]]$fish,
    fleet = temp$prepped_fishery[[i]]$fleet,
    experiment = temp$experiment[i],
    economic_model = 6,
    scrooge_file = "scrooge",
    iter = 2000,
    warmup = 1000,
    adapt_delta = 0.8,
    max_treedepth = 12,
    max_perc_change_f = 0.2,
    in_clouds = in_clouds,
    cloud_dir = cloud_dir,
    chains = 1,
    cv_effort = 0.5,
    q_guess = mean(possible_q),
    r0 = 100,
    sd_sigma_r = 0.1,
    cores = 1,
    sigma_f = 0.2
  )

} # close fitting loop


fit <- fits[[1]]$result

rstanarm::launch_shinystan(fit)

sigma_r <- tidybayes::spread_samples(fit, sigma_r)

hist(sigma_r$sigma_r)

rec_devs <- tidybayes::spread_samples(fit, rec_dev_t[year])

true_recdevs <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_rec_dev = exp(unique(rec_dev))) %>% 
  filter(year >=179) %>% 
  ungroup() %>% 
  mutate(year = 1:nrow(.))

rec_devs %>%
  left_join(true_recdevs, by = "year") %>% 
  ggplot(aes(year,rec_dev_t, group = .iteration)) +
  geom_line(alpha = 0.25) + 
  geom_point(aes(year,true_rec_dev), color = "red") +
  geom_hline(aes(yintercept = 1), color = "red") 

f_t <- tidybayes::spread_samples(fit, f_t[year]) %>% 
  ungroup() %>% 
mutate(year = year - 1 + 180 )

true_f <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f))

f_t %>%
  left_join(true_f, by = "year") %>% 
  group_by(year) %>% 
  summarise(mean_f = mean(f_t),
            true_f = mean(true_f)) %>% 
  ggplot() +
  geom_line(aes(year,mean_f)) + 
  geom_point(aes(year, true_f), color = "red")

f_t %>% 
  ggplot() + 
  geom_line(aes(year, f_t, group = .iteration),alpha = 0.1) + 
  geom_point(data = true_f, aes(year, true_f), color = "red")

lcomps <- tidybayes::spread_samples(fit, p_lbin_sampled[year,lbin])

observed_lcomps <- temp$prepped_fishery[[1]]$scrooge_data$length_comps %>% 
  as_data_frame() %>% 
  mutate(year = temp$prepped_fishery[[1]]$scrooge_data$length_comps_years)%>% 
  gather(lbin, numbers, -year) %>% 
  mutate(lbin = as.numeric(lbin)) %>% 
  group_by(year) %>% 
  mutate(numbers = numbers / sum(numbers))

lcomps %>%
  group_by(year, lbin) %>%
  summarise(mean_n = mean(p_lbin_sampled)) %>%
  group_by(year) %>%
  mutate(mean_n = mean_n / sum(mean_n)) %>%
  ggplot() +
  geom_line(aes(lbin, mean_n, color = year, group = year),show.legend = F) + 
  geom_point(data = observed_lcomps, aes(lbin, numbers), size = .5, alpha = 0.5) +
  facet_wrap(~year) + 
  theme_classic()

selectivity <- tidybayes::spread_samples(fit, mean_selectivity_at_age[age]) 

selectivity%>% 
  ggplot() +
  geom_line(aes(age, mean_selectivity_at_age, group = .iteration),alpha = 0.5)

ppue <- tidybayes::spread_samples(fit, ppue_hat[year])

true_ppue <- data_frame(year = 1:length(temp$prepped_fishery[[1]]$scrooge_data$ppue_t
), ppue = temp$prepped_fishery[[1]]$scrooge_data$ppue_t
) %>% 
  ungroup() %>% 
  mutate(ppue = ppue/max(ppue))


ppue %>%
  left_join(true_ppue, by = "year") %>%
  ggplot() +
  geom_line(aes(year, ppue_hat, group = .iteration),alpha = 0.25) +
  geom_point(data = true_ppue, aes(year, ppue), color = "red") 


# check posterior predictive 

pp_n_tl <- tidybayes::spread_samples(fit, n_tl[year,length_bin]) 

pp_n_tl %>% 
  group_by(year, .chain,.iteration) %>% 
  mutate(p_n_tl = n_tl / sum(n_tl)) %>% 
  group_by(year, .chain, length_bin) %>% 
  summarise(lower_90 = quantile(p_n_tl,0.05),
            upper_90 = quantile(p_n_tl,0.95)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = length_bin, ymin = lower_90, ymax = upper_90), fill = "lightgrey") +
  facet_wrap(~year) + 
  theme_minimal() +
    geom_point(data = observed_lcomps, aes(lbin, numbers), size = .5, alpha = 0.5, color = "red")


```


```{r}

sfs <- safely(fit_scrooge)

doParallel::registerDoParallel(cores = n_cores)

foreach::getDoParWorkers()

set.seed(42)
fits <- foreach::foreach(i = 1:nrow(temp)) %dopar% {
  out <- sfs(
    data = temp$prepped_fishery[[i]]$scrooge_data,
    fish = temp$prepped_fishery[[i]]$fish,
    fleet = temp$prepped_fishery[[i]]$fleet,
    experiment = temp$experiment[i],
    economic_model = 1,
    scrooge_file = "scrooge",
    iter = 2000,
    warmup = 1000,
    adapt_delta = 0.8,
    max_treedepth = 12,
    max_perc_change_f = 0.2,
    in_clouds = in_clouds,
    cloud_dir = cloud_dir,
    chains = 1,
    cv_effort = 0.5,
    q_guess = mean(possible_q),
    r0 = 100,
    sd_sigma_r = 0.1,
    cores = 1
  )

} # close fitting loop


fit <- fits[[1]]$result

rstanarm::launch_shinystan(fit)

sigma_r <- tidybayes::spread_samples(fit, sigma_r)

hist(sigma_r$sigma_r)

rec_devs <- tidybayes::spread_samples(fit, rec_dev_t[year])

true_recdevs <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_rec_dev = exp(unique(rec_dev))) %>% 
  filter(year >=179) %>% 
  ungroup() %>% 
  mutate(year = 1:nrow(.))

rec_devs %>%
  left_join(true_recdevs, by = "year") %>% 
  ggplot(aes(year,rec_dev_t, group = .iteration)) +
  geom_line(alpha = 0.25) + 
  geom_point(aes(year,true_rec_dev), color = "red") +
  geom_hline(aes(yintercept = 1), color = "red") 

f_t <- tidybayes::spread_samples(fit, f_t[year]) %>% 
  ungroup() %>% 
mutate(year = year - 1 + 180 )

true_f <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f))

f_t %>%
  left_join(true_f, by = "year") %>% 
  group_by(year) %>% 
  summarise(mean_f = median(f_t),
            true_f = mean(true_f)) %>% 
  ggplot() +
  geom_line(aes(year,mean_f)) + 
  geom_point(aes(year, true_f), color = "red")

f_t %>% 
  ggplot() + 
  geom_line(aes(year, f_t, group = .iteration),alpha = 0.1) + 
  geom_point(data = true_f, aes(year, true_f), color = "red")

lcomps <- tidybayes::spread_samples(fit, p_lbin_sampled[year,lbin])

observed_lcomps <- temp$prepped_fishery[[1]]$scrooge_data$length_comps %>% 
  as_data_frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  gather(lbin, numbers, -year) %>% 
  mutate(lbin = as.numeric(lbin)) %>% 
  group_by(year) %>% 
  mutate(numbers = numbers / sum(numbers))

lcomps %>%
  group_by(year, lbin) %>%
  summarise(mean_n = mean(p_lbin_sampled)) %>%
  group_by(year) %>%
  mutate(mean_n = mean_n / sum(mean_n)) %>%
  ggplot() +
  geom_line(aes(lbin, mean_n, color = year, group = year),show.legend = F) + 
  geom_point(data = observed_lcomps, aes(lbin, numbers), size = .5, alpha = 0.5) +
  facet_wrap(~year) + 
  theme_classic()

selectivity <- tidybayes::spread_samples(fit, mean_selectivity_at_age[age]) 

selectivity%>% 
  ggplot() +
  geom_line(aes(age, mean_selectivity_at_age, group = .iteration),alpha = 0.5)

ppue <- tidybayes::spread_samples(fit, ppue_hat[year])

true_ppue <- data_frame(year = 1:length(temp$prepped_fishery[[1]]$scrooge_data$ppue_t
), ppue = temp$prepped_fishery[[1]]$scrooge_data$ppue_t
) %>% 
  mutate(ppue = ppue / max(ppue))


ppue %>%
  left_join(true_ppue, by = "year") %>%
  ggplot() +
  geom_line(aes(year, ppue_hat, group = .iteration),alpha = 0.25) +
  geom_point(data = true_ppue, aes(year, ppue), color = "red")

```


```{r}

sfs <- safely(fit_scrooge)

doParallel::registerDoParallel(cores = n_cores)

foreach::getDoParWorkers()

set.seed(42)
fits <- foreach::foreach(i = 1:nrow(temp)) %dopar% {
  out <- sfs(
    data = temp$prepped_fishery[[i]]$scrooge_data,
    fish = temp$prepped_fishery[[i]]$fish,
    fleet = temp$prepped_fishery[[i]]$fleet,
    experiment = temp$experiment[i],
    economic_model = 1,
    scrooge_file = "scrooge",
    iter = 2000,
    warmup = 1000,
    adapt_delta = 0.8,
    max_treedepth = 12,
    max_perc_change_f = 0.2,
    in_clouds = in_clouds,
    cloud_dir = cloud_dir,
    chains = 1,
    cv_effort = 0.5,
    q_guess = mean(possible_q),
    r0 = 100,
    sd_sigma_r = 0.1,
    cores = 1
  )

} # close fitting loop


fit <- fits[[1]]$result

rstanarm::launch_shinystan(fit)

sigma_r <- tidybayes::spread_samples(fit, sigma_r)

hist(sigma_r$sigma_r)

rec_devs <- tidybayes::spread_samples(fit, rec_dev_t[year])

true_recdevs <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_rec_dev = exp(unique(rec_dev))) %>% 
  filter(year >=179) %>% 
  ungroup() %>% 
  mutate(year = 1:nrow(.))

rec_devs %>%
  left_join(true_recdevs, by = "year") %>% 
  ggplot(aes(year,rec_dev_t, group = .iteration)) +
  geom_line(alpha = 0.25) + 
  geom_point(aes(year,true_rec_dev), color = "red") +
  geom_hline(aes(yintercept = 1), color = "red") 

f_t <- tidybayes::spread_samples(fit, f_t[year]) %>% 
  ungroup() %>% 
mutate(year = year - 1 + 180 )

true_f <- temp$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f))

f_t %>%
  left_join(true_f, by = "year") %>% 
  group_by(year) %>% 
  summarise(mean_f = median(f_t),
            true_f = mean(true_f)) %>% 
  ggplot() +
  geom_line(aes(year,mean_f)) + 
  geom_point(aes(year, true_f), color = "red")

f_t %>% 
  ggplot() + 
  geom_line(aes(year, f_t, group = .iteration),alpha = 0.1) + 
  geom_point(data = true_f, aes(year, true_f), color = "red")

lcomps <- tidybayes::spread_samples(fit, p_lbin_sampled[year,lbin])

observed_lcomps <- temp$prepped_fishery[[1]]$scrooge_data$length_comps %>% 
  as_data_frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  gather(lbin, numbers, -year) %>% 
  mutate(lbin = as.numeric(lbin)) %>% 
  group_by(year) %>% 
  mutate(numbers = numbers / sum(numbers))

lcomps %>%
  group_by(year, lbin) %>%
  summarise(mean_n = mean(p_lbin_sampled)) %>%
  group_by(year) %>%
  mutate(mean_n = mean_n / sum(mean_n)) %>%
  ggplot() +
  geom_line(aes(lbin, mean_n, color = year, group = year),show.legend = F) + 
  geom_point(data = observed_lcomps, aes(lbin, numbers), size = .5, alpha = 0.5) +
  facet_wrap(~year) + 
  theme_classic()

selectivity <- tidybayes::spread_samples(fit, mean_selectivity_at_age[age]) 

selectivity%>% 
  ggplot() +
  geom_line(aes(age, mean_selectivity_at_age, group = .iteration),alpha = 0.5)

ppue <- tidybayes::spread_samples(fit, ppue_hat[year])

true_ppue <- data_frame(year = 1:length(temp$prepped_fishery[[1]]$scrooge_data$ppue_t
), ppue = temp$prepped_fishery[[1]]$scrooge_data$ppue_t
) %>% 
  mutate(ppue = ppue / max(ppue))


ppue %>%
  left_join(true_ppue, by = "year") %>%
  ggplot() +
  geom_line(aes(year, ppue_hat, group = .iteration),alpha = 0.25) +
  geom_point(data = true_ppue, aes(year, ppue), color = "red")

```

# pathology with random walk

So I think the problem was that the prior was overwhemling the data in the random walk. It was happiest when making F and sigma F really low, basically keeping it constant, since this made the prior happy, but made the surface really hard to explore. 

# final tuning

Oh of course, the model has a really hard time dealing with recruitment pulses in the early years since the pulse that cause it isn't in the data, especially if selectivity is higher! and so, it wants to compensate with F. Interesting. Is there a way to add a few buffer years of recruitment to the burn-in period? There must be. So the idea here would be to estimate a few more years of recruitment data during the burn-in period to account for the early pulses you see moving through the first years of the length comps. 
Pretty easy to make that dynamic given likely age at selectivity. I.e. 

Also need to do a bit of tuning of the fishery simulator now. Could 

duh. you can't fit to a thing that you're also free to estimate, and in your case, ppue is a linear transformation of the efforts themselves, so it's perfeclt determined. It needs to be a priors or you need to treat it as a random effect with a mean isntead of just a penalty. 


# sorting out recruitment timing

```{r}

nt <- 10

n_burn <- 20

n_total <- nt + n_burn

age_sel <- 7

rec_devs <- 1:(nt + age_sel)

temp_rec <- rep(NA, length(rec_devs))

for (t in 1:(n_total - 1)) {

if (t > (n_burn - age_sel - 1)) {
  
  print(t - n_burn + age_sel + 1)
  
  temp_rec[t - n_burn + age_sel + 1] = rec_devs[t - n_burn + age_sel + 1]

}


}


```

# AHHHHHHH!!!!!! IT WORKS

holy shit, that was the ticket, you idiot. That's what you get for blindly copying in ideas. What you were doing before by decreasing sigma was not increasing the strength of the prior, but decreasing the ability of the model to move year to year. So that's why things fit great. 

By building it into an actualy obs error/process error framework, you now use sigma to specify how little process error there is, i.e. how much creedence to give to the prior structural model. 

With model 0 now, the more you jack up sigma, the more you just let things be whatever the hell they want to be year to year. 

You can use a posterior predictive process to tune this thing. At the moment, it seems like you're being over confident. So, can find a sigma_effort such that test statistics on the posterior predictive look good, i.e. 50% of the time the data are covered by the 50% credible interval. 

Now, this will allow you to build in the fit to the PPUE, and potentially adding in the other parameters!. about. effing. time. 

# early case study check

```{r}

please <- readRDS(here::here("woop.RDS"))

dim(please)

  please$scrooge_fit <- map(please$scrooge_fit , "result")


  please <- please %>%
    mutate(performance = map2(prepped_fishery, scrooge_fit, assess_fits))

  perf_summaries <- please %>%
    mutate(others = map(performance, "others")) %>%
    select(-scrooge_fit,-prepped_fishery, -performance) %>%
    unnest() %>%
    group_by(
      year,
      variable,
      experiment,
      case_study,
      economic_model,
      period,
      window,
      prop_years_lcomp_data
    ) %>%
    summarise(
      lower_90 = quantile(predicted, 0.05),
      upper_90 = quantile(predicted, 0.95),
      lower_50 = quantile(predicted, 0.25),
      upper_50 = quantile(predicted, 0.75),
      mean_predicted = mean(predicted),
      observed = mean(observed)
    )
  
oof <-  perf_summaries %>% 
    group_by(variable, case_study, economic_model, period, window, prop_years_lcomp_data) %>% 
    summarise(rmse = sqrt(mean(mean((mean_predicted - observed)^2))))

oof %>% 
  ggplot(aes(economic_model, rmse)) + 
  geom_point()


```

# problem children


```{r}

a <- map(case_study_fits, "result")


b <- map(a, get_elapsed_time)

pfit <- a[[1]]


  scrooge_model <-
    rstan::stan_model(here::here("src", paste0(scrooge_file, ".stan")), model_name = scrooge_file)

  sfs <- safely(fit_scrooge)

 test <-
    foreach::foreach(i = 3) %dopar% {
      out <- sfs(
        chains = 1,
        cores = 1,
        refresh = 25,
        scrooge_model = scrooge_model,
        data = case_studies$prepped_fishery[[i]]$scrooge_data,
        fish = case_studies$prepped_fishery[[i]]$fish,
        fleet = case_studies$prepped_fishery[[i]]$fleet,
        experiment = case_studies$experiment[i],
        economic_model =  case_studies$economic_model[i],
        likelihood_model = case_studies$likelihood_model[i],
        scrooge_file = "scrooge",
        iter = 2000,
        warmup = 1000,
        adapt_delta = 0.95,
        max_treedepth = 8,
        max_perc_change_f = 0.01,
        in_clouds = in_clouds,
        cloud_dir = cloud_dir,
        cv_effort = 0.5,
        q_guess = mean(possible_q),
        r0 = 100,
        sd_sigma_r = 0.4,
        sigma_effort = 0.2,
      )
    } # close fitting loop
 
 problem <- case_studies %>% slice(3)
 
 problem$scrooge_fit <- list(test[[1]]$result)

  problem <- problem %>%
    mutate(performance = map2(prepped_fishery, scrooge_fit, assess_fits))


perf_summaries <- problem %>%
    mutate(others = map(performance, "others")) %>%
    select(-scrooge_fit,-prepped_fishery, -performance) %>%
    unnest() %>%
    group_by(
      year,
      variable,
      experiment,
      case_study,
      economic_model,
      likelihood_model,
      period,
      window,
      prop_years_lcomp_data
    ) %>%
    summarise(
      lower_90 = quantile(predicted, 0.05),
      upper_90 = quantile(predicted, 0.95),
      lower_50 = quantile(predicted, 0.25),
      upper_50 = quantile(predicted, 0.75),
      mean_predicted = mean(predicted),
      mean_predicted = median(mean_predicted),
      observed = mean(observed)
    )


  perf_summaries %>%
    filter(variable == "f") %>%
    ggplot() +
    geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90), fill = "lightgrey") +
    geom_ribbon(aes(year, ymin = lower_50, ymax = upper_50), fill = "darkgrey") +
    geom_line(aes(year, mean_predicted), color = "steelblue") +
    geom_point(
      aes(year, observed),
      fill = "tomato",
      size = 4,
      shape = 21
    ) +
    labs(y = "", x = "Year") +
    facet_grid(likelihood_model ~ economic_model) +
    theme_minimal()
 
 rstanarm::launch_shinystan(map(test, "result")[[1]])

```

Oh that's freaking awesome. Estimating max percentage change instead of the stupid p response factor makes like waaaay better. 

Let's test it against a bunch of open access


```{r}

case_studies <- case_studies %>% 
  filter(economic_model %in% c(1,2))

  case_studies <- case_studies %>% 
    sample_n(3)

  
  
 scrooge_model <-
    rstan::stan_model(here::here("src", paste0(scrooge_file, ".stan")), model_name = scrooge_file)

  sfs <- safely(fit_scrooge)

  doParallel::registerDoParallel(cores = n_cores)

  foreach::getDoParWorkers()
  
  set.seed(42)
  case_study_fits <-
    foreach::foreach(i = 1:nrow(case_studies)) %dopar% {
      out <- sfs(
        chains = 1,
        cores = 1,
        refresh = 25,
        scrooge_model = scrooge_model,
        data = case_studies$prepped_fishery[[i]]$scrooge_data,
        fish = case_studies$prepped_fishery[[i]]$fish,
        fleet = case_studies$prepped_fishery[[i]]$fleet,
        experiment = case_studies$experiment[i],
        economic_model =  case_studies$economic_model[i],
        likelihood_model = case_studies$likelihood_model[i],
        scrooge_file = "scrooge",
        iter = 2000,
        warmup = 1000,
        adapt_delta = 0.9,
        max_treedepth = 8,
        max_perc_change_f = 0.2,
        in_clouds = in_clouds,
        cloud_dir = cloud_dir,
        cv_effort = 0.5,
        q_guess = mean(possible_q),
        r0 = 100,
        sd_sigma_r = 0.4,
        sigma_effort = 0.2,
      )
    } # close fitting loop

  case_studies$scrooge_fit <- map(case_study_fits, "result")

  case_studies <- case_studies %>%
    mutate(performance = map2(prepped_fishery, scrooge_fit, assess_fits))
  
   perf_summaries <- case_studies %>%
    mutate(others = map(performance, "others")) %>%
    select(-scrooge_fit,-prepped_fishery, -performance) %>%
    unnest() %>%
    group_by(
      year,
      variable,
      experiment,
      case_study,
      economic_model,
      likelihood_model,
      period,
      window,
      prop_years_lcomp_data
    ) %>%
    summarise(
      lower_90 = quantile(predicted, 0.05),
      upper_90 = quantile(predicted, 0.95),
      lower_50 = quantile(predicted, 0.25),
      upper_50 = quantile(predicted, 0.75),
      mean_predicted = mean(predicted),
      mean_predicted = median(mean_predicted),
      observed = mean(observed)
    )


  perf_summaries %>%
    filter(variable == "f") %>%
    ggplot() +
    geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90), fill = "lightgrey") +
    geom_ribbon(aes(year, ymin = lower_50, ymax = upper_50), fill = "darkgrey") +
    geom_line(aes(year, mean_predicted), color = "steelblue") +
    geom_point(
      aes(year, observed),
      fill = "tomato",
      size = 4,
      shape = 21
    ) +
    labs(y = "", x = "Year") +
    facet_wrap(~experiment, scales = "free") +
    theme_minimal()


  rstanarm::launch_shinystan(case_studies$scrooge_fit[[3]])

```



```{r}

performance <- case_studies %>% 
  select(case_study, economic_model, likelihood_model, performance) %>% 
  mutate(rmse = map(performance, ~.x$others %>% group_by(variable) %>% 
  summarise(rmse = sqrt(mean(sq_er))))) %>% 
  select(-performance) %>% 
  unnest()



performance %>% 
  ggplot(aes(case_study, rmse, fill = factor(economic_model))) + 
  geom_boxplot() + 
  coord_cartesian(ylim = c(0,3))

```



```{r}

others <- map(case_studies$performance, "others")

f_and_r_performance <- case_studies %>% 
  select(-performance,-scrooge_fit, -prepped_fishery) %>% 
  mutate(others = others) %>% 
  unnest()

rmse_and_bias <- f_and_r_performance  %>% 
  group_by(economic_model, likelihood_model,prop_years_lcomp_data, case_study, variable) %>% 
  summarise(rmse = sqrt(mean(sq_er)),
            bias = mean(predicted - observed))

rmse_and_bias %>% 
  filter(variable %in% "f") %>% 
  ggplot(aes(factor(likelihood_model), rmse)) + 
  ggbeeswarm::geom_beeswarm() + 
  facet_wrap(~variable)


```


case study one: realistic fishery, compare LIME to 1 + 1


```{r}
cs1 <- perf_summaries %>% 
  filter(case_study == "realistic",
         prop_years_lcomp_data == 1) %>% 
  # filter(case_study == "realistic",
  #        (economic_model == 0 & likelihood_model == 0) | (economic_model == 0 & likelihood_model == 1),
  #        prop_years_lcomp_data == 1) %>% 
  mutate(model = glue::glue("em{economic_model}-lm{likelihood_model}"))

cs1 %>% 
  filter(variable == "f") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = model), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = model)) + 
  labs(x = "Year", y = "Fishing Mortality") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

cs1 %>% 
  filter(variable == "recruitment_deviates") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = model), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = model)) + 
  labs(x = "Year", y = "recruitment_deviates") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

cs1 %>% 
  filter(variable == "ppue") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = model), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = model)) + 
  labs(x = "Year", y = "ppue") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

```


```{r}

cs2 <- perf_summaries %>% 
  ungroup() %>% 
  filter(case_study == "realistic",
         (economic_model == 0 & likelihood_model == 0) | (economic_model == 1 & likelihood_model == 1),
         prop_years_lcomp_data == min(prop_years_lcomp_data))

cs2 %>% 
  filter(variable == "f") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = factor(economic_model)), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = factor(economic_model))) + 
  labs(x = "Year", y = "Fishing Mortality") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

cs2 %>% 
  filter(variable == "recruitment_deviates") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = factor(economic_model)), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = factor(economic_model))) + 
  labs(x = "Year", y = "recruitment_deviates") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

cs2 %>% 
  filter(variable == "ppue") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = factor(economic_model)), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = factor(economic_model))) + 
  labs(x = "Year", y = "ppue") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

```

```{r}

cs3 <- perf_summaries %>% 
  ungroup() %>% 
  filter(case_study == "decoupled",
         (economic_model == 0 & likelihood_model == 0) | (economic_model == 1 & likelihood_model == 1),
         prop_years_lcomp_data == max(prop_years_lcomp_data))

cs3 %>% 
  filter(variable == "f") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = factor(economic_model)), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = factor(economic_model))) + 
  labs(x = "Year", y = "Fishing Mortality") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

cs3 %>% 
  filter(variable == "recruitment_deviates") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = factor(economic_model)), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = factor(economic_model))) + 
  labs(x = "Year", y = "recruitment_deviates") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

cs3 %>% 
  filter(variable == "ppue") %>% 
  ggplot() + 
  geom_point(aes(year, observed), fill = "tomato", size = 2, alpha = 0.75, shape = 21) +
  geom_ribbon(aes(year, ymin = lower_90, ymax = upper_90, fill = factor(economic_model)), alpha = 0.2) +
  geom_line(aes(year,mean_predicted, color = factor(economic_model))) + 
  labs(x = "Year", y = "ppue") + 
  ggsci::scale_color_aaas() + 
  ggsci::scale_fill_aaas()

```


Interesting, why isn't the PPUE coming into the equation more?

```{r}
reserve <- case_studies

case_studies %>% 
  filter(experiment %in% unique(cs1$experiment))

rstanarm::launch_shinystan(case_studies$scrooge_fit[[2]])


```

# Whats up with the bad fit cases?


```{r}


a <- sandbox_performance %>% 
  ungroup() %>% 
  arrange(desc(rmse)) %>% 
  slice(1)

bad_fishery <- a$fishery

bad_experiment <- a$experiment


  hmm <- processed_sandbox %>%
    filter(experiment == bad_experiment) %>% 
    select(-scrooge_fit,-prepped_fishery,-summary_plot,-fleet_params) %>%
    unnest() %>%
    group_by(
      year,
      variable,
      experiment,
      fishery,
      economic_model,
      likelihood_model,
      period,
      window,
      prop_years_lcomp_data
    ) %>%
    summarise(
      lower_90 = quantile(predicted, 0.05),
      upper_90 = quantile(predicted, 0.95),
      lower_50 = quantile(predicted, 0.25),
      upper_50 = quantile(predicted, 0.75),
      mean_predicted = mean(predicted),
      mean_predicted = median(mean_predicted),
      observed = mean(observed)
    )

hmm %>% 
  filter(experiment == bad_experiment) %>% 
  filter(variable == "f") %>% 
  ggplot() + 
  geom_line(aes(year, mean_predicted)) + 
  geom_point(aes(year, observed))

  wtf <- processed_sandbox %>%
    filter(experiment == bad_experiment) 
  
  wtf$prepped_fishery[[1]]$simed_fishery %>% 
    group_by(year) %>% 
    summarise(q = unique(f / effort)) %>% 
    ggplot(aes(year, q)) + 
    geom_point()
  
  wtf$summary_plot

  fisheries_sandbox <- wtf

    future::plan(future::multiprocess, workers = 1)

      spf <- safely(prepare_fishery)
      
      fisheries_sandbox$fleet_model <- "random-walk"

          fisheries_sandbox$sigma_effort[fisheries_sandbox$fleet_model == "random-walk"] <- runif(sum(fisheries_sandbox$fleet_model == "random-walk"), 0.1, 0.4)

  
      
  blerg <- fisheries_sandbox %>% 
    mutate(prepped_fishery = 
    furrr::future_pmap(
      list(
        sci_name = sci_name,
        fleet_model = fleet_model,
        fleet_params = fleet_params,
        sigma_r = sigma_r,
        sigma_effort = sigma_effort,
        price_cv = price_cv,
        cost_cv = cost_cv,
        q_cv = q_cv,
        price_slope = price_slope,
        cost_slope = cost_slope,
        q_slope = q_slope,
        price_ac = price_ac,
        cost_ac = cost_ac,
        q_ac = q_ac,
        steepness = steepness,
        percnt_loo_selected = percnt_loo_selected,
        obs_error = obs_error,
        initial_f = initial_f,
        r0 = r0,
        price = price,
        q = q,
        profit_lags = profit_lags,
        max_perc_change_f = max_perc_change_f,
        max_cp_ratio = max_cp_ratio,
        beta = beta),
        spf,
        sim_years = 100,
        burn_years = 100,
        cv_len = 0.2,
        linf_buffer = 1.25
      ))
  

  no_error <- map(blerg$prepped_fishery, 'error') %>%
    map_lgl(is.null)

  blerg <- blerg %>%
    filter(no_error == T)

  blerg <- blerg %>%
    mutate(prepped_fishery = map(prepped_fishery, "result")) %>%
    mutate(summary_plot = map(prepped_fishery, plot_simmed_fishery))

  blerg$summary_plot
  

  
```



