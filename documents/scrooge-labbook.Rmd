---
title: "Lab notebook"
author: "Dan Ovando"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
---

Arlington had radioacoustic tags for CPUE and viability in reserves 

Olivier critique the non-fished species aren't a good control. This is the lynchpun

* Following recommendations of "bayesian primer"; think of the biomass as a hierarchical process where the observed biomass is a random variable drawn from a true model, say log-normal, with a mean of observed numbers-at-length times weight ogive and standard deviation, where the numbers-at-length are drawn from a multinomial or something like that. Need to draw this out but that's the right way to account for all this. You'll follow a similar process in project `zissou`, but now you're more concerned with modeling the age structure itself. See Box 6.2.2

# Ideas

- Think about crowding of other vessels as a covariate in the model: both a sign of abundance but also an incentive to go elsewhere if vessels are competing/crowding each other out

## The Pivot

Zissou is now about economic priors. 

Project Summary: 

Many fisheries around the world require management guidance but lack the robust data streams that underpin state-of-the art science driven fisheries management. To resolve this problem, a large and growing suite of "data-limited stock assessments" (DLAs) have emerged, designed to provide management advice using relatively minimal data (but generous assumptions). Interestingly though, nearly all of the quantitative DLAs established rely exclusively on fish-centric data, for example length frequency distributions, catch data, or CPUE trends. While these data are clearly critical to proper understanding of a fishery's status, they also ignore the potential light that the economic history of a fishery may shed on its current biological status. This project proposes to address this challenge, by developing a quantitative method for integrating economic information into the stock assessment process. Length-based DLAs provide a useful entry point for this process. Length-based approaches often rely on equilibrium assumptions in order to "account" for recruitment fluctuations. However, this assumption is rarely justified. Attempts to relax this assumption can be hampered by the difficulties in separating changes in recruitment from changes in fishing mortality. We propose to address this challenge by utilizing economic data to set informative priors on the rate of change of fishing mortality.

Proposed Methods:

  - Collect historic data on fishing mortality *f* and economic data (e.g. prices, costs, labor) for a series of case-study fisheries
  
  - Explore ability of alternative models to utilize economic data to predict observed changes in *f*
  
  - Simultaneously explore simulation testing of evolution of *f* under different economic assumptions
  
  - Once (hopefully) a suitable functional for for estimating changes in *f* as a function of economic data is established, begin integration to DLAs
  
  - Develop model to use lengths (plus additional data as desired) to attempt to estimate a vector of *f*  and recruitment *r* over time. Use the established economic model to provide informative priors on the evolution of *f* over time. 
  
  - Simulation test model
  
  - Test against stock assessments 

Key Questions:

- What might be candidate fisheries for empirical exploration?
    - Something with long time period of *f*, including *f* from period unconstrained by management
    - Must also have access to concurrent dataset of economic data (prices, cost, labor, fleet size, etc.)
  
- Are there good examples of similar approaches in fisheries/other disciplines
    - Predicting demand? E.g. how many widgets should you stock up on this year based on widget consumer behavior last year

- What conditions would make this a non-starter
    - All models are wrong, when is this one useful?
    - What might be management scenarios that would completely invalidate this approach
  
- As an economist, what evidence would you need to see to be convinced of this idea?
  
  
# 2017-05-15

Sent email to Erin Steiner to get the data request ball rolling again https://www.nwfsc.noaa.gov/contact/display_staffprofile.cfm?staffid=2870

Also, don't forget about the regulations database at 

http://calcomfish.ucsc.edu/regulation_main.asp


# Idea for empirical model exploration

Holy cow, this could be a great way to start. Build of Cody's framework for classifying the "predictability" of a fishery. Take the RAM dynamics. Your goal is to predict the dynamics of the fishery using available data. Consider for example the change in fishing effort and or F over time. Can that change by predicted by a simple model of profits and expansion factor?

So, take the F or effort. Calcualte the changes. In each time step, calculate the "profits", as price * catch - cost * effort ^ beta, where you break the cost thing into component factors of labor and fuel, times some coefficient, plus some constant ( c= 1 + fuel * effort + labor * effort). 

See how good that is at predicting dynamics. Test out more complicated forms, a random forest approach, and a GAM based approach. This will be your evidence for the ability of economic parameters to guide stock assessment. 

You can then work on degrading the data to develop a sense for the minimum amount needed to be informative. 

As a simple one, regress delta F on delta profits, on economic indicators, and use those coefficients to describe coefficients for the prior idea. 


# 2017-07 Check in

OK, you really need to make some serious progress on this thing. 

There seem to be a few key things that you need to iron out. 

The first is what you are actually interested in modeling here. Are you trying to estimate F? Effort? Change in F? Actual biomass? SPR?

The cleanest goal still seems to be an informative prior on F based on changes in economic conditions, building off the LIME package. 

But, this should be informed by reading that summary paper that just came out...

Once you have that set out, you need to actually write out the model that you're going to use, how you plan on estimating it, and the theory underpinning it

From there you need data. That will come from two places. A really fast operating model, which I'm thinking needs to be C++ based, since it needs to be able to be spatial, with lots of complex fleet bells and whistles, and I want it to fly. 

You'll also need to try and collect empirical data from some RAM based things

Then test away. 

From there you need to write the model to estimate it

# Paper

Theory + simulation (what you were already pitching)

Theory + emprical (show in a few different places)

Theory: when would the tipping points of value be?


# March 2018 launch party

Thorson et al. 2013 seems liek a good starting place, along with Vasconcellos & Cochrane

@Thorson2013 is a decent starting place, but doesn't really give much besides literature for the general idea. But, that functional form isn't all that useful for me

Neilsen 2017 (@Nielsen2017) is that lit review of coupled ecological-economic models. Ah right, these are all MSE style models though, not assessment modules. So, a good reference for ways to model effort dynamics, but not redundant on the effort dynamics. 

So, the tricky part in this is going to be thinking about how to not cheat too much. Obviously if you model effort as a function of price and pass a prior on that the model is going to work pretty damn well. So, to make realistic you need to introduce what, the simplest would just be process error running on up to model error. 

From a starting point, it would be nice to stick with demonstrating with more or less open access dynamics, and messing around with process/observation error around that. 


So, step one is writing out your economic model. As an easy starting point, you could with that model write out what dE/Dp/c/q/b is, to think about what the shape of the priors would actually be. 

One nice wrinkle in this would be to have the assessment model assume 

# Structure of F priors

consider something of the form 

$$f_{t+1} = f_{t} + \epsilon{P_{t}}$$

To rearrange that then, we can say that 

$$\Delta^f_{t} = \epsilon{P}_{t}$$

Expanding that out then we get

$$\Delta^f_{t} = \epsilon(p_{t}q_{t}E_{t}b_{t} - cE^{b})$$

And you could start taking derivatives from there... if you want to think about an x unit change in delta F as a function of a Y change in a covariate. 

Or, and this is interesting, you could think about it like elasticities? hmmm

One may to make this a bit more tractable. Suppose that there is some max fleet expansion factor, e.g. 120%, and some min, like 0%. So, in any given year you could either expand f to some max value, or decrease it to zero if you wanted to. Then, what you estimate is the % of this expansion percentage. So a massive increase in prices, decreases in cost, increase in tech, produces a massive/small part of that max change

Make yourself a damn shiny app to think this through. as well, or at least make some plots and see what happens. 

could use `dagR` to draw model graphs

So, a relatively simple step would just be to think about each of the parameters in here as a constant besides the variable that you "have" and take derivatives. 

i.e. the df/dp would be qEb, df/dq would be pEB, df/dc would be -E^b^, etc. 

This has some appeal, the problem though is it's trickier to think about what happens if you have arbitrary numbers and combinations of parameters, e.g. what happens if both p and q are changing? You can certainly look up the math on that one... joint derivative?

An alternative though would be a much more structural approach. 

Suppose that you have t years of length frequency data. Right now, let's think about how LIME/LBSPR wowrks. 

It estimates a vector of recruits and f's and selectivities and then compares the observed vs predicted length distributions. 

Now let's focus on the f priors. Right now, it's just saying that the f in any given year has the f in the last year as a prior. 

Now suppose that instead of that you let f evolve through some bioeconomic driver. 

You start the population at r0. Now, you let the F's evolve according to your open access model, holding constant the things that hold constant, and changing the things you have some prior knowledge of. That now gives you a vector of F's, which combined with your recruitment estimates fives you an expected length distribution. 

Now, you could either treat those F's as priors on the latent F's estimated by the model, or maybe better, estimate F deviates with means based on the economic model!


So now, you'd estimate deviates with mean 0 and some standard deviation, around the bioeconomically estimated F's. So in that sense the bioeconomic model is still a prior on the final F... booya? 

The reall nice thing abotu this is that it abstracts to any time series or combination of economic parameters that you have, and any functional form for F that comes in!!!!!!

As a killer extension, you could then repeat the process under alternative model specifications for the bioeconomic side, and pick the model with the most support!. 

Oh I really like this. 

So what needs to happen to make this happen. 

1. You need to modularize the "fleet model" side of things, rather than just if toggles in the code. The end goal is that sim_fishery can now take economic "data" as inputs

2. You need to revisit your length-comps sampling and make sure that that thing works well. 

3. That's the core barrier at this point. Once you have that, you can use spasm to simulate length comps over time given those parameters, and then you pass that on to scrooge

BOOYA. 



# Sea Grant Abstract Update

Many fisheries around the world require management guidance but lack the robust data that underpin state-of-the-art fisheries management. To resolve this problem, a large and growing suite of "data-limited stock assessments" (DLAs) has emerged, designed to provide management advice using relatively minimal data (but generous assumptions). Interestingly though, nearly all of the quantitative DLAs established rely exclusively on fish-centric data, for example length frequency distributions, catch data, or CPUE trends. While these data are clearly critical to proper understanding of a fishery's status, the economic history of a fishery can also shed light on its current biological status. This project proposes to address this challenge, by developing a quantitative method for integrating economic information into the stock assessment process. Length-based DLAs provide a useful entry point for this process. Length-based DLAs either rely on equilibrium assumptions or are faced with the challenge of disentangling trends in fishing mortality from trends in recruitment. We demonstrate how integration of often available data on the economic history of a fishery, such as prices, costs, and labor, can improve the performance of these length-based DLAs. Our expected result is a user-friendly tool for helping communities utilize length and economic data to better manage their fisheries.

# model development

This section is for sketching out the structure of the scrooge estimation model. Here goes nothing!

We're going to stary by writing out the likelihoods and components for the model, as well as the DAC for the model. 

Once you've got that working, it's off to stan for fitting fun! Once that works you can code up a TMB version as well, but I feel more confident in diagnosing STAN


## DAC

## Likelihood

The only true "likelihood" in here is relating the observed vs predicted length frequencies. 

This will likely be some sort of multinomial likelihood. LIME uses Dirichlet-multinomial log-likelihood, think I'll just stick the multinomial for right now to make life a little simpler to start. 

So, this will look like 

$$l_{b,y} \sim multinom(p_{b,t}) $$

where p~b,y~ is the probability of being observed in length bin *b* in time *t*. Stan aslo provides upporto for the dirichelet but let's come to that later. 

So, from there

$$p_{b,y} = \frac{n_{b,t}}{\sum_{1:B}n_{b,t}}$$


the number in each length bin is then your growth function

$$ n_{b,t} = g(f_t,r_t,s_b,bio)$$
Where *bio* is biological data (growth, mortality, etc)

recruitment will be mean BH/ricker, with some sigma

$$r_{t} = bh(ssb_{t}, bio)e^{rdev_t - \sigma_r^2/2} $$

and 

$$ rdev \sim normal(0,\sigma_r)$$

and borrowing from LIME

$$ log(\sigma_r) \sim normal(0.7,0.2)$$

Now, the fun part. estimating F goes actually pretty damn similarly. The current idea is to estiamte F deviates, with the prior being the bioeconomic model. It gets a little tricky thinking about scale here though. 

Suppose that you hold everything constant and just let the bioeconomic model evolve over time. The problem there is that that would require tuning of the qs and things to get the f's in the right units, even if the trend is in the right ballpark. i.e., to say that f is mean bioeconomic f times some deviate, where teh deviate is mean 0, then on avereage the fleet model needs to be getting the scale of the f's correct. 

What if instead you leave the fleet model estimating latent F's, but you put a prior on the change in F based on the economic model. 

Jim does something like 

$$f_t \sim normal(f_{t-1},\sigma_f)$$

Now, suppose that you add in a little wrinkle


$$f_t \sim normal(\Delta_{f}f_{t-1},\sigma_f)$$

Where 

$$\Delta_{f} = \frac{\hat{f_t}}{\hat{f_{t-1}}} $$

where 

$$\hat{f_{t}} = q_tE_t$$

and 

$$E_t  = E_{t-1} + \theta\Pi_{t-1}$$

Where finally 

$$\Pi_{t} = p_tq_tE_tb_t - cE_t^\beta $$
sweeeeet. 

Now, one thing that is a royal pain in the ass in here is $\theta$: Since it scales to profits it's pretty hard to make generic. Do you want a \$1 increase in profits to equal a 1 unit increase in effort? a 1000 increase in effort?

It would be nice to come up with a way to make this a bit cleaner. 

A right, one night thing in here is that because you're dealing with delta effort, you can set the effort itself at some arbitrary level, as well as the r0 so that at least helps you scale things somewhat, given reasonable prices and costs, all of which can be scaled to be relative. 

i.e. you set p,q,E,b,c,beta to baseline levels, and then just amplify those baselines based on data. 

From there you could mess around to set theta at some reasonable level. 

You could also actually think about allowing $\theta$ to be a free parameter with highly informative priors? 

What does sscom do about this?

ah right, they do that by working with biomasses instead of profits, since a given biomass would produce bionomic equilibrium. The problem there though is that that assumes that price etc. are constant, which you implicitly want to change. 

So maybe for now play with what a reasonable level of theta would be such that effort dynamics given yourbase values are somewhat reasonable. 

Use something like ssb0

find $\theta$ such that 

$$1 - \frac{E_{t0 + 1}}{E_{t0}} \sim target \sim 0.25  $$

and 

$$E_{t0 + 1} = E_{t0} + \theta(pqE_{t0}B_{t0} - cE_{t0}^\beta)$$

which would be

$$ target = \theta(pqB_{t0} - cE_{t0}^{\beta - 1})$$

and then 

$$\theta = \frac{target}{(pqB_{t0} - cE_{t0}^{\beta - 1})}$$

```{r}
p <- 1
q <- .01
E <- 100
b <- 1000
cost <- 1
beta <- 1.3
target <- 0.25

profits <- p*q*E*b - cost * E^beta

theta = target / (p * q * b - cost*E^(beta - 1))

E2 = E + theta * profits

```
Which works so long as the fishery is profitable at B0, which makes sense, why would you model something that wasn't profitable at the getgo. So, set a stop that would say fishery is unprofitable at virgin biomass if theta comes up nevative. 

You could then either set that theta as data, or let be a parameter with that theta as a highly informative prior. 

Let's go back to spasm and make that adjustment

## test case


```{r}
library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

fish <-
create_fish(
query_fishlife = T,
mat_mode = "length",
max_age = 30,
time_step = 1
)

fleet <- create_fleet(fish = fish)

manager <- create_manager(mpa_size = 0)

fleet <-
spasm::update_fleet(
fleet = purrr::list_modify(
fleet,
fleet_model = "open-access",
initial_effort = 1,
theta = 0.01,
cost = 20,
price = 1,
target_catch = 100,
sigma_effort = 0,
length_50_sel = 25,
theta_tuner = 0.5
),
fish = fish
)

sim <- spasm::sim_fishery(
fish = fish,
fleet = fleet,
manager = manager,
num_patches = 1,
sim_years = 100,
burn_year = 50,
time_step = fish$time_step
)

sim %>%
group_by(year) %>%
summarise(ssb = sum(effort)) %>%
ggplot(aes(year, ssb)) +
geom_line() +
geom_point()

length_and_age_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age)


length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = 2
)

length_at_age_key <- length_at_age_key %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_and_age_comps <- length_and_age_comps %>%
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'population',
percent_sampled = 1,
time_step = fish$time_step
)
))

length_and_age_comps <- length_and_age_comps %>%
mutate(catch_ages = map(
catch_length,
~ length_to_age(
length_samples = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
max_age = fish$max_age,
min_age = fish$min_age,
time_step = fish$time_step
)
))

length_and_age_comps %>%
filter(year == max(year)) %>%
select(catch_length) %>%
unnest() %>%
filter(length_bin < 150) %>%
ggplot() +
geom_col(aes(length_bin, numbers)) +
geom_vline(xintercept = fish$linf) +
geom_vline(xintercept = fish$length_at_age)
```

Not bad. Might need to go back and double check some of your aging ideas, but it mostly works. If you were catually that close to the true age structure in real life you'd be very excited. 

Interesting, something is going wrong in the length-to-ages at the far right end. Let's take a brief look at this some more. 

OK fixed somewhat, but going to ignore the problems that persist there since who really cares about age comps for this assessment: it's length based. 


## Testing stan fit

```{r}

scrooge_data <- list(
  nt = 20,
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
)

test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1)

check<- rstan::extract(test_scrooge)

test_numbers <- array_branch(check$n_ta,1)

test_numbers <- data_frame(thing = map(test_numbers, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_numbers <- test_numbers %>% 
  filter(iteration == 1) %>% 
  select(-iteration)

test_ssb <- array_branch(check$ssb_ta,1)

test_ssb <- data_frame(thing = map(test_ssb, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_ssb <- test_ssb %>% 
  filter(iteration == 1) %>% 
  select(-iteration)

test_catch <- array_branch(check$cn_ta,1)

test_catch <- data_frame(thing = map(test_catch, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_catch <- test_catch %>% 
  filter(iteration == 1) %>% 
  select(-iteration)


total_ssb <- rowSums(test_ssb)

recruits <- test_numbers$V1

plot(total_ssb, lead(recruits))

test_catch %>% 
  slice(20) %>% 
  gather(age, numbers) %>% 
  mutate(age = str_replace_all(age,"\\D","") %>% as.numeric()) %>% 
  ggplot(aes(age, numbers)) + 
  geom_col()


test_length <- array_branch(check$n_tl,1)

test_length <- data_frame(thing = map(test_length, as_data_frame)) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()


test_length <- test_length %>% 
  filter(iteration == 1) %>% 
  select(-iteration)




a = sim %>% 
  filter(year == min(year)) %>% 
  select(age, numbers_caught)

b = t(a$numbers_caught) %*% as.matrix(length_at_age_key)

test_catch %>% 
  slice(1) %>% 
  gather(age, numbers) %>% 
  mutate(true_numbers = as.numeric(a$numbers_caught)) %>% 
  mutate(age = str_replace_all(age,"\\D","") %>% as.numeric()) %>% 
  gather(source, numbers,-age)  %>% 
  group_by(source) %>% 
  mutate(snumbers = numbers / sum(numbers)) %>% 
  ggplot(aes(age, snumbers, fill = source)) + 
  geom_col(stat = "identity", alpha = 0.5, position = "dodge")



```

God damn dude, nicely done. Now, add in generation of length comps. This is going to be fuuuuun. 

### generating length comps

For now, don't worry about survey selectivity of the catch itself. For now, just focus on generating a length distribution from the catch. In theory, you're going to pass samped data, which should be the "true" length distribution of the catch. You're now going to try and match those length comps with the length comps of the catch that would result from your model. 


So, the simplest thing to do would be to generate in R a matrix that is rows age, columns probability in each length bin at age. 

Then, you just take the catch at age, and multiply by the proportion in each age at eathc length bin, then sum the columns. 

```{r}

set.seed(42)
n_at_a <- matrix(rnorm(31,100))

d =  t(n_at_a) %*% (as.matrix(length_at_age_key))


b =  t(as.matrix(length_at_age_key)) %*% n_at_a


as.matrix(length_at_age_key) %*% t(n_at_a)

```

## Setting up likelihoods

OK, let's pass in the lengths!


```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

rstan_options(auto_write = TRUE)

fish <-
create_fish(
query_fishlife = T,
mat_mode = "length",
max_age = 30,
time_step = 1
)

fleet <- create_fleet(fish = fish)

manager <- create_manager(mpa_size = 0)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
fish = fish,
fleet = fleet,
manager = manager,
num_patches = 1,
sim_years = 25,
burn_year = 50,
time_step = fish$time_step
)


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
estimate_recruits = 0,
length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year)) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")


# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison
  
  
```

hell yes. model now nails F perfectly in a perfect world with perfect data. 

Let's see how much of that was just the warmup...

```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

rstan_options(auto_write = TRUE)

fish <-
create_fish(
query_fishlife = T,
mat_mode = "length",
max_age = 30,
time_step = 1
)

fleet <- create_fleet(fish = fish)

manager <- create_manager(mpa_size = 0)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 10,
      cost = 2,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )

sim <- spasm::sim_fishery(
fish = fish,
fleet = fleet,
manager = manager,
num_patches = 1,
sim_years = 25,
burn_year = 50,
time_step = fish$time_step
)


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
  estimate_recruits = 0,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <-
  rstan::stan(
  file = here::here("scripts", "old-scrooge.stan"),
  data = scrooge_data,
  chains = 1,
  refresh = 10,
  cores = 1,
  iter = 4000,
  warmup = 2000
  )
  
check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year)) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")


# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison
  

```


IN the meantime, best damn idae I've had in a while: train a model to predict accuracy for F, SPR, trend in F, etc, across the range of scenarios that you try!!!!!

So, you'd plug in your fishery characteristics, and the model would give an idea of how accurate it is likely to be!!

Awesome idea. 

## Adding in recruitment estimation

```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

rstan_options(auto_write = TRUE)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1
  )



manager <- create_manager(mpa_size = 0)

fleet <- create_fleet(fish = fish, sigma_effort = 0)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      length_50_sel = 50,
      theta_tuner = 0.1,
      beta = 1.3
    ),
    fish = fish
  )


set.seed(42)

sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = manager,
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(tb = sum(biomass)) %>% 
  ggplot(aes(year, tb)) + 
  geom_point()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")




```


## adding in econ

OK, here we go. Let's get the econ module up and running. 

Now, one slightly weird thing in here. You need to think about what to do about effort. If you're passing in p,c,q, and taking the catch from the model, profits will then depend on effort....

But you need to rationalize the effort based on the other parameters to prodice the catch. 

So, what if you do something like this. 

Take as your starting point the current model, with no other information. 

With that, the most reasonable prior certainly seems like fishing mortality this year will be similar to fishing mortality last year. 

That seems more reasonable that imposing a random oscilation on it. 

Now, suppose that you had the complete price, cost, and technological history of the fishery in hand, as you do here. What woud your prior be then? In the current case, you could of course feed it in, in which case your prior on the evolution of the fishery would depend on the initial profits, the degree of F, etc. In other words, it seems entirely plausible that if you just feed in a bioeconomic model, your prior could very easily be off sync with the truth, e.g. if you catch the fishery on a downswing in profits but you start it assuming an upswing?

But let's follow that thought exercise, what would it look like?

if you have p,c,q over time (and assume for now they are constant), is there value in making the prior the bioeconomic delta? to get the whole cycle going, you need to make some judgement about initial profits. If you assume zero profits from the getto, then the delta is 1, and your prior is back to the current prior (f is just f last year). 

If you assume some positive initial profits, then the cycle can get going until it hits equilibrium. 

So, the question of what those initial profits are is of course critical: are they going up or down? If ther are profits, then you predict fishing pressure to go up next year. if negative, you expect fishing pressure to decline. 

Under your old idea, you would pick a totally arbitrary effort level and stick with that, with the idea being that you're only focusing on the change in fishing mortality. But, that still requires an assumption about how profitable things are. 

one idea there would be to pick a totally arbitrary effort level, apply your data, and calculate catch and profits based on the biomass from the model and those thing. So, in that world, the effort (and f) is independent of the f's estimated by the model. 

Which is actually better right? Otherwise it's a little weird, your effort is now coming from your latent variable, not vice versa, i.e. f is a prior on itself. Which is actually what you're already doing, but with more of a functional form. 

Another idea would be to assumine something about intiial profitability, say by profits/effort, where we call effort a day of fishing or something. 

You could divie across, to get p/e, and then solve for e such that p/e is some number that you like. In that world you'd get p/e, price for a given unit of catch, cost per day of fishing, q.... which no one would be able to tell you q, so you'd have to be back to f....  you could play with it. 

A third idea is to make everything in terms of deltas: your prior is that f is last years f unless something changes in p,q, or c, or E. Under that model, you'd calculate the effort in a given time step using the last time steps p,q,c,E, and then calculate it using the changed p,q,c,E, and apply that change to f (up by 10%, down by 25%, etc.)

To summarize: 

1. Take your "data" on p,q,c, along with a random effort, and project the evolution of effort using the biomass from the model, and use the deltas from this to inform your prior on f. has the appeal of being more independent of f, though not totally through the biomass link, and makes it pretty robust to just plugging and chugging: add your timeseries of "data" and let it go to down

2. Bring the f's a bit more specifically into the equation. Assuming something about q, find the E that at least gives you the observed catch. Hmm though by that logic E is just a linear transformation of F in each time step, and as such is useless. You could "seed" the system there to get the scale right and then let it evolve? COuld also sove for profits per unit effort, instead of catch

3. Make it all about change: your prior is the default prior unless there is a shock to prices, q, or c. i.e. if price doubles youd expect effort to double relative to what it would have been without the price increase. This seems like the cleanest idea.... you still use the model to get at joint changes and non-linear effects, but you're not assuming much about where you are in the cycle. 

Think of it this way, if I tell you the p, c, q, and they are all constant, and ask me what I think is happening to effort, the honest answer at any given timestep is that I don't know. It could be going up, it could be going down, it could be constant, depending where on the equilibrium path I think I am. The only real "prior" that I have then is that it oscilates, so I could pass those to the bioeconomic model and hope that I get the osscilations right. The only way this really adds value then is with some other piece of information, like an idea of profits per trip. Or, maybe through some linkage through the evolution of the system, but that seems more complicated. 

So, given just that above information, it seems more logical to go with the shock-based prior: If you tell me that price doubled last year, that does give me some idea of what might be happening to effort (though need to think through supply/demand dynamics here). It would be nice to generalize the case here, which actually I think you can do. If you evolve the system and just calculate the counterfactual difference every year, it should be 1 if nothing changes, so your prior defaults back to the basic one, but change when there is a shock. Let's explore this one for the week. To put it more clearly, I think you have a stronger prior on how thins should change in response to schocks than you do on how effort should evolve per-say. 

Suppose you knew nothing else, but that price had doubled. What would you expect to happen?


```{r}

library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

set.seed(42)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    linf = 100,
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1,
    price_cv = 2,
    price_ac = 0
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0.75,
  cost_ac = 0.2,
  q_cv = .1,
  q_ac = .85
)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0),
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(f)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(cost)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

scrooge_data <- list(
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")




```


booyakasha. So what do to do with this. 

I like the idea of starting with the delta model. So there, you want to quantify the change in F relative to the F that would have been expected if prices/costs hadn't changed. 

How to do that?

In each time step, you're calculating profits in that time step. you could calculate profits in that time step based on the conditions on the current year, and the conditions in the last year. 

When does that affect your prior?

Under the bioeconomic model you're working with, effort in the current year responds to profits in the prior year. Or specifically, $f_{t + 1} = f_t + profits_t$

```{r}


```

Right, so this is informative if you have an idea of the effort and biomass. But, if you're going with the shock model, suppose now that you have the case you put above, where things change. How do you incorporate that. I think the answer is now you need three years of data. 

Effort in year t is delta percent different than the delta in year t that would have been expected without the shock. 

$$f_t + 1 \sim normal(f_t * \Delta)$$


so what is $\delta$?

delta is new f resulting from the current prices and costs / new f resulting from the last prices and costs. 


in practice then. 

$$\frac{f_t + \theta(p_t^sq_tE_tB_t  - c_t^sE_t^\beta)}{f_t + \theta(p_tq_tE_tB_t  - c_tE_t^\beta)}$$

Then $f_t + \theta$ drops out, as does anything els that doesn't change (I think), doesn't really matter, since the nice thing is if things don't change this is 1

So to operationalize this, you need to track changes in the things of interest and calculate two profits in each time step: the profits using the current values and the profits using last time steps values, focusing in on profits and costs at the moment. 

For now, assume a beta of 1, since if you do that then effort drops out, and the change is all dependent 

```{r}
library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

set.seed(42)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    linf = 100,
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1,
    price_cv = 2,
    price_ac = 0
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0.75,
  cost_ac = 0.2,
  q_cv = .1,
  q_ac = .85
)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      price = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0),
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(f)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(cost)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

price_and_cost_history <- sim %>% 
  group_by(year) %>% 
  summarise(price = unique(price),
            cost = unique(cost)) %>% 
  gather(variable, value, -year) %>% 
  group_by(variable) %>% 
  mutate(lag_value = lag(value,1)) %>% 
  ungroup() %>% 
  mutate(lag_value = ifelse(is.na(lag_value), value, lag_value))

price_history <- price_and_cost_history %>% 
  filter(variable == "price")

cost_history <- price_and_cost_history %>% 
  filter(variable == "cost")


scrooge_data <- list(
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  price_t = price_history %>% select(value, lag_value),
  cost_t = cost_history %>% select(value, lag_value),
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_selectivity_at_age = fleet$sel_at_age,
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")
```

```{r}

p1 <-  1
p2 <- 3.46

c1 <- 1
c2 <- 1

q <- .0001

beta <- 1.3


test <- expand.grid(effort = 1:100, biomass = seq(1000,10000, length.out = 100)) %>% 
  mutate(prof1 = p1 * q *effort * biomass - c1 * effort ^ beta,
         prof2 = p2 * q * effort * biomass - c2 * effort ^ beta) %>% 
  mutate(delta = prof2 - prof1)



test %>% 
  ggplot(aes(effort, biomass, fill = delta) ) +
  geom_raster() + 
    scale_fill_viridis_c()

```

OK the net different has much better properties than the damn delta percent. So, you're back to to modifying by addition. hooray. take a look back at SSCOM and see how he dealt with that. 

What you can at least do is back out the efforts based on the q... and to some extent this should just give a fucntional form to the evolution. Let's try it and see what happens. Where "it" refers now to a max/min absolute change in f, calculated from the bioeconomic model. 

See how that goes, and if the answer is badly, so be it, back to the drawing board. 

Could think about some kind of distribution to draw from based on plausible states of the world.... ugh. well this is why it hasn't been done!

So, where are we now?

One idea: Start with a seed effort that rationalizes the initial catch (which is made up anyway). 

Now, in one world, you have a sort of "parallel" stock trajectory, where you let effort and F, and subsequently biomass evolve on their own according to the profit function you provide. 

You then calculate the shocks and update your priors on F, which affect the "actual" stock dynamics. 

In the other example, you utilize the estimated F's by the model, and just keep track of the change in F that would have been expected under a shock. So, if in time step 2, price doubled, and you think that means a doubling in F, updating that delta. 

So what that means is that you have a vector of deltas, and in each time step, you're simply estimating that delta. 

That seems like the slightly simpler and approach, and the one that seems less prone to just being way off by virtue of model misspecification. With the other approach, if you're a) adding in a lot of computation time by running a literal parallel population model and b) since all you're utilizing is the shocks,but since the degree of the shock depends on E and B, if your E and B are wildly off from the population model expected E and B

so, the problem now is how to get E to rationalize all this.... 

poop. If you pick a random R, then your catches don't match up. If you stick with F, then you need to translate cost per unit F instead of cost per unit E.

So in a perfect world in every time step you need the E that produces the observed catch... BALLS. Simplest would be a bisection method?

What about estimating Effort and effort devs and translating to F to save time and effort... intereating. 

So what would that look like. THe only reason you're working with F right now is that it makes the search space a bit neater, between 0 and 10. 

What if you worked in raw effort instead. Fiven that you're fixing the size of the population at some arbitraty level, and given that you're going to specify q as well, it wouldn't be that hard to 

So in that world, you estimate effort_t, with lower of 0 and upper of some much bigger number. 

Alternatively, and this would probably be numerically simpler, you estimate a mean effort, and then a vector of effort_dev_t that are deviates from that effort. with mean 0 and some sigma. So you're just shifting the model over to effort based instead of F based. 

So what would that look like

in the simpler case....

 $$effort_t \sim unif(effort^{min}, effort^{max})$$
 
 AHA, no, here's what you do, you keep the uniform multiplier from 0 to like 10, and you just multiply that by your base effort factor, that instead of estimating (for now) you set at some arbitrary level, like the effort that makes f = m, which is just m / q!!!! BOOM. you could make it a free parameter later, but doesn't make a lot of sense since it's just helping scale the system. 
 
 $$ effort_t \sim normal(effort_{t-1}, \sigma_{effort}) $$




If that for some reason doesn't work, then you stick with estimating F's, and have to find the E that produces the catch given the other parameters using probably a bisection method. 


```{r}
library(rstan)
library(FishLife)
library(spasm)
library(tidyverse)

set.seed(42)
fish <-
  create_fish(
    query_fishlife = T,
    mat_mode = "length",
    linf = 100,
    max_age = 30,
    time_step = 1,
    sigma_r = 2,
    price = 1,
    price_cv = .1,
    price_ac = 0
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0.75,
  cost_ac = 0.2,
  q_cv = .1,
  q_ac = .85
)

fleet <-
  spasm::update_fleet(
    fleet = purrr::list_modify(
      fleet,
      fleet_model = "open-access",
      initial_effort = 100,
      cost = 1,
      target_catch = 100,
      sigma_effort = 0,
      length_50_sel = 50,
      theta_tuner = 0.25
    ),
    fish = fish
  )


sim <- spasm::sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(mpa_size = 0),
  num_patches = 1,
  sim_years = 25,
  burn_year = 50,
  time_step = fish$time_step
)

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(f)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()

sim %>% 
  group_by(year) %>% 
  summarise(thing = unique(cost)) %>% 
  ggplot(aes(year, thing)) + 
  geom_point()


true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

true_f %>% 
  ggplot(aes(year, true_f)) + 
  geom_point()

linf_buffer <- 1.5

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = linf_buffer
) %>% 
  ungroup() %>% 
  select(age,length_bin, p_bin) %>% 
  spread(length_bin, p_bin) %>% 
  select(-age)


length_comps <- sim %>%
select(year, patch, age, numbers, numbers_caught) %>%
nest(-year, -patch, .key = n_at_age) %>% 
mutate(catch_length = map(
n_at_age,
~ sample_lengths(
n_at_age = .x,
cv = fish$cv_len,
k = fish$vbk,
linf = fish$linf,
t0 = fish$t0,
sample_type = 'catch',
percent_sampled = 1,
time_step = fish$time_step,
linf_buffer = linf_buffer
)
)) %>% 
select(year, catch_length ) %>% 
  unnest() %>% 
  mutate(numbers = round(numbers)) %>% 
  spread(length_bin, numbers) %>% 
  mutate(year = year - min(year) + 1)

price_and_cost_history <- sim %>% 
  group_by(year) %>% 
  summarise(price = unique(price),
            cost = unique(cost),
            q = fleet$q) %>% 
  gather(variable, value, -year) %>% 
  group_by(variable) %>% 
  mutate(lag_value = lag(value,1)) %>% 
  ungroup() %>% 
  mutate(lag_value = ifelse(is.na(lag_value), value, lag_value))

price_t <- price_and_cost_history %>% 
  filter(variable == "price")

cost_t <- price_and_cost_history %>% 
  filter(variable == "cost")

q_t <- price_and_cost_history %>% 
  filter(variable == "q")


scrooge_data <- list(
  economic_model = 1,
  estimate_recruits = 1,
  length_comps = length_comps %>% select(-year),
  length_comps_years  = length_comps$year,
  price_t = price_t %>% select(value, lag_value),
  cost_t = cost_t %>% select(value, lag_value),
  q_t = q_t %>% select(value, lag_value),
  beta = 1.3,
  base_effort = fish$m / mean(q_t$value),
  length_50_sel_guess = fish$linf / 2,
  delta_guess = 2,
  n_lcomps = nrow(length_comps),
  nt = length(length_comps$year),
  n_ages = fish$max_age + 1,
  n_lbins = ncol(length_at_age_key),
  ages = 1:(fish$max_age + 1),
  mean_length_at_age = fish$length_at_age,
  mean_weight_at_age = fish$weight_at_age,
  mean_maturity_at_age = fish$maturity_at_age,
  m = fish$m,
  h = fish$steepness,
  r0 = fish$r0,
  k = fish$vbk,
  loo = fish$linf,
  t0 = fish$t0,
  length_at_age_key = as.matrix(length_at_age_key)
) 

set.seed(42)
test_scrooge <- rstan::stan(file = here::here("scripts", "scrooge.stan"), data = scrooge_data, chains = 1, refresh = 10, cores = 1, iter = 4000, warmup = 2000)

check<- rstan::extract(test_scrooge)

# check f fit

fs <- array_branch(check$f_t,1) 

fs <- data_frame(thing = map(fs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_f <- sim %>% 
  group_by(year) %>% 
  summarise(true_f = unique(f)) %>% 
  mutate(year = year - min(year) + 1) %>% 
  ungroup()

econ_fplot <- fs %>% 
  ggplot(aes(year,value)) + 
  geom_boxplot(aes(group = year), alpha = 0.75) +
  geom_point(data = true_f, aes(year, true_f), color = "red") + 
  geom_line(data = true_f, aes(year, true_f), color = "red")

# check length fits

lengths <- array_branch(check$n_tl,1) 

lengths <- data_frame(thing = map(lengths, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

colnames(lengths) <- c("iteration", colnames(length_at_age_key),"year")

lengths <- lengths %>% 
  gather(length_bin, number, -iteration,-year)

mean_lengths <- lengths %>% 
  group_by(year, length_bin) %>% 
  filter(iteration == 1) %>% 
  summarise(mean_numbers = mean(number))


tidy_length_comps = length_comps %>% 
  gather(length_bin, mean_numbers, -year) %>% 
  mutate(length_bin = as.numeric(length_bin)) %>% 
  mutate(source = "observed")


length_comparison <- mean_lengths %>% 
  ungroup() %>% 
  mutate(length_bin = as.numeric(length_bin),
         source = "predicted") %>% 
  bind_rows(tidy_length_comps) %>% 
  ungroup() %>% 
  group_by(year, source) %>% 
  mutate(mean_numbers = mean_numbers / sum(mean_numbers)) %>% 
  filter(year %in% c(1,10,24)) %>% 
  ggplot(aes(length_bin, mean_numbers, fill = source)) + 
  geom_col(position = "dodge") +
  # geom_density(stat = "identity", alpha = 0.5) +
  facet_wrap(~year)

length_comparison



# recruits comparison

rec_devs <- array_branch(check$rec_dev_t,1) 

rec_devs <- data_frame(thing = map(rec_devs, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest()

true_rec_devs <- sim %>% 
  group_by(year) %>% 
  summarise(true_value = unique(rec_dev) + fish$sigma_r^2/2) %>% 
  ungroup() %>% 
  mutate(year = year - min(year)  + 1 )

rec_devs <- rec_devs #%>% 
  # left_join(true_rec_devs, by = "year") %>% 
  # gather(source, value, contains("value"))
  # 
  
rec_devs %>% 
  ggplot(aes(year, value)) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_boxplot(aes(group = year)) + 
  geom_line(data = true_rec_devs, aes(year, true_value), color = "red") +
  geom_point(data = true_rec_devs, aes(year, true_value), color = "red")

profit_shocks <- array_branch(check$profit_shock_t,1) 

total_effort <- array_branch(check$total_effort_t,1) 


profit_shocks <- data_frame(thing = map(profit_shocks, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest() %>% 
  mutate(type = "profits")

total_effort <- data_frame(thing = map(total_effort, ~as_data_frame(.x) %>% mutate(year = 1:nrow(.)))) %>% 
  mutate(iteration = 1:nrow(.)) %>% 
  unnest() %>% 
  mutate(type = "effort")

thing <- profit_shocks %>% 
  bind_rows(total_effort) %>% 
  spread(type, value) %>% 
  mutate(relative_thing = profits / effort)

thing %>% 
  ggplot(aes(x = year,y = profits / sd(profits))) +
  geom_point()




```

OK! Time to set this thing up, concept is up and running. 

# check in with Jason

Damn good point, this thing actually provides estimated catch histories based on the lengths. Catch-only model actually estimates SPR (simple stock synthesis, since it's age structured catch dynamics), but SPR doesn't provide catch

measure performance as a function of bias (i.e. identify probability of estimating F that's waaaay too low)

fold LIME into SS, a future thing would be to take this concept and fold into SS

Check with an economist or two on the shock idea 

# check in with merril

Multifleet stuff is the the biggest sticking point so far

Incorporating uncertainty in life history 

Seems like my fits 

# 2018-03-22 Thoughts

SO where to now? You have a model that fits and seems to do pretty well under some circumstances. 

You can obviously tinker till the cows come home, what's an effective strategy to move forward?


Jason had two important outputs that would be good to build in: estiamted catch trajectory and bias (how often do you think you're overfishing when you're under, and vice versa)


It would be good to have something to benchmark performance against. 

To that end, I think the best option to add in at this point would be scrooge-lite (i.e. knock out the economic prior), LIME, and LBSPR, and measure performance of those and compare. 

That gives you something to push against in terms of performance: If you're already doing way better than the others one no need to go too crazy. 

From there, try and fit your performance model. 

That gives you a nice complete draft of results to work with over the next month. 


# Testing LIME

from lime::LIME_example.R

```{r}
library(LIME)

fish <- v3$prepped_fishery[[1]]$fish

fleet <- v3$prepped_fishery[[1]]$fleet

scrooge_lh <- create_lh_list(vbk= fish$vbk,
					 linf= fish$linf,
					 t0= fish$t0,
					 lwa= fish$weight_a,
					 lwb= fish$weight_b,
					 S50= fleet$length_50_sel,
					 S95=fleet$length_95_sel,
					 selex_input="length",
					 selex_type=c("logistic"),
					 M50= fish$length_50_mature,
					 M95= fish$length_95_mature,
					 maturity_input="length",
					 M= fish$m,
					 binwidth=1,
					 CVlen= fish$cv_len,
					 SigmaR= fish$sigma_r + .001,
					 SigmaF= fleet$sigma_effort + .001,
					 SigmaC=0.2,
					 SigmaI=0.2,
					 R0= fish$r0,
					 qcoef=1e-5,
					 start_ages=0,
					 rho=0.43,
					 nseasons=1)

temp_LF_matrix <- v3$prepped_fishery[[1]]$length_comps


LF_matrix <- temp_LF_matrix %>% 
  as.matrix()

rownames(LF_matrix) <- LF_matrix[,"year"]


LF_matrix <- LF_matrix[, -c(1)]

scrooge_data_LF <-
  list("years" = 1:nrow(LF_matrix), "LF" = LF_matrix)
  

start <- Sys.time()
res <- run_LIME(modpath=NULL,
				lh=scrooge_lh,
				input_data=scrooge_data_LF,
				est_sigma="log_sigma_R",
				data_avail="LC",
				newtonsteps=3)
end <- Sys.time() - start


check <- res$df

## check for other issues
issues <- res$issues

## check TMB inputs
Inputs <- res$Inputs

## Report file
Report <- res$Report

## Standard error report
Sdreport <- res$Sdreport

##----------------------------------------------------
## Step 4: Plot fits
## ---------------------------------------------------
## plot length composition data
plot_LCfits(LFlist=list("LF"=LF_matrix),
			Inputs=Inputs,
			Report=Report)

# plot_output(Inputs=Inputs,
#             Report=Report,
#             Sdreport=Sdreport,
#             lh=scrooge_lh,
#             True=1,
#             plot=c("Fish","Rec","SPR","ML","SB","Selex"),
#             set_ylim=list("Fish" = c(0,1), "SPR" = c(0,1)))

true_f <- v3$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(f = unique(f))

true_f$predicted_f <- Report$F_y

# reportnames <- Sdreport$value %>% names()

# Sdreport$sd[str_detect(reportnames,"F_t")]

predicted_f <- data_frame(f = Report$F_y)

true_f %>% 
  ggplot() +
  geom_point(aes(year,f,color = "True")) + 
  geom_line(aes(year, predicted_f,color = "Predicted"))

temp_LF_matrix %>% 
  gather(length, numbers, -year) %>% 
  mutate(length = as.numeric(length)) %>% 
  ggplot(aes(length, numbers, fill = factor(year))) + 
  geom_density(stat = "identity", alpha = 0.5)
```

Innnnteresting that doesn't work well at all. Let's try and flip it around and use LIME to predict F from LIME


```{r}


lh <- create_lh_list(vbk=0.21,
					 linf=65,
					 t0=-0.01,
					 lwa=0.0245,
					 lwb=2.79,
					 S50=20,
					 S95=26,
					 selex_input="length",
					 selex_type=c("logistic"),
					 M50=34,
					 M95=NULL,
					 maturity_input="length",
					 M=0.27,
					 binwidth=1,
					 CVlen=0.1,
					 SigmaR=0.7,
					 SigmaF=0.1,
					 SigmaC=0.2,
					 SigmaI=0.2,
					 R0=1,
					 qcoef=1e-5,
					 start_ages=0,
					 rho=0.43,
					 nseasons=1)

true <- generate_data(modpath=NULL,
					  itervec=1,
					  Fdynamics="Endogenous",
					  Rdynamics="Constant",
					  lh=lh,
					  Nyears=20,
					  Nyears_comp=20,
					  comp_sample=200,
					  init_depl=0.5,
					  seed=123)

plot(true$F_t)

LF_matrix <- true$LF

## example with length data only
data_LF <- list("years"=1:true$Nyears, "LF"=LF_matrix)


lh <- create_lh_list(vbk=0.21,
                     linf=65,
                     t0=-0.01,
                     lwa=0.0245,
                     lwb=2.79,
                     S50=20,
                     S95=26,
                     selex_input="length",
                     selex_type=c("logistic"),
                     M50=34,
                     M95=NULL,
                     maturity_input="length",
                     M=0.27,
                     binwidth=1,
                     CVlen=0.1,
                     SigmaR=0.7,
                     SigmaF=0.1,
                     SigmaC=0.2,
                     SigmaI=0.2,
                     R0=1,
                     qcoef=1e-5,
                     start_ages=0,
                     rho=0.43,
                     nseasons=1)

true <- generate_data(modpath=NULL,
					  itervec=1,
					  Fdynamics="Endogenous",
					  Rdynamics="Constant",
					  lh=lh,
					  Nyears=20,
					  Nyears_comp=20,
					  comp_sample=200,
					  init_depl=0.5,
					  seed=123)

LF_matrix <- true$LF

fish <-
  create_fish(
    scientific_name = NA,
    min_age = 0,
    max_age = lh$AgeMax,
    linf = lh$linf,
    vbk = lh$vbk,
    t0 = lh$t0,
    weight_a = lh$lwa,
    weight_b = lh$lwb,
    length_50_mature = 34,
    length_95_mature = 36,
    length_mature = 34,
    m = lh$M,
    cv_len = lh$CVlen,
    query_fishlife = F,
    mat_mode = "length",
    time_step = 1,
    sigma_r = lh$SigmaR,
    price = 1,
    price_cv = 0,
    price_ac = 0,
    r0 = 100000
  )

fleet <- create_fleet(
  fish = fish,
  cost_cv =  0,
  cost_ac = 0,
  q_cv = 0,
  q_ac = 0,
  fleet_model = 'constant-effort',
  target_catch = 10,
  length_50_sel = lh$S50,
  delta = lh$SL95 - lh$S50
)

length_at_age_key <- generate_length_at_age_key(
  min_age = fish$min_age,
  max_age = fish$max_age,
  cv = fish$cv_len,
  linf = fish$linf,
  k = fish$vbk,
  t0 = fish$t0,
  time_step = fish$time_step,
  linf_buffer = 1.5
) %>%
  ungroup() %>%
  select(age, length_bin, p_bin) %>%
  spread(length_bin, p_bin) %>%
  select(-age)

length_comps <- LF_matrix %>% 
  as.data.frame() %>% 
  mutate(year = 1:nrow(.)) %>% 
  select(year, everything())

length_comps %>% 
  gather(length,numbers, -year) %>% 
  mutate(length = as.numeric(length)) %>% 
  ggplot(aes(length,numbers, fill = factor(year))) + 
  geom_density(stat = "identity", show.legend = F)

price_t = data.frame(value = rep(2,nrow(length_comps)), lag_value = rep(2,nrow(length_comps)))
  
cost_t = data.frame(value = rep(1,nrow(length_comps)), lag_value = rep(1,nrow(length_comps)))

q_t <- data.frame(value = rep(0.001,nrow(length_comps)), lag_value = rep(0.001,nrow(length_comps)))

scrooge_data <- list(
      economic_model = 1,
      estimate_recruits = 1,
      length_comps = length_comps %>% select(-year),
      length_comps_years  = length_comps$year,
      price_t = price_t %>% select(value, lag_value),
      cost_t = cost_t %>% select(value, lag_value),
      q_t = q_t %>% select(value, lag_value),
      beta = 1.3,
      base_effort = fish$m / mean(q_t$value),
      length_50_sel_guess = fish$linf / 2,
      delta_guess = 2,
      n_lcomps = nrow(length_comps),
      nt = length(length_comps$year),
      n_ages = fish$max_age + 1,
      n_lbins = ncol(length_at_age_key),
      ages = 1:(fish$max_age + 1),
      mean_length_at_age = fish$length_at_age,
      mean_weight_at_age = fish$weight_at_age,
      mean_maturity_at_age = fish$maturity_at_age,
      m = fish$m,
      h = fish$steepness,
      r0 = fish$r0,
      k = fish$vbk,
      loo = fish$linf,
      t0 = fish$t0,
      length_at_age_key = as.matrix(length_at_age_key)
    )

  fit <-
    rstan::stan(
      file = here::here("src", "scrooge_v2.0.stan"),
      data = scrooge_data,
      chains = 1,
      refresh = 25,
      cores = 1,
      iter = 2000,
      warmup = 1000,
      control = list(
      adapt_delta = 0.8
    ))
  
  processed_fit <- process_scrooge(fit = fit)

  true_f <- data_frame(year = 1:length(true$F_t), true_f = true$F_t)
  
processed_fit$f_t %>% 
  left_join(true_f, by = "year") %>% 
  ggplot() + 
  geom_boxplot(aes(year,value, group = year)) +
  geom_point(aes(year, true_f), color = "red") + 
  lims(y = c(0,2))


```

Hell yes! scrooge predicts LIME. 

# Getting scrooge to work on VFO

OK, so you can see that scrooge works when it works, but gets waaay off track sometimes. 

Taking the VFO example, there's a few telltale signs: the effort distributions are crazy right tailed, but the effor deviations are getting stuck near zero. it's also near efforts of zero where the divergences are happening  

```{r}
sel_at_age <- vfo$prepped_fishery[[1]]$fleet$sel_at_age 

length_at_age <-  vfo$prepped_fishery[[1]]$fish$length_at_age 

plot(length_at_age, sel_at_age)

```

It's nailing selectivity at least. 

So, there's a few potential culprits here. Easiest answer is that it's the starting guess, and the model would do better if you let it run out longer. 

You could test that by feeding it the "right" starting guess out the bat and see if that gets it. If it does, cool, then you can work on helping the model find that starting guess. 

If that doesn't work, then you need to revisit the model structure itself, maybe by testing estimating F instead of effort madness. 
```{r}

true_effort_devs <- vfo$prepped_fishery[[1]]$simed_fishery %>% 
  group_by(year) %>% 
  summarise(effort = unique(effort))

true_effort_devs %>% 
  ggplot(aes(year, effort)) + 
  geom_point()


true_effort_devs <- true_effort_devs %>% 
  mutate(effort_devs = effort / vfo$prepped_fishery[[1]]$scrooge_data$base_effort)

true_effort_devs %>% 
  ggplot(aes(year, effort_devs)) + 
  geom_point()

```

One thing I really don't like at this point is that the magnitude of the deviations is tied to the appropriateness of the base effort parameter. Though to some extent that doesn't really matter since it's scaleess, except for the case of the supplied catch. So so long as Emsy is an roughly OK effort, then your multiplier shouldn't be that bad. Could use some improvement but it's not the worst thing. 


Another thing you could try: estimate effort deviates in the same manner that you estimate recruitment deviates right now: with mean from the bionomic model and deviations from that. So in that model, you're estimating deviations from bionomic equilibrium, and probably theta as well. You could still update your prior on the deviate by the economic shock. Seems a reasonable defense is that we put a lot of stock in beverton holt with limited evidence, why not this as well?

This seems like a different enough idea that it warrants a separate model... which if you can't fix this one might be the way to go

poop, feeding it the right starting guess doesn't seem to solve it... 

Let's see how good the fit is is to the length comps: if it's getting it pretty close then that means there's something abotu the explanation that is more satisfactory to the model... which poop. 

One possibility, also try really constraining sigma effort a bit more, I wonder if it's too big at the moment. Part of the problem is that the likelihood actually is going up at those crazy low efforts.... 

To an exte

```{r}

a <- vfo$processed_scrooge[[1]]$n_tl

predicted <- a %>% 
  gather(length,numbers, contains('V')) %>% 
  mutate(length = str_replace(length,'\\D','') %>% as.numeric()) %>% 
  group_by(year, length) %>% 
  summarise(numbers = median(numbers)) %>% 
  mutate(source = "predicted")
 

observed <- vfo$prepped_fishery[[1]]$length_comps %>% 
  gather(length, numbers, -year) %>% 
  mutate(length = as.numeric(length)) %>% 
  mutate(source = "observed")

observed %>% 
bind_rows(predicted) %>% 
 ungroup() %>% 
  group_by(source, year) %>% 
  mutate(numbers = numbers / sum(numbers)) %>% 
  ungroup() %>% 
  ggplot(aes(length,numbers, fill = source)) + 
  geom_col(position = "dodge") + 
  facet_wrap(~year) + 
  theme_minimal()

```

Innnnteresting, those fit's aren't very good at all. Let's inspect the actual likelihood a bit here and make sure there's not just an error in there. 

Aha, sigma effort is part of what' causing such a problem in here

I wonder if the core problem here is that duh, recruitment and F are really hard to tease appart, and so given variation in both, the model prefers to pack the variation into one of the things, like recruitment in thise case

OK, digging into PFO a bit, even there, where the fit is "good", it's not great. I't missing the length comps a decent bit in some years (though you obviuosly also need to burn for a bit). In the PFO example, the likelihood  gets better the lower sigma_r is, which is correct, but divergences also start to pile up the lower sigma R gets. 

Could explore some transformations here: simple one would be to estimate log sigma r / log sigma effort. There's definitely some hierarchichal funneling going on... read up on that some more. As a starting point KISS, let's try log transforming. 

Alternatively, you could estimate one, and estimate and or pass a ratio of the variation in recruitment relative to the variaion in effort

Also see neil's funnel in the stan documentation page 341

Well loggint at least helped with the divergences

With little data (aka here), non-centering can help a lot. Let's think about how to do that in the context of this model 

Starting with the recruitment, you're making a strong assumption that there's no bias in the recruitment, therefore mean_beta is 0. So, in that case you're now estimating sigma_r and a vector of rec_devs ~ normal(0,1)

the actual rec devs in a given time step will then be sigma_r * rec_devs

What about for F? 

Following the example from the stan documentation, the staight forward translation would be


effort_devs  = mean_effort + sigma_effort * ed

ed ~ normal(0,1). 

So what if you went off script a bit here on the f's saying that f in the current year is most likely F last year, i'm unclear how that differs from saying that sigma effort should be really small. 

so, you have base effort, and you want to multiply that by a thing with mean 1. 

So, base_effort * exp(deviation)

Where deviation is  mean 0. 

Now, you could say that the deviation in any given time step 

`total_effort_t = base_effort * exp(effort_devs * sigma_effort)`

So how do you build the economic shocks in there?

`effort_devs[t] ~ normal(profit_shocks[t],1)`

where profit_shock is usualyl 0, but sometimes is very positive


AHA, wait, I think there's a problem with the low profit shock scenario; Suppose that sd is really low, isn't that going to crazy inflate your effort deviation? Check on this. 

Also, can use this formulation to actually estimate mean effort a bit easier

DOUBLE CHECK that you're not estimating parameters with zero information (i.e. recruitment in the final year)

Given the near fact that the model is working pretty damn well for the simpler cases, I'm wondering if there is something systematically wrong with the way that you're setting up that simulation, or if it's simply that you can't possibly estimate two random processes with the same variance. 

# ideas to improve scrooge

The more I dig into it, it seems likely that the model isn't really capable of dealing 
substantial variaion in both recruitment and effort dynamics. To an extent that's logical: without a prior 

# check in with cody

which one is in the drivers seat: effort or recruitment. So, add in to your simulation testing steepness of 1 and autocorrelated recruitment variation vs steepness of like 0.6 and effort following that

estimate rho (autocorrelation in recruitment)

address szuwalski & thorson 2017
